[ Using CUDA ]
{'tokenizer': <function tokenize_jobs at 0x7f3eeb043310>, 'pretrained_word_emb_name': '6B', 'pretrained_word_emb_url': None, 'pretrained_word_emb_cache_dir': None, 'merge_strategy': 'tailhead', 'edge_strategy': 'heterogeneous', 'seed': None, 'word_emb_size': 300, 'thread_number': 1, 'port': 9000}
Using Table2Text read_raw_data
Loaded WikitableQA, train
Found:  14149  items
Loaded SQL
Found:  11276  items
Loaded WikitableQA, test
Found:  4344  items
Loaded WikitableQA, dev1
Found:  2831  items
Loaded WikitableQA, dev2
Found:  2838  items
Loaded WikitableQA, dev3
Found:  2838  items
table _process works!
Port 9000, processing: 0 / 14149
Port 9000, processing: 1000 / 14149
Port 9000, processing: 2000 / 14149
Port 9000, processing: 3000 / 14149
Port 9000, processing: 4000 / 14149
Port 9000, processing: 5000 / 14149
Port 9000, processing: 6000 / 14149
Port 9000, processing: 7000 / 14149
Port 9000, processing: 8000 / 14149
Port 9000, processing: 9000 / 14149
Port 9000, processing: 10000 / 14149
Port 9000, processing: 11000 / 14149
Port 9000, processing: 12000 / 14149
Port 9000, processing: 13000 / 14149
Port 9000, processing: 14000 / 14149
Port 9000, processing: 0 / 11276
Port 9000, processing: 1000 / 11276
Port 9000, processing: 2000 / 11276
Port 9000, processing: 3000 / 11276
Port 9000, processing: 4000 / 11276
Port 9000, processing: 5000 / 11276
Port 9000, processing: 6000 / 11276
Port 9000, processing: 7000 / 11276
Port 9000, processing: 8000 / 11276
Port 9000, processing: 9000 / 11276
Port 9000, processing: 10000 / 11276
Port 9000, processing: 11000 / 11276
Port 9000, processing: 0 / 4344
Port 9000, processing: 1000 / 4344
Port 9000, processing: 2000 / 4344
Port 9000, processing: 3000 / 4344
Port 9000, processing: 4000 / 4344
Port 9000, processing: 0 / 2831
Port 9000, processing: 1000 / 2831
Port 9000, processing: 2000 / 2831
Port 9000, processing: 0 / 2838
Port 9000, processing: 1000 / 2838
Port 9000, processing: 2000 / 2838
Port 9000, processing: 0 / 2838
Port 9000, processing: 1000 / 2838
Port 9000, processing: 2000 / 2838
Loading pre-built vocab model stored in bothWTQ/processed/TableGraph/vocab.pt
loading model
Traceback (most recent call last):
  File "main_bert.py", line 329, in <module>
    runner = TableQA(opt)
  File "main_bert.py", line 28, in __init__
    self._build_model()
  File "main_bert.py", line 128, in _build_model
    self.model = Graph2Seq.load_checkpoint(self.opt["checkpoint_save_path"],
  File "/home/lily/lyf6/miniconda3/envs/graph4nlp_ziva/lib/python3.8/site-packages/torch/nn/modules/module.py", line 899, in to
    return self._apply(convert)
  File "/home/lily/lyf6/miniconda3/envs/graph4nlp_ziva/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/lily/lyf6/miniconda3/envs/graph4nlp_ziva/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  File "/home/lily/lyf6/miniconda3/envs/graph4nlp_ziva/lib/python3.8/site-packages/torch/nn/modules/module.py", line 570, in _apply
    module._apply(fn)
  [Previous line repeated 4 more times]
  File "/home/lily/lyf6/miniconda3/envs/graph4nlp_ziva/lib/python3.8/site-packages/torch/nn/modules/module.py", line 593, in _apply
    param_applied = fn(param)
  File "/home/lily/lyf6/miniconda3/envs/graph4nlp_ziva/lib/python3.8/site-packages/torch/nn/modules/module.py", line 897, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
[ Using CUDA ]
{'tokenizer': <function tokenize_jobs at 0x7f3db0d15310>, 'pretrained_word_emb_name': '6B', 'pretrained_word_emb_url': None, 'pretrained_word_emb_cache_dir': None, 'merge_strategy': 'tailhead', 'edge_strategy': 'heterogeneous', 'seed': None, 'word_emb_size': 300, 'thread_number': 1, 'port': 9000}
Using Table2Text read_raw_data
Loaded WikitableQA, train
Found:  14149  items
Loaded SQL
Found:  11276  items
Loaded WikitableQA, test
Found:  4344  items
Loaded WikitableQA, dev1
Found:  2831  items
Loaded WikitableQA, dev2
Found:  2838  items
Loaded WikitableQA, dev3
Found:  2838  items
Main File: ../../../graph4nlp/pytorch/WikiTableQA/WikiTableQuestions/seq2seq_data/train_perturb.csv
Loaded WikitableQA, perturb_test_orig
Found:  2720  items
Main File: ../../../graph4nlp/pytorch/WikiTableQA/WikiTableQuestions/seq2seq_data/train_perturb.csv
Loaded WikitableQA, perturb_test_changed
Found:  2720  items
Main File: ../../../graph4nlp/pytorch/WikiTableQA/WikiTableQuestions/seq2seq_data/dev_perturb.csv
Loaded WikitableQA, perturb_dev_orig
Found:  585  items
Main File: ../../../graph4nlp/pytorch/WikiTableQA/WikiTableQuestions/seq2seq_data/dev_perturb.csv
Loaded WikitableQA, perturb_dev_changed
Found:  585  items
table _process works!
Port 9000, processing: 0 / 14149
Port 9000, processing: 1000 / 14149
Port 9000, processing: 2000 / 14149
Port 9000, processing: 3000 / 14149
Port 9000, processing: 4000 / 14149
Port 9000, processing: 5000 / 14149
Port 9000, processing: 6000 / 14149
Port 9000, processing: 7000 / 14149
Port 9000, processing: 8000 / 14149
Port 9000, processing: 9000 / 14149
Port 9000, processing: 10000 / 14149
Port 9000, processing: 11000 / 14149
Port 9000, processing: 12000 / 14149
Port 9000, processing: 13000 / 14149
Port 9000, processing: 14000 / 14149
Port 9000, processing: 0 / 11276
Port 9000, processing: 1000 / 11276
Port 9000, processing: 2000 / 11276
Port 9000, processing: 3000 / 11276
Port 9000, processing: 4000 / 11276
Port 9000, processing: 5000 / 11276
Port 9000, processing: 6000 / 11276
Port 9000, processing: 7000 / 11276
Port 9000, processing: 8000 / 11276
Port 9000, processing: 9000 / 11276
Port 9000, processing: 10000 / 11276
Port 9000, processing: 11000 / 11276
Port 9000, processing: 0 / 4344
Port 9000, processing: 1000 / 4344
Port 9000, processing: 2000 / 4344
Port 9000, processing: 3000 / 4344
Port 9000, processing: 4000 / 4344
Port 9000, processing: 0 / 2831
Port 9000, processing: 1000 / 2831
Port 9000, processing: 2000 / 2831
Port 9000, processing: 0 / 2838
Port 9000, processing: 1000 / 2838
Port 9000, processing: 2000 / 2838
Port 9000, processing: 0 / 2838
Port 9000, processing: 1000 / 2838
Port 9000, processing: 2000 / 2838
Port 9000, processing: 0 / 2720
Port 9000, processing: 1000 / 2720
Port 9000, processing: 2000 / 2720
Port 9000, processing: 0 / 2720
Port 9000, processing: 1000 / 2720
Port 9000, processing: 2000 / 2720
Port 9000, processing: 0 / 585
Port 9000, processing: 0 / 585
2022-12-12 16:56:46,892 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `test` split: 0.020
2022-12-12 17:32:50,201 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `dev1` split: 0.022
2022-12-12 18:09:07,935 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `dev2` split: 0.020
2022-12-12 18:45:59,462 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `dev3` split: 0.020
Loading pre-built vocab model stored in bothWTQ/processed/TableGraph/vocab.pt
loading model
Loaded pretrained model!
Skipping evaluation batch 30, Reason: CUDA out of memory. Tried to allocate 464.00 MiB (GPU 6; 23.70 GiB total capacity; 20.01 GiB already allocated; 207.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 158, Reason: CUDA out of memory. Tried to allocate 768.00 MiB (GPU 6; 23.70 GiB total capacity; 18.23 GiB already allocated; 75.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 208, Reason: CUDA out of memory. Tried to allocate 456.00 MiB (GPU 6; 23.70 GiB total capacity; 17.19 GiB already allocated; 439.69 MiB free; 21.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 263, Reason: CUDA out of memory. Tried to allocate 464.00 MiB (GPU 6; 23.70 GiB total capacity; 21.00 GiB already allocated; 135.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 279, Reason: CUDA out of memory. Tried to allocate 994.00 MiB (GPU 6; 23.70 GiB total capacity; 18.69 GiB already allocated; 59.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 348, Reason: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 6; 23.70 GiB total capacity; 10.54 GiB already allocated; 1.56 GiB free; 20.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 367, Reason: CUDA out of memory. Tried to allocate 360.00 MiB (GPU 6; 23.70 GiB total capacity; 19.11 GiB already allocated; 137.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 395, Reason: CUDA out of memory. Tried to allocate 446.00 MiB (GPU 6; 23.70 GiB total capacity; 20.12 GiB already allocated; 95.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 401, Reason: CUDA out of memory. Tried to allocate 456.00 MiB (GPU 6; 23.70 GiB total capacity; 20.73 GiB already allocated; 131.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 434, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 18.75 GiB already allocated; 27.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 444, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 19.15 GiB already allocated; 27.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 455, Reason: CUDA out of memory. Tried to allocate 534.00 MiB (GPU 6; 23.70 GiB total capacity; 20.04 GiB already allocated; 443.69 MiB free; 21.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 570, Reason: CUDA out of memory. Tried to allocate 476.00 MiB (GPU 6; 23.70 GiB total capacity; 18.85 GiB already allocated; 45.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 573, Reason: CUDA out of memory. Tried to allocate 1.07 GiB (GPU 6; 23.70 GiB total capacity; 18.72 GiB already allocated; 753.69 MiB free; 21.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 575, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 20.90 GiB already allocated; 59.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 659, Reason: CUDA out of memory. Tried to allocate 320.00 MiB (GPU 6; 23.70 GiB total capacity; 19.72 GiB already allocated; 185.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 690, Reason: CUDA out of memory. Tried to allocate 334.00 MiB (GPU 6; 23.70 GiB total capacity; 18.01 GiB already allocated; 321.69 MiB free; 21.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 718, Reason: CUDA out of memory. Tried to allocate 372.00 MiB (GPU 6; 23.70 GiB total capacity; 18.94 GiB already allocated; 349.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 899, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 20.71 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 904, Reason: CUDA out of memory. Tried to allocate 320.00 MiB (GPU 6; 23.70 GiB total capacity; 19.63 GiB already allocated; 47.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 919, Reason: CUDA out of memory. Tried to allocate 464.00 MiB (GPU 6; 23.70 GiB total capacity; 20.00 GiB already allocated; 301.69 MiB free; 21.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 936, Reason: CUDA out of memory. Tried to allocate 994.00 MiB (GPU 6; 23.70 GiB total capacity; 17.91 GiB already allocated; 607.69 MiB free; 21.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 973, Reason: CUDA out of memory. Tried to allocate 446.00 MiB (GPU 6; 23.70 GiB total capacity; 20.07 GiB already allocated; 315.69 MiB free; 21.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 999, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 20.31 GiB already allocated; 323.69 MiB free; 21.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1004, Reason: CUDA out of memory. Tried to allocate 1.25 GiB (GPU 6; 23.70 GiB total capacity; 16.57 GiB already allocated; 681.69 MiB free; 21.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1016, Reason: CUDA out of memory. Tried to allocate 534.00 MiB (GPU 6; 23.70 GiB total capacity; 20.06 GiB already allocated; 165.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1084, Reason: CUDA out of memory. Tried to allocate 446.00 MiB (GPU 6; 23.70 GiB total capacity; 19.78 GiB already allocated; 441.69 MiB free; 21.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1140, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 19.98 GiB already allocated; 75.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1166, Reason: CUDA out of memory. Tried to allocate 372.00 MiB (GPU 6; 23.70 GiB total capacity; 20.40 GiB already allocated; 245.69 MiB free; 21.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1183, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 17.20 GiB already allocated; 61.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1185, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 20.09 GiB already allocated; 137.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1259, Reason: CUDA out of memory. Tried to allocate 446.00 MiB (GPU 6; 23.70 GiB total capacity; 20.54 GiB already allocated; 441.69 MiB free; 21.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1330, Reason: CUDA out of memory. Tried to allocate 372.00 MiB (GPU 6; 23.70 GiB total capacity; 19.82 GiB already allocated; 257.69 MiB free; 21.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1365, Reason: CUDA out of memory. Tried to allocate 590.00 MiB (GPU 6; 23.70 GiB total capacity; 20.22 GiB already allocated; 267.69 MiB free; 21.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1382, Reason: CUDA out of memory. Tried to allocate 446.00 MiB (GPU 6; 23.70 GiB total capacity; 20.59 GiB already allocated; 263.69 MiB free; 21.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1406, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 20.22 GiB already allocated; 167.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1430, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 20.09 GiB already allocated; 59.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1492, Reason: CUDA out of memory. Tried to allocate 426.00 MiB (GPU 6; 23.70 GiB total capacity; 19.33 GiB already allocated; 117.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1528, Reason: CUDA out of memory. Tried to allocate 426.00 MiB (GPU 6; 23.70 GiB total capacity; 16.63 GiB already allocated; 381.69 MiB free; 21.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1596, Reason: CUDA out of memory. Tried to allocate 372.00 MiB (GPU 6; 23.70 GiB total capacity; 18.63 GiB already allocated; 73.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1624, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 20.38 GiB already allocated; 109.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1627, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 20.66 GiB already allocated; 43.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1671, Reason: CUDA out of memory. Tried to allocate 320.00 MiB (GPU 6; 23.70 GiB total capacity; 20.23 GiB already allocated; 269.69 MiB free; 21.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1707, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 19.92 GiB already allocated; 27.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1822, Reason: CUDA out of memory. Tried to allocate 426.00 MiB (GPU 6; 23.70 GiB total capacity; 18.71 GiB already allocated; 357.69 MiB free; 21.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1873, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 19.04 GiB already allocated; 29.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1895, Reason: CUDA out of memory. Tried to allocate 638.00 MiB (GPU 6; 23.70 GiB total capacity; 19.82 GiB already allocated; 627.69 MiB free; 21.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1899, Reason: CUDA out of memory. Tried to allocate 2.08 GiB (GPU 6; 23.70 GiB total capacity; 18.35 GiB already allocated; 2.00 GiB free; 19.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1955, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 19.99 GiB already allocated; 61.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2018, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 20.63 GiB already allocated; 45.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2047, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 18.93 GiB already allocated; 81.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2100, Reason: CUDA out of memory. Tried to allocate 818.00 MiB (GPU 6; 23.70 GiB total capacity; 19.88 GiB already allocated; 511.69 MiB free; 21.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2160, Reason: CUDA out of memory. Tried to allocate 546.00 MiB (GPU 6; 23.70 GiB total capacity; 18.69 GiB already allocated; 345.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2212, Reason: CUDA out of memory. Tried to allocate 372.00 MiB (GPU 6; 23.70 GiB total capacity; 17.86 GiB already allocated; 245.69 MiB free; 21.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2219, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 20.65 GiB already allocated; 47.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2243, Reason: CUDA out of memory. Tried to allocate 428.00 MiB (GPU 6; 23.70 GiB total capacity; 16.94 GiB already allocated; 383.69 MiB free; 21.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2292, Reason: CUDA out of memory. Tried to allocate 352.00 MiB (GPU 6; 23.70 GiB total capacity; 19.87 GiB already allocated; 267.69 MiB free; 21.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2293, Reason: CUDA out of memory. Tried to allocate 1002.00 MiB (GPU 6; 23.70 GiB total capacity; 19.33 GiB already allocated; 233.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2323, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 17.74 GiB already allocated; 77.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2359, Reason: CUDA out of memory. Tried to allocate 446.00 MiB (GPU 6; 23.70 GiB total capacity; 19.44 GiB already allocated; 385.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2443, Reason: CUDA out of memory. Tried to allocate 446.00 MiB (GPU 6; 23.70 GiB total capacity; 20.35 GiB already allocated; 269.69 MiB free; 21.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2467, Reason: CUDA out of memory. Tried to allocate 650.00 MiB (GPU 6; 23.70 GiB total capacity; 20.56 GiB already allocated; 295.69 MiB free; 21.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2477, Reason: CUDA out of memory. Tried to allocate 426.00 MiB (GPU 6; 23.70 GiB total capacity; 19.17 GiB already allocated; 363.69 MiB free; 21.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2533, Reason: CUDA out of memory. Tried to allocate 426.00 MiB (GPU 6; 23.70 GiB total capacity; 19.64 GiB already allocated; 349.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2565, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 20.39 GiB already allocated; 21.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2577, Reason: CUDA out of memory. Tried to allocate 568.00 MiB (GPU 6; 23.70 GiB total capacity; 18.59 GiB already allocated; 531.69 MiB free; 21.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2673, Reason: CUDA out of memory. Tried to allocate 372.00 MiB (GPU 6; 23.70 GiB total capacity; 18.69 GiB already allocated; 221.69 MiB free; 21.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2729, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 20.27 GiB already allocated; 23.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2743, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 20.70 GiB already allocated; 99.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2797, Reason: CUDA out of memory. Tried to allocate 334.00 MiB (GPU 6; 23.70 GiB total capacity; 18.94 GiB already allocated; 273.69 MiB free; 21.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2803, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 19.97 GiB already allocated; 61.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2816, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 18.92 GiB already allocated; 177.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2900, Reason: CUDA out of memory. Tried to allocate 320.00 MiB (GPU 6; 23.70 GiB total capacity; 20.02 GiB already allocated; 295.69 MiB free; 21.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2906, Reason: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 6; 23.70 GiB total capacity; 15.90 GiB already allocated; 117.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3085, Reason: CUDA out of memory. Tried to allocate 426.00 MiB (GPU 6; 23.70 GiB total capacity; 19.88 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3092, Reason: CUDA out of memory. Tried to allocate 372.00 MiB (GPU 6; 23.70 GiB total capacity; 19.21 GiB already allocated; 7.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3139, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 20.20 GiB already allocated; 21.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3158, Reason: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 6; 23.70 GiB total capacity; 19.87 GiB already allocated; 1.30 GiB free; 20.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3246, Reason: CUDA out of memory. Tried to allocate 534.00 MiB (GPU 6; 23.70 GiB total capacity; 19.83 GiB already allocated; 115.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3253, Reason: CUDA out of memory. Tried to allocate 1.25 GiB (GPU 6; 23.70 GiB total capacity; 15.17 GiB already allocated; 317.69 MiB free; 21.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3276, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 20.53 GiB already allocated; 29.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3290, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 20.29 GiB already allocated; 171.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3419, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 19.55 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3455, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 18.57 GiB already allocated; 17.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3469, Reason: CUDA out of memory. Tried to allocate 1.14 GiB (GPU 6; 23.70 GiB total capacity; 20.81 GiB already allocated; 109.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3487, Reason: CUDA out of memory. Tried to allocate 820.00 MiB (GPU 6; 23.70 GiB total capacity; 15.14 GiB already allocated; 295.69 MiB free; 21.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3573, Reason: CUDA out of memory. Tried to allocate 320.00 MiB (GPU 6; 23.70 GiB total capacity; 20.20 GiB already allocated; 121.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3632, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 20.39 GiB already allocated; 9.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3662, Reason: CUDA out of memory. Tried to allocate 372.00 MiB (GPU 6; 23.70 GiB total capacity; 19.60 GiB already allocated; 269.69 MiB free; 21.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3693, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 19.60 GiB already allocated; 61.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3709, Reason: CUDA out of memory. Tried to allocate 2.08 GiB (GPU 6; 23.70 GiB total capacity; 17.91 GiB already allocated; 727.69 MiB free; 21.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3750, Reason: CUDA out of memory. Tried to allocate 584.00 MiB (GPU 6; 23.70 GiB total capacity; 19.79 GiB already allocated; 407.69 MiB free; 21.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3912, Reason: CUDA out of memory. Tried to allocate 426.00 MiB (GPU 6; 23.70 GiB total capacity; 19.53 GiB already allocated; 285.69 MiB free; 21.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3972, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 20.47 GiB already allocated; 23.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3990, Reason: CUDA out of memory. Tried to allocate 2.08 GiB (GPU 6; 23.70 GiB total capacity; 18.79 GiB already allocated; 1.57 GiB free; 20.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4057, Reason: CUDA out of memory. Tried to allocate 534.00 MiB (GPU 6; 23.70 GiB total capacity; 20.08 GiB already allocated; 295.69 MiB free; 21.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4068, Reason: CUDA out of memory. Tried to allocate 728.00 MiB (GPU 6; 23.70 GiB total capacity; 19.84 GiB already allocated; 283.69 MiB free; 21.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4222, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 19.51 GiB already allocated; 79.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
Skipping evaluation batch 35, Reason: CUDA out of memory. Tried to allocate 556.00 MiB (GPU 6; 23.70 GiB total capacity; 18.17 GiB already allocated; 363.69 MiB free; 21.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 42, Reason: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 6; 23.70 GiB total capacity; 19.90 GiB already allocated; 51.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 173, Reason: CUDA out of memory. Tried to allocate 746.00 MiB (GPU 6; 23.70 GiB total capacity; 19.37 GiB already allocated; 279.69 MiB free; 21.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 185, Reason: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 6; 23.70 GiB total capacity; 19.01 GiB already allocated; 119.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 277, Reason: CUDA out of memory. Tried to allocate 396.00 MiB (GPU 6; 23.70 GiB total capacity; 18.87 GiB already allocated; 157.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 336, Reason: CUDA out of memory. Tried to allocate 396.00 MiB (GPU 6; 23.70 GiB total capacity; 18.10 GiB already allocated; 161.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 409, Reason: CUDA out of memory. Tried to allocate 488.00 MiB (GPU 6; 23.70 GiB total capacity; 18.80 GiB already allocated; 153.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 515, Reason: CUDA out of memory. Tried to allocate 488.00 MiB (GPU 6; 23.70 GiB total capacity; 18.65 GiB already allocated; 105.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 646, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 18.65 GiB already allocated; 91.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 683, Reason: CUDA out of memory. Tried to allocate 476.00 MiB (GPU 6; 23.70 GiB total capacity; 20.02 GiB already allocated; 157.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 748, Reason: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 6; 23.70 GiB total capacity; 20.77 GiB already allocated; 25.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 786, Reason: CUDA out of memory. Tried to allocate 1.86 GiB (GPU 6; 23.70 GiB total capacity; 15.48 GiB already allocated; 1.61 GiB free; 20.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 912, Reason: CUDA out of memory. Tried to allocate 652.00 MiB (GPU 6; 23.70 GiB total capacity; 18.25 GiB already allocated; 555.69 MiB free; 21.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 921, Reason: CUDA out of memory. Tried to allocate 906.00 MiB (GPU 6; 23.70 GiB total capacity; 17.87 GiB already allocated; 823.69 MiB free; 21.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1067, Reason: CUDA out of memory. Tried to allocate 396.00 MiB (GPU 6; 23.70 GiB total capacity; 20.28 GiB already allocated; 97.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1122, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 20.41 GiB already allocated; 7.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1328, Reason: CUDA out of memory. Tried to allocate 3.13 GiB (GPU 6; 23.70 GiB total capacity; 16.06 GiB already allocated; 2.57 GiB free; 19.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1461, Reason: CUDA out of memory. Tried to allocate 134.00 MiB (GPU 6; 23.70 GiB total capacity; 20.59 GiB already allocated; 101.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1477, Reason: CUDA out of memory. Tried to allocate 476.00 MiB (GPU 6; 23.70 GiB total capacity; 18.07 GiB already allocated; 379.69 MiB free; 21.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1604, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 20.64 GiB already allocated; 17.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1637, Reason: CUDA out of memory. Tried to allocate 522.00 MiB (GPU 6; 23.70 GiB total capacity; 19.51 GiB already allocated; 343.69 MiB free; 21.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1651, Reason: CUDA out of memory. Tried to allocate 396.00 MiB (GPU 6; 23.70 GiB total capacity; 19.19 GiB already allocated; 289.69 MiB free; 21.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1664, Reason: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 6; 23.70 GiB total capacity; 17.91 GiB already allocated; 65.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1739, Reason: CUDA out of memory. Tried to allocate 476.00 MiB (GPU 6; 23.70 GiB total capacity; 17.94 GiB already allocated; 233.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1753, Reason: CUDA out of memory. Tried to allocate 556.00 MiB (GPU 6; 23.70 GiB total capacity; 18.63 GiB already allocated; 339.69 MiB free; 21.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1762, Reason: CUDA out of memory. Tried to allocate 520.00 MiB (GPU 6; 23.70 GiB total capacity; 19.09 GiB already allocated; 187.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1825, Reason: CUDA out of memory. Tried to allocate 520.00 MiB (GPU 6; 23.70 GiB total capacity; 18.75 GiB already allocated; 175.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1849, Reason: CUDA out of memory. Tried to allocate 354.00 MiB (GPU 6; 23.70 GiB total capacity; 19.59 GiB already allocated; 153.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2042, Reason: CUDA out of memory. Tried to allocate 364.00 MiB (GPU 6; 23.70 GiB total capacity; 20.77 GiB already allocated; 49.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2054, Reason: CUDA out of memory. Tried to allocate 260.00 MiB (GPU 6; 23.70 GiB total capacity; 19.17 GiB already allocated; 121.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2070, Reason: CUDA out of memory. Tried to allocate 3.13 GiB (GPU 6; 23.70 GiB total capacity; 18.37 GiB already allocated; 1.29 GiB free; 20.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2095, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 18.50 GiB already allocated; 31.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2106, Reason: CUDA out of memory. Tried to allocate 488.00 MiB (GPU 6; 23.70 GiB total capacity; 19.49 GiB already allocated; 447.69 MiB free; 21.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2216, Reason: CUDA out of memory. Tried to allocate 420.00 MiB (GPU 6; 23.70 GiB total capacity; 19.62 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2384, Reason: CUDA out of memory. Tried to allocate 476.00 MiB (GPU 6; 23.70 GiB total capacity; 17.84 GiB already allocated; 469.69 MiB free; 21.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2482, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 19.90 GiB already allocated; 51.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2659, Reason: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 6; 23.70 GiB total capacity; 20.41 GiB already allocated; 73.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2680, Reason: CUDA out of memory. Tried to allocate 3.13 GiB (GPU 6; 23.70 GiB total capacity; 18.79 GiB already allocated; 2.03 GiB free; 19.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2703, Reason: CUDA out of memory. Tried to allocate 746.00 MiB (GPU 6; 23.70 GiB total capacity; 19.86 GiB already allocated; 309.69 MiB free; 21.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2706, Reason: CUDA out of memory. Tried to allocate 520.00 MiB (GPU 6; 23.70 GiB total capacity; 19.66 GiB already allocated; 281.69 MiB free; 21.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2716, Reason: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 6; 23.70 GiB total capacity; 19.91 GiB already allocated; 99.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2727, Reason: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 6; 23.70 GiB total capacity; 18.35 GiB already allocated; 75.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2805, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 20.23 GiB already allocated; 37.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
Skipping evaluation batch 84, Reason: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 6; 23.70 GiB total capacity; 19.07 GiB already allocated; 63.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 153, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 20.65 GiB already allocated; 75.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 185, Reason: CUDA out of memory. Tried to allocate 134.00 MiB (GPU 6; 23.70 GiB total capacity; 20.48 GiB already allocated; 61.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 325, Reason: CUDA out of memory. Tried to allocate 354.00 MiB (GPU 6; 23.70 GiB total capacity; 17.83 GiB already allocated; 163.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 410, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 20.18 GiB already allocated; 79.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 461, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 18.89 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 493, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 18.72 GiB already allocated; 11.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 502, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 20.11 GiB already allocated; 347.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 577, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 19.58 GiB already allocated; 9.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 583, Reason: CUDA out of memory. Tried to allocate 354.00 MiB (GPU 6; 23.70 GiB total capacity; 19.37 GiB already allocated; 25.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 587, Reason: CUDA out of memory. Tried to allocate 530.00 MiB (GPU 6; 23.70 GiB total capacity; 18.00 GiB already allocated; 471.69 MiB free; 21.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 709, Reason: CUDA out of memory. Tried to allocate 354.00 MiB (GPU 6; 23.70 GiB total capacity; 19.47 GiB already allocated; 185.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 757, Reason: CUDA out of memory. Tried to allocate 134.00 MiB (GPU 6; 23.70 GiB total capacity; 18.16 GiB already allocated; 81.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 804, Reason: CUDA out of memory. Tried to allocate 1.22 GiB (GPU 6; 23.70 GiB total capacity; 20.80 GiB already allocated; 415.69 MiB free; 21.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 870, Reason: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 6; 23.70 GiB total capacity; 19.19 GiB already allocated; 179.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 912, Reason: CUDA out of memory. Tried to allocate 636.00 MiB (GPU 6; 23.70 GiB total capacity; 19.28 GiB already allocated; 361.69 MiB free; 21.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1125, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 20.43 GiB already allocated; 57.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1232, Reason: CUDA out of memory. Tried to allocate 410.00 MiB (GPU 6; 23.70 GiB total capacity; 20.51 GiB already allocated; 97.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1299, Reason: CUDA out of memory. Tried to allocate 558.00 MiB (GPU 6; 23.70 GiB total capacity; 19.04 GiB already allocated; 179.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1404, Reason: CUDA out of memory. Tried to allocate 530.00 MiB (GPU 6; 23.70 GiB total capacity; 15.50 GiB already allocated; 483.69 MiB free; 21.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1456, Reason: CUDA out of memory. Tried to allocate 520.00 MiB (GPU 6; 23.70 GiB total capacity; 20.10 GiB already allocated; 413.69 MiB free; 21.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1470, Reason: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 6; 23.70 GiB total capacity; 17.40 GiB already allocated; 27.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1513, Reason: CUDA out of memory. Tried to allocate 1.10 GiB (GPU 6; 23.70 GiB total capacity; 20.14 GiB already allocated; 701.69 MiB free; 21.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1528, Reason: CUDA out of memory. Tried to allocate 312.00 MiB (GPU 6; 23.70 GiB total capacity; 19.14 GiB already allocated; 165.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1530, Reason: CUDA out of memory. Tried to allocate 354.00 MiB (GPU 6; 23.70 GiB total capacity; 19.94 GiB already allocated; 231.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1610, Reason: CUDA out of memory. Tried to allocate 636.00 MiB (GPU 6; 23.70 GiB total capacity; 20.02 GiB already allocated; 197.69 MiB free; 21.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1645, Reason: CUDA out of memory. Tried to allocate 410.00 MiB (GPU 6; 23.70 GiB total capacity; 20.91 GiB already allocated; 155.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1662, Reason: CUDA out of memory. Tried to allocate 520.00 MiB (GPU 6; 23.70 GiB total capacity; 18.98 GiB already allocated; 91.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1747, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 20.23 GiB already allocated; 163.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1757, Reason: CUDA out of memory. Tried to allocate 520.00 MiB (GPU 6; 23.70 GiB total capacity; 18.88 GiB already allocated; 159.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1785, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 20.03 GiB already allocated; 231.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1788, Reason: CUDA out of memory. Tried to allocate 1.04 GiB (GPU 6; 23.70 GiB total capacity; 20.02 GiB already allocated; 307.69 MiB free; 21.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1798, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 20.41 GiB already allocated; 27.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1829, Reason: CUDA out of memory. Tried to allocate 520.00 MiB (GPU 6; 23.70 GiB total capacity; 19.67 GiB already allocated; 365.69 MiB free; 21.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1923, Reason: CUDA out of memory. Tried to allocate 410.00 MiB (GPU 6; 23.70 GiB total capacity; 19.09 GiB already allocated; 407.69 MiB free; 21.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1964, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 18.81 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2115, Reason: CUDA out of memory. Tried to allocate 354.00 MiB (GPU 6; 23.70 GiB total capacity; 17.63 GiB already allocated; 337.69 MiB free; 21.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2127, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 18.80 GiB already allocated; 139.69 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2185, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 19.52 GiB already allocated; 29.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2191, Reason: CUDA out of memory. Tried to allocate 1.22 GiB (GPU 6; 23.70 GiB total capacity; 19.81 GiB already allocated; 653.69 MiB free; 21.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2204, Reason: CUDA out of memory. Tried to allocate 410.00 MiB (GPU 6; 23.70 GiB total capacity; 19.46 GiB already allocated; 393.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2382, Reason: CUDA out of memory. Tried to allocate 538.00 MiB (GPU 6; 23.70 GiB total capacity; 20.54 GiB already allocated; 169.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2402, Reason: CUDA out of memory. Tried to allocate 728.00 MiB (GPU 6; 23.70 GiB total capacity; 17.83 GiB already allocated; 615.69 MiB free; 21.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2425, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 20.42 GiB already allocated; 295.69 MiB free; 21.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2483, Reason: CUDA out of memory. Tried to allocate 2.84 GiB (GPU 6; 23.70 GiB total capacity; 12.48 GiB already allocated; 307.69 MiB free; 21.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2505, Reason: CUDA out of memory. Tried to allocate 1.04 GiB (GPU 6; 23.70 GiB total capacity; 19.57 GiB already allocated; 475.69 MiB free; 21.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2530, Reason: CUDA out of memory. Tried to allocate 1.20 GiB (GPU 6; 23.70 GiB total capacity; 20.94 GiB already allocated; 161.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2534, Reason: CUDA out of memory. Tried to allocate 728.00 MiB (GPU 6; 23.70 GiB total capacity; 17.67 GiB already allocated; 213.69 MiB free; 21.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2558, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 21.25 GiB already allocated; 85.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2564, Reason: CUDA out of memory. Tried to allocate 2.84 GiB (GPU 6; 23.70 GiB total capacity; 14.03 GiB already allocated; 1.30 GiB free; 20.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2570, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 20.97 GiB already allocated; 71.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2588, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 20.21 GiB already allocated; 57.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2635, Reason: CUDA out of memory. Tried to allocate 312.00 MiB (GPU 6; 23.70 GiB total capacity; 19.64 GiB already allocated; 145.69 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2700, Reason: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 6; 23.70 GiB total capacity; 19.85 GiB already allocated; 85.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2728, Reason: CUDA out of memory. Tried to allocate 354.00 MiB (GPU 6; 23.70 GiB total capacity; 19.58 GiB already allocated; 123.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2740, Reason: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 6; 23.70 GiB total capacity; 18.34 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2790, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 19.99 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
Skipping evaluation batch 40, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 20.58 GiB already allocated; 69.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 54, Reason: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 6; 23.70 GiB total capacity; 20.79 GiB already allocated; 71.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 122, Reason: CUDA out of memory. Tried to allocate 324.00 MiB (GPU 6; 23.70 GiB total capacity; 18.86 GiB already allocated; 179.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 127, Reason: CUDA out of memory. Tried to allocate 742.00 MiB (GPU 6; 23.70 GiB total capacity; 19.60 GiB already allocated; 81.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 143, Reason: CUDA out of memory. Tried to allocate 636.00 MiB (GPU 6; 23.70 GiB total capacity; 19.16 GiB already allocated; 91.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 146, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 20.31 GiB already allocated; 335.69 MiB free; 21.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 166, Reason: CUDA out of memory. Tried to allocate 552.00 MiB (GPU 6; 23.70 GiB total capacity; 18.86 GiB already allocated; 515.69 MiB free; 21.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 175, Reason: CUDA out of memory. Tried to allocate 948.00 MiB (GPU 6; 23.70 GiB total capacity; 16.88 GiB already allocated; 757.69 MiB free; 21.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 243, Reason: CUDA out of memory. Tried to allocate 552.00 MiB (GPU 6; 23.70 GiB total capacity; 19.60 GiB already allocated; 185.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 262, Reason: CUDA out of memory. Tried to allocate 552.00 MiB (GPU 6; 23.70 GiB total capacity; 19.54 GiB already allocated; 173.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 288, Reason: CUDA out of memory. Tried to allocate 324.00 MiB (GPU 6; 23.70 GiB total capacity; 19.17 GiB already allocated; 133.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 370, Reason: CUDA out of memory. Tried to allocate 336.00 MiB (GPU 6; 23.70 GiB total capacity; 18.91 GiB already allocated; 143.69 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 384, Reason: CUDA out of memory. Tried to allocate 324.00 MiB (GPU 6; 23.70 GiB total capacity; 18.72 GiB already allocated; 203.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 430, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 20.00 GiB already allocated; 69.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 443, Reason: CUDA out of memory. Tried to allocate 910.00 MiB (GPU 6; 23.70 GiB total capacity; 17.54 GiB already allocated; 829.69 MiB free; 21.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 448, Reason: CUDA out of memory. Tried to allocate 552.00 MiB (GPU 6; 23.70 GiB total capacity; 19.77 GiB already allocated; 83.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 472, Reason: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 6; 23.70 GiB total capacity; 20.67 GiB already allocated; 145.69 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 476, Reason: CUDA out of memory. Tried to allocate 1.09 GiB (GPU 6; 23.70 GiB total capacity; 18.99 GiB already allocated; 33.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 493, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 20.89 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 497, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 18.96 GiB already allocated; 51.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 500, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 21.22 GiB already allocated; 73.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 525, Reason: CUDA out of memory. Tried to allocate 434.00 MiB (GPU 6; 23.70 GiB total capacity; 18.62 GiB already allocated; 25.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 557, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 18.28 GiB already allocated; 31.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 565, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 20.37 GiB already allocated; 13.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 657, Reason: CUDA out of memory. Tried to allocate 280.00 MiB (GPU 6; 23.70 GiB total capacity; 20.15 GiB already allocated; 11.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 677, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 19.74 GiB already allocated; 77.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 771, Reason: CUDA out of memory. Tried to allocate 380.00 MiB (GPU 6; 23.70 GiB total capacity; 18.79 GiB already allocated; 239.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 797, Reason: CUDA out of memory. Tried to allocate 1.22 GiB (GPU 6; 23.70 GiB total capacity; 20.76 GiB already allocated; 393.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 852, Reason: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 6; 23.70 GiB total capacity; 19.92 GiB already allocated; 133.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 861, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 20.75 GiB already allocated; 63.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 885, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 20.64 GiB already allocated; 33.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 905, Reason: CUDA out of memory. Tried to allocate 824.00 MiB (GPU 6; 23.70 GiB total capacity; 18.41 GiB already allocated; 199.69 MiB free; 21.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 914, Reason: CUDA out of memory. Tried to allocate 482.00 MiB (GPU 6; 23.70 GiB total capacity; 19.55 GiB already allocated; 221.69 MiB free; 21.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 917, Reason: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 6; 23.70 GiB total capacity; 17.86 GiB already allocated; 183.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 951, Reason: CUDA out of memory. Tried to allocate 932.00 MiB (GPU 6; 23.70 GiB total capacity; 18.75 GiB already allocated; 311.69 MiB free; 21.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 976, Reason: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 6; 23.70 GiB total capacity; 20.92 GiB already allocated; 95.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 994, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 20.55 GiB already allocated; 23.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1060, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 19.73 GiB already allocated; 81.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1117, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 17.80 GiB already allocated; 57.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1123, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 19.21 GiB already allocated; 207.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1169, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 18.68 GiB already allocated; 65.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1191, Reason: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 6; 23.70 GiB total capacity; 20.84 GiB already allocated; 157.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1216, Reason: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 6; 23.70 GiB total capacity; 12.85 GiB already allocated; 187.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1220, Reason: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 6; 23.70 GiB total capacity; 20.17 GiB already allocated; 21.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1226, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 19.87 GiB already allocated; 69.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1228, Reason: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 6; 23.70 GiB total capacity; 16.04 GiB already allocated; 81.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1319, Reason: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 6; 23.70 GiB total capacity; 19.91 GiB already allocated; 15.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1368, Reason: CUDA out of memory. Tried to allocate 280.00 MiB (GPU 6; 23.70 GiB total capacity; 20.14 GiB already allocated; 225.69 MiB free; 21.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1412, Reason: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 6; 23.70 GiB total capacity; 17.33 GiB already allocated; 77.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1454, Reason: CUDA out of memory. Tried to allocate 1.09 GiB (GPU 6; 23.70 GiB total capacity; 20.36 GiB already allocated; 769.69 MiB free; 21.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1481, Reason: CUDA out of memory. Tried to allocate 552.00 MiB (GPU 6; 23.70 GiB total capacity; 20.42 GiB already allocated; 33.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1498, Reason: CUDA out of memory. Tried to allocate 324.00 MiB (GPU 6; 23.70 GiB total capacity; 20.23 GiB already allocated; 15.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1509, Reason: CUDA out of memory. Tried to allocate 312.00 MiB (GPU 6; 23.70 GiB total capacity; 20.26 GiB already allocated; 297.69 MiB free; 21.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1525, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 18.23 GiB already allocated; 117.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1583, Reason: CUDA out of memory. Tried to allocate 636.00 MiB (GPU 6; 23.70 GiB total capacity; 18.00 GiB already allocated; 399.69 MiB free; 21.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1618, Reason: CUDA out of memory. Tried to allocate 742.00 MiB (GPU 6; 23.70 GiB total capacity; 18.50 GiB already allocated; 721.69 MiB free; 21.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1631, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 18.44 GiB already allocated; 49.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1632, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 18.44 GiB already allocated; 49.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1634, Reason: CUDA out of memory. Tried to allocate 380.00 MiB (GPU 6; 23.70 GiB total capacity; 19.84 GiB already allocated; 79.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1635, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 19.76 GiB already allocated; 79.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1735, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 18.46 GiB already allocated; 9.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1742, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 20.85 GiB already allocated; 31.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1755, Reason: CUDA out of memory. Tried to allocate 742.00 MiB (GPU 6; 23.70 GiB total capacity; 19.74 GiB already allocated; 77.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1780, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 20.49 GiB already allocated; 91.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1795, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 20.41 GiB already allocated; 89.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1830, Reason: CUDA out of memory. Tried to allocate 642.00 MiB (GPU 6; 23.70 GiB total capacity; 20.44 GiB already allocated; 45.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1843, Reason: CUDA out of memory. Tried to allocate 742.00 MiB (GPU 6; 23.70 GiB total capacity; 17.59 GiB already allocated; 445.69 MiB free; 21.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1895, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 16.31 GiB already allocated; 61.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1926, Reason: CUDA out of memory. Tried to allocate 380.00 MiB (GPU 6; 23.70 GiB total capacity; 20.02 GiB already allocated; 157.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1970, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 21.18 GiB already allocated; 89.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2068, Reason: CUDA out of memory. Tried to allocate 434.00 MiB (GPU 6; 23.70 GiB total capacity; 19.50 GiB already allocated; 353.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2077, Reason: CUDA out of memory. Tried to allocate 308.00 MiB (GPU 6; 23.70 GiB total capacity; 19.75 GiB already allocated; 27.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2081, Reason: CUDA out of memory. Tried to allocate 482.00 MiB (GPU 6; 23.70 GiB total capacity; 20.05 GiB already allocated; 455.69 MiB free; 21.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2116, Reason: CUDA out of memory. Tried to allocate 760.00 MiB (GPU 6; 23.70 GiB total capacity; 15.65 GiB already allocated; 293.69 MiB free; 21.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2119, Reason: CUDA out of memory. Tried to allocate 552.00 MiB (GPU 6; 23.70 GiB total capacity; 20.31 GiB already allocated; 249.69 MiB free; 21.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2126, Reason: CUDA out of memory. Tried to allocate 922.00 MiB (GPU 6; 23.70 GiB total capacity; 17.05 GiB already allocated; 275.69 MiB free; 21.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2134, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 17.97 GiB already allocated; 53.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2145, Reason: CUDA out of memory. Tried to allocate 386.00 MiB (GPU 6; 23.70 GiB total capacity; 17.50 GiB already allocated; 313.69 MiB free; 21.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2150, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 18.93 GiB already allocated; 49.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2190, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 18.52 GiB already allocated; 17.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2199, Reason: CUDA out of memory. Tried to allocate 1.22 GiB (GPU 6; 23.70 GiB total capacity; 19.82 GiB already allocated; 957.69 MiB free; 20.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2252, Reason: CUDA out of memory. Tried to allocate 482.00 MiB (GPU 6; 23.70 GiB total capacity; 18.23 GiB already allocated; 461.69 MiB free; 21.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2264, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 19.77 GiB already allocated; 69.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2278, Reason: CUDA out of memory. Tried to allocate 362.00 MiB (GPU 6; 23.70 GiB total capacity; 19.04 GiB already allocated; 275.69 MiB free; 21.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2370, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 18.72 GiB already allocated; 87.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2390, Reason: CUDA out of memory. Tried to allocate 910.00 MiB (GPU 6; 23.70 GiB total capacity; 19.04 GiB already allocated; 409.69 MiB free; 21.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2392, Reason: CUDA out of memory. Tried to allocate 362.00 MiB (GPU 6; 23.70 GiB total capacity; 18.64 GiB already allocated; 69.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2401, Reason: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 6; 23.70 GiB total capacity; 20.57 GiB already allocated; 35.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2406, Reason: CUDA out of memory. Tried to allocate 1.09 GiB (GPU 6; 23.70 GiB total capacity; 20.75 GiB already allocated; 75.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2432, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 19.96 GiB already allocated; 35.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2456, Reason: CUDA out of memory. Tried to allocate 636.00 MiB (GPU 6; 23.70 GiB total capacity; 18.92 GiB already allocated; 617.69 MiB free; 21.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2516, Reason: CUDA out of memory. Tried to allocate 280.00 MiB (GPU 6; 23.70 GiB total capacity; 20.20 GiB already allocated; 115.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2542, Reason: CUDA out of memory. Tried to allocate 380.00 MiB (GPU 6; 23.70 GiB total capacity; 19.61 GiB already allocated; 113.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2547, Reason: CUDA out of memory. Tried to allocate 280.00 MiB (GPU 6; 23.70 GiB total capacity; 18.97 GiB already allocated; 279.69 MiB free; 21.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2553, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 20.07 GiB already allocated; 59.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2555, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 18.03 GiB already allocated; 237.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2609, Reason: CUDA out of memory. Tried to allocate 312.00 MiB (GPU 6; 23.70 GiB total capacity; 20.46 GiB already allocated; 101.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2632, Reason: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 6; 23.70 GiB total capacity; 19.85 GiB already allocated; 123.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2680, Reason: CUDA out of memory. Tried to allocate 434.00 MiB (GPU 6; 23.70 GiB total capacity; 18.57 GiB already allocated; 111.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2695, Reason: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 6; 23.70 GiB total capacity; 19.82 GiB already allocated; 3.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2723, Reason: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 6; 23.70 GiB total capacity; 20.25 GiB already allocated; 79.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2733, Reason: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 6; 23.70 GiB total capacity; 20.91 GiB already allocated; 179.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2746, Reason: CUDA out of memory. Tried to allocate 552.00 MiB (GPU 6; 23.70 GiB total capacity; 18.72 GiB already allocated; 157.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2752, Reason: CUDA out of memory. Tried to allocate 386.00 MiB (GPU 6; 23.70 GiB total capacity; 15.92 GiB already allocated; 129.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2830, Reason: CUDA out of memory. Tried to allocate 386.00 MiB (GPU 6; 23.70 GiB total capacity; 15.09 GiB already allocated; 205.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
