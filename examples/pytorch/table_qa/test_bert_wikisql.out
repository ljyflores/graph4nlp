[ Using CUDA ]
{'tokenizer': <function tokenize_jobs at 0x7f9389092310>, 'pretrained_word_emb_name': '6B', 'pretrained_word_emb_url': None, 'pretrained_word_emb_cache_dir': None, 'merge_strategy': 'tailhead', 'edge_strategy': 'heterogeneous', 'seed': None, 'word_emb_size': 300, 'thread_number': 1, 'port': 9000}
Using Table2Text read_raw_data
Found:  56355  items
Loaded SQL
Found:  11276  items
Found:  15878  items
Found:  8421  items
Found:  8421  items
Found:  8421  items
table _process works!
Port 9000, processing: 0 / 56355
Port 9000, processing: 1000 / 56355
Port 9000, processing: 2000 / 56355
Port 9000, processing: 3000 / 56355
Port 9000, processing: 4000 / 56355
Port 9000, processing: 5000 / 56355
Port 9000, processing: 6000 / 56355
Port 9000, processing: 7000 / 56355
Port 9000, processing: 8000 / 56355
Port 9000, processing: 9000 / 56355
Port 9000, processing: 10000 / 56355
Port 9000, processing: 11000 / 56355
Port 9000, processing: 12000 / 56355
Port 9000, processing: 13000 / 56355
Port 9000, processing: 14000 / 56355
Port 9000, processing: 15000 / 56355
Port 9000, processing: 16000 / 56355
Port 9000, processing: 17000 / 56355
Port 9000, processing: 18000 / 56355
Port 9000, processing: 19000 / 56355
Port 9000, processing: 20000 / 56355
Port 9000, processing: 21000 / 56355
Port 9000, processing: 22000 / 56355
Port 9000, processing: 23000 / 56355
Port 9000, processing: 24000 / 56355
Port 9000, processing: 25000 / 56355
Port 9000, processing: 26000 / 56355
Port 9000, processing: 27000 / 56355
Port 9000, processing: 28000 / 56355
Port 9000, processing: 29000 / 56355
Port 9000, processing: 30000 / 56355
Port 9000, processing: 31000 / 56355
Port 9000, processing: 32000 / 56355
Port 9000, processing: 33000 / 56355
Port 9000, processing: 34000 / 56355
Port 9000, processing: 35000 / 56355
Port 9000, processing: 36000 / 56355
Port 9000, processing: 37000 / 56355
Port 9000, processing: 38000 / 56355
Port 9000, processing: 39000 / 56355
Port 9000, processing: 40000 / 56355
Port 9000, processing: 41000 / 56355
Port 9000, processing: 42000 / 56355
Port 9000, processing: 43000 / 56355
Port 9000, processing: 44000 / 56355
Port 9000, processing: 45000 / 56355
Port 9000, processing: 46000 / 56355
Port 9000, processing: 47000 / 56355
Port 9000, processing: 48000 / 56355
Port 9000, processing: 49000 / 56355
Port 9000, processing: 50000 / 56355
Port 9000, processing: 51000 / 56355
Port 9000, processing: 52000 / 56355
Port 9000, processing: 53000 / 56355
Port 9000, processing: 54000 / 56355
Port 9000, processing: 55000 / 56355
Port 9000, processing: 56000 / 56355
Port 9000, processing: 0 / 11276
Port 9000, processing: 1000 / 11276
Port 9000, processing: 2000 / 11276
Port 9000, processing: 3000 / 11276
Port 9000, processing: 4000 / 11276
Port 9000, processing: 5000 / 11276
Port 9000, processing: 6000 / 11276
Port 9000, processing: 7000 / 11276
Port 9000, processing: 8000 / 11276
Port 9000, processing: 9000 / 11276
Port 9000, processing: 10000 / 11276
Port 9000, processing: 11000 / 11276
Port 9000, processing: 0 / 15878
Port 9000, processing: 1000 / 15878
Port 9000, processing: 2000 / 15878
Port 9000, processing: 3000 / 15878
Port 9000, processing: 4000 / 15878
Port 9000, processing: 5000 / 15878
Port 9000, processing: 6000 / 15878
Port 9000, processing: 7000 / 15878
Port 9000, processing: 8000 / 15878
Port 9000, processing: 9000 / 15878
Port 9000, processing: 10000 / 15878
Port 9000, processing: 11000 / 15878
Port 9000, processing: 12000 / 15878
Port 9000, processing: 13000 / 15878
Port 9000, processing: 14000 / 15878
Port 9000, processing: 15000 / 15878
Port 9000, processing: 0 / 8421
Port 9000, processing: 1000 / 8421
Port 9000, processing: 2000 / 8421
Port 9000, processing: 3000 / 8421
Port 9000, processing: 4000 / 8421
Port 9000, processing: 5000 / 8421
Port 9000, processing: 6000 / 8421
Port 9000, processing: 7000 / 8421
Port 9000, processing: 8000 / 8421
Port 9000, processing: 0 / 8421
Port 9000, processing: 1000 / 8421
Port 9000, processing: 2000 / 8421
Port 9000, processing: 3000 / 8421
Port 9000, processing: 4000 / 8421
Port 9000, processing: 5000 / 8421
Port 9000, processing: 6000 / 8421
Port 9000, processing: 7000 / 8421
Port 9000, processing: 8000 / 8421
Port 9000, processing: 0 / 8421
Port 9000, processing: 1000 / 8421
Port 9000, processing: 2000 / 8421
Port 9000, processing: 3000 / 8421
Port 9000, processing: 4000 / 8421
Port 9000, processing: 5000 / 8421
Port 9000, processing: 6000 / 8421
Port 9000, processing: 7000 / 8421
Port 9000, processing: 8000 / 8421
2022-12-05 05:51:43,659 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `test` split: 0.000
2022-12-05 07:44:50,994 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `dev1` split: 0.000
2022-12-05 09:37:44,578 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `dev2` split: 0.000
2022-12-05 11:26:32,595 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `dev3` split: 0.000
Loading pre-built vocab model stored in bothWikiSQL/processed/TableGraph/vocab.pt
loading model
Loaded pretrained model!
Skipping evaluation batch 73, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 18.45 GiB already allocated; 15.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 338, Reason: CUDA out of memory. Tried to allocate 608.00 MiB (GPU 6; 23.70 GiB total capacity; 20.54 GiB already allocated; 243.69 MiB free; 21.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 339, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 17.27 GiB already allocated; 45.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 340, Reason: CUDA out of memory. Tried to allocate 168.00 MiB (GPU 6; 23.70 GiB total capacity; 20.89 GiB already allocated; 31.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 348, Reason: CUDA out of memory. Tried to allocate 562.00 MiB (GPU 6; 23.70 GiB total capacity; 19.71 GiB already allocated; 547.69 MiB free; 21.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 349, Reason: CUDA out of memory. Tried to allocate 562.00 MiB (GPU 6; 23.70 GiB total capacity; 19.71 GiB already allocated; 547.69 MiB free; 21.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1535, Reason: CUDA out of memory. Tried to allocate 2.14 GiB (GPU 6; 23.70 GiB total capacity; 17.74 GiB already allocated; 1.71 GiB free; 20.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1536, Reason: CUDA out of memory. Tried to allocate 2.28 GiB (GPU 6; 23.70 GiB total capacity; 16.48 GiB already allocated; 2.14 GiB free; 19.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1537, Reason: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 6; 23.70 GiB total capacity; 14.74 GiB already allocated; 1.22 GiB free; 20.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1538, Reason: CUDA out of memory. Tried to allocate 2.14 GiB (GPU 6; 23.70 GiB total capacity; 15.60 GiB already allocated; 1.87 GiB free; 19.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1539, Reason: CUDA out of memory. Tried to allocate 446.00 MiB (GPU 6; 23.70 GiB total capacity; 17.79 GiB already allocated; 133.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1892, Reason: CUDA out of memory. Tried to allocate 408.00 MiB (GPU 6; 23.70 GiB total capacity; 19.22 GiB already allocated; 281.69 MiB free; 21.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1915, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 20.18 GiB already allocated; 45.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1916, Reason: CUDA out of memory. Tried to allocate 196.00 MiB (GPU 6; 23.70 GiB total capacity; 18.67 GiB already allocated; 45.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1917, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 20.08 GiB already allocated; 45.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1918, Reason: CUDA out of memory. Tried to allocate 274.00 MiB (GPU 6; 23.70 GiB total capacity; 18.70 GiB already allocated; 135.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1919, Reason: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 6; 23.70 GiB total capacity; 19.71 GiB already allocated; 79.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1920, Reason: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 6; 23.70 GiB total capacity; 19.71 GiB already allocated; 79.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1921, Reason: CUDA out of memory. Tried to allocate 234.00 MiB (GPU 6; 23.70 GiB total capacity; 19.63 GiB already allocated; 79.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1925, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 19.99 GiB already allocated; 45.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2061, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 20.46 GiB already allocated; 23.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2062, Reason: CUDA out of memory. Tried to allocate 118.00 MiB (GPU 6; 23.70 GiB total capacity; 19.29 GiB already allocated; 23.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2309, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 19.07 GiB already allocated; 25.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2310, Reason: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 6; 23.70 GiB total capacity; 19.10 GiB already allocated; 169.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2311, Reason: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 6; 23.70 GiB total capacity; 19.22 GiB already allocated; 113.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2312, Reason: CUDA out of memory. Tried to allocate 208.00 MiB (GPU 6; 23.70 GiB total capacity; 18.72 GiB already allocated; 113.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2645, Reason: CUDA out of memory. Tried to allocate 1020.00 MiB (GPU 6; 23.70 GiB total capacity; 18.13 GiB already allocated; 927.69 MiB free; 20.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2646, Reason: CUDA out of memory. Tried to allocate 1.11 GiB (GPU 6; 23.70 GiB total capacity; 20.16 GiB already allocated; 331.69 MiB free; 21.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2647, Reason: CUDA out of memory. Tried to allocate 1.05 GiB (GPU 6; 23.70 GiB total capacity; 19.14 GiB already allocated; 117.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2648, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 20.59 GiB already allocated; 99.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2649, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 20.48 GiB already allocated; 99.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2946, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 18.61 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2947, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 18.23 GiB already allocated; 67.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2948, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 17.84 GiB already allocated; 67.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2999, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 19.84 GiB already allocated; 109.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3000, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 20.95 GiB already allocated; 15.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3001, Reason: CUDA out of memory. Tried to allocate 186.00 MiB (GPU 6; 23.70 GiB total capacity; 20.58 GiB already allocated; 15.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3574, Reason: CUDA out of memory. Tried to allocate 312.00 MiB (GPU 6; 23.70 GiB total capacity; 19.19 GiB already allocated; 309.69 MiB free; 21.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3584, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 19.52 GiB already allocated; 15.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3585, Reason: CUDA out of memory. Tried to allocate 212.00 MiB (GPU 6; 23.70 GiB total capacity; 18.35 GiB already allocated; 103.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3600, Reason: CUDA out of memory. Tried to allocate 542.00 MiB (GPU 6; 23.70 GiB total capacity; 20.57 GiB already allocated; 135.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4065, Reason: CUDA out of memory. Tried to allocate 274.00 MiB (GPU 6; 23.70 GiB total capacity; 20.83 GiB already allocated; 245.69 MiB free; 21.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4120, Reason: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 6; 23.70 GiB total capacity; 20.01 GiB already allocated; 249.69 MiB free; 21.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4121, Reason: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 6; 23.70 GiB total capacity; 20.01 GiB already allocated; 249.69 MiB free; 21.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4122, Reason: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 6; 23.70 GiB total capacity; 20.01 GiB already allocated; 249.69 MiB free; 21.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4123, Reason: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 6; 23.70 GiB total capacity; 20.01 GiB already allocated; 249.69 MiB free; 21.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4164, Reason: CUDA out of memory. Tried to allocate 268.00 MiB (GPU 6; 23.70 GiB total capacity; 20.47 GiB already allocated; 151.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4295, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 20.83 GiB already allocated; 11.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4296, Reason: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 6; 23.70 GiB total capacity; 20.18 GiB already allocated; 11.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4304, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 19.69 GiB already allocated; 89.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4306, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 17.86 GiB already allocated; 85.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4314, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 20.21 GiB already allocated; 13.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4315, Reason: CUDA out of memory. Tried to allocate 340.00 MiB (GPU 6; 23.70 GiB total capacity; 19.39 GiB already allocated; 185.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5516, Reason: CUDA out of memory. Tried to allocate 1.03 GiB (GPU 6; 23.70 GiB total capacity; 18.75 GiB already allocated; 469.69 MiB free; 21.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5722, Reason: CUDA out of memory. Tried to allocate 666.00 MiB (GPU 6; 23.70 GiB total capacity; 17.58 GiB already allocated; 451.69 MiB free; 21.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6191, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 19.87 GiB already allocated; 1.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6192, Reason: CUDA out of memory. Tried to allocate 382.00 MiB (GPU 6; 23.70 GiB total capacity; 18.53 GiB already allocated; 241.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6478, Reason: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 6; 23.70 GiB total capacity; 18.54 GiB already allocated; 183.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6707, Reason: CUDA out of memory. Tried to allocate 670.00 MiB (GPU 6; 23.70 GiB total capacity; 19.20 GiB already allocated; 163.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6758, Reason: CUDA out of memory. Tried to allocate 336.00 MiB (GPU 6; 23.70 GiB total capacity; 19.91 GiB already allocated; 169.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6798, Reason: CUDA out of memory. Tried to allocate 422.00 MiB (GPU 6; 23.70 GiB total capacity; 18.99 GiB already allocated; 257.69 MiB free; 21.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6799, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 19.01 GiB already allocated; 19.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6819, Reason: CUDA out of memory. Tried to allocate 520.00 MiB (GPU 6; 23.70 GiB total capacity; 20.71 GiB already allocated; 229.69 MiB free; 21.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6987, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 20.10 GiB already allocated; 7.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6988, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 20.10 GiB already allocated; 7.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6989, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 20.10 GiB already allocated; 7.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6990, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 20.10 GiB already allocated; 7.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6991, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 20.10 GiB already allocated; 7.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7155, Reason: CUDA out of memory. Tried to allocate 496.00 MiB (GPU 6; 23.70 GiB total capacity; 18.29 GiB already allocated; 353.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8432, Reason: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 6; 23.70 GiB total capacity; 18.62 GiB already allocated; 137.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8433, Reason: CUDA out of memory. Tried to allocate 336.00 MiB (GPU 6; 23.70 GiB total capacity; 19.72 GiB already allocated; 137.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8434, Reason: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 6; 23.70 GiB total capacity; 19.78 GiB already allocated; 137.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8435, Reason: CUDA out of memory. Tried to allocate 312.00 MiB (GPU 6; 23.70 GiB total capacity; 19.24 GiB already allocated; 137.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8693, Reason: CUDA out of memory. Tried to allocate 336.00 MiB (GPU 6; 23.70 GiB total capacity; 19.39 GiB already allocated; 241.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8895, Reason: CUDA out of memory. Tried to allocate 708.00 MiB (GPU 6; 23.70 GiB total capacity; 19.72 GiB already allocated; 183.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8897, Reason: CUDA out of memory. Tried to allocate 418.00 MiB (GPU 6; 23.70 GiB total capacity; 16.88 GiB already allocated; 411.69 MiB free; 21.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9015, Reason: CUDA out of memory. Tried to allocate 418.00 MiB (GPU 6; 23.70 GiB total capacity; 17.98 GiB already allocated; 287.69 MiB free; 21.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9547, Reason: CUDA out of memory. Tried to allocate 896.00 MiB (GPU 6; 23.70 GiB total capacity; 19.10 GiB already allocated; 501.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9548, Reason: CUDA out of memory. Tried to allocate 502.00 MiB (GPU 6; 23.70 GiB total capacity; 17.20 GiB already allocated; 193.69 MiB free; 21.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9549, Reason: CUDA out of memory. Tried to allocate 538.00 MiB (GPU 6; 23.70 GiB total capacity; 16.67 GiB already allocated; 331.69 MiB free; 21.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9550, Reason: CUDA out of memory. Tried to allocate 574.00 MiB (GPU 6; 23.70 GiB total capacity; 17.74 GiB already allocated; 183.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9850, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 19.30 GiB already allocated; 71.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9851, Reason: CUDA out of memory. Tried to allocate 146.00 MiB (GPU 6; 23.70 GiB total capacity; 20.51 GiB already allocated; 101.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9852, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 19.47 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9885, Reason: CUDA out of memory. Tried to allocate 580.00 MiB (GPU 6; 23.70 GiB total capacity; 19.98 GiB already allocated; 289.69 MiB free; 21.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10099, Reason: CUDA out of memory. Tried to allocate 244.00 MiB (GPU 6; 23.70 GiB total capacity; 20.76 GiB already allocated; 9.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10100, Reason: CUDA out of memory. Tried to allocate 244.00 MiB (GPU 6; 23.70 GiB total capacity; 20.76 GiB already allocated; 9.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10101, Reason: CUDA out of memory. Tried to allocate 244.00 MiB (GPU 6; 23.70 GiB total capacity; 20.76 GiB already allocated; 9.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10102, Reason: CUDA out of memory. Tried to allocate 278.00 MiB (GPU 6; 23.70 GiB total capacity; 20.52 GiB already allocated; 9.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10103, Reason: CUDA out of memory. Tried to allocate 244.00 MiB (GPU 6; 23.70 GiB total capacity; 20.76 GiB already allocated; 9.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10703, Reason: CUDA out of memory. Tried to allocate 746.00 MiB (GPU 6; 23.70 GiB total capacity; 19.83 GiB already allocated; 253.69 MiB free; 21.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 11066, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 20.01 GiB already allocated; 9.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 11067, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 20.01 GiB already allocated; 9.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 11068, Reason: CUDA out of memory. Tried to allocate 246.00 MiB (GPU 6; 23.70 GiB total capacity; 20.17 GiB already allocated; 213.69 MiB free; 21.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 11075, Reason: CUDA out of memory. Tried to allocate 292.00 MiB (GPU 6; 23.70 GiB total capacity; 18.90 GiB already allocated; 119.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 11458, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 18.91 GiB already allocated; 29.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 11460, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 19.01 GiB already allocated; 19.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 11461, Reason: CUDA out of memory. Tried to allocate 292.00 MiB (GPU 6; 23.70 GiB total capacity; 18.37 GiB already allocated; 193.69 MiB free; 21.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12184, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 18.78 GiB already allocated; 61.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12185, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 20.45 GiB already allocated; 55.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12186, Reason: CUDA out of memory. Tried to allocate 500.00 MiB (GPU 6; 23.70 GiB total capacity; 19.72 GiB already allocated; 91.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12212, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 18.48 GiB already allocated; 19.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12294, Reason: CUDA out of memory. Tried to allocate 444.00 MiB (GPU 6; 23.70 GiB total capacity; 19.75 GiB already allocated; 287.69 MiB free; 21.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12295, Reason: CUDA out of memory. Tried to allocate 568.00 MiB (GPU 6; 23.70 GiB total capacity; 20.62 GiB already allocated; 511.69 MiB free; 21.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12296, Reason: CUDA out of memory. Tried to allocate 420.00 MiB (GPU 6; 23.70 GiB total capacity; 18.65 GiB already allocated; 235.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12297, Reason: CUDA out of memory. Tried to allocate 542.00 MiB (GPU 6; 23.70 GiB total capacity; 19.70 GiB already allocated; 235.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12298, Reason: CUDA out of memory. Tried to allocate 420.00 MiB (GPU 6; 23.70 GiB total capacity; 18.65 GiB already allocated; 235.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12497, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 19.41 GiB already allocated; 79.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12669, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 19.76 GiB already allocated; 49.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12796, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 19.11 GiB already allocated; 149.69 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12797, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 19.00 GiB already allocated; 37.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12855, Reason: CUDA out of memory. Tried to allocate 382.00 MiB (GPU 6; 23.70 GiB total capacity; 19.16 GiB already allocated; 325.69 MiB free; 21.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12856, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 20.10 GiB already allocated; 75.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13046, Reason: CUDA out of memory. Tried to allocate 360.00 MiB (GPU 6; 23.70 GiB total capacity; 20.16 GiB already allocated; 341.69 MiB free; 21.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13071, Reason: CUDA out of memory. Tried to allocate 298.00 MiB (GPU 6; 23.70 GiB total capacity; 20.35 GiB already allocated; 257.69 MiB free; 21.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13072, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 20.20 GiB already allocated; 11.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13073, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 19.56 GiB already allocated; 11.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13074, Reason: CUDA out of memory. Tried to allocate 280.00 MiB (GPU 6; 23.70 GiB total capacity; 19.79 GiB already allocated; 11.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13290, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 19.76 GiB already allocated; 9.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13291, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 19.04 GiB already allocated; 9.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13292, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 19.43 GiB already allocated; 9.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13374, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 19.79 GiB already allocated; 57.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13375, Reason: CUDA out of memory. Tried to allocate 552.00 MiB (GPU 6; 23.70 GiB total capacity; 16.22 GiB already allocated; 475.69 MiB free; 21.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13376, Reason: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 6; 23.70 GiB total capacity; 20.53 GiB already allocated; 91.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13377, Reason: CUDA out of memory. Tried to allocate 700.00 MiB (GPU 6; 23.70 GiB total capacity; 19.87 GiB already allocated; 349.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13526, Reason: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 6; 23.70 GiB total capacity; 17.35 GiB already allocated; 101.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13527, Reason: CUDA out of memory. Tried to allocate 410.00 MiB (GPU 6; 23.70 GiB total capacity; 17.03 GiB already allocated; 101.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13529, Reason: CUDA out of memory. Tried to allocate 364.00 MiB (GPU 6; 23.70 GiB total capacity; 18.69 GiB already allocated; 317.69 MiB free; 21.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13530, Reason: CUDA out of memory. Tried to allocate 246.00 MiB (GPU 6; 23.70 GiB total capacity; 19.55 GiB already allocated; 69.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13623, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 19.39 GiB already allocated; 3.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13750, Reason: CUDA out of memory. Tried to allocate 450.00 MiB (GPU 6; 23.70 GiB total capacity; 18.77 GiB already allocated; 191.69 MiB free; 21.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13790, Reason: CUDA out of memory. Tried to allocate 440.00 MiB (GPU 6; 23.70 GiB total capacity; 19.73 GiB already allocated; 283.69 MiB free; 21.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13897, Reason: CUDA out of memory. Tried to allocate 572.00 MiB (GPU 6; 23.70 GiB total capacity; 20.51 GiB already allocated; 121.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13898, Reason: CUDA out of memory. Tried to allocate 482.00 MiB (GPU 6; 23.70 GiB total capacity; 18.65 GiB already allocated; 121.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13966, Reason: CUDA out of memory. Tried to allocate 824.00 MiB (GPU 6; 23.70 GiB total capacity; 18.86 GiB already allocated; 181.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 13967, Reason: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 6; 23.70 GiB total capacity; 17.94 GiB already allocated; 27.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14175, Reason: CUDA out of memory. Tried to allocate 808.00 MiB (GPU 6; 23.70 GiB total capacity; 21.24 GiB already allocated; 185.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14176, Reason: CUDA out of memory. Tried to allocate 4.54 GiB (GPU 6; 23.70 GiB total capacity; 16.79 GiB already allocated; 1.23 GiB free; 20.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14177, Reason: CUDA out of memory. Tried to allocate 4.23 GiB (GPU 6; 23.70 GiB total capacity; 11.36 GiB already allocated; 3.41 GiB free; 18.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14178, Reason: CUDA out of memory. Tried to allocate 3.85 GiB (GPU 6; 23.70 GiB total capacity; 14.11 GiB already allocated; 623.69 MiB free; 21.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14179, Reason: CUDA out of memory. Tried to allocate 5.08 GiB (GPU 6; 23.70 GiB total capacity; 13.87 GiB already allocated; 2.61 GiB free; 19.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14180, Reason: CUDA out of memory. Tried to allocate 3.69 GiB (GPU 6; 23.70 GiB total capacity; 13.53 GiB already allocated; 2.61 GiB free; 19.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14198, Reason: CUDA out of memory. Tried to allocate 254.00 MiB (GPU 6; 23.70 GiB total capacity; 21.16 GiB already allocated; 51.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14199, Reason: CUDA out of memory. Tried to allocate 292.00 MiB (GPU 6; 23.70 GiB total capacity; 21.05 GiB already allocated; 125.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14329, Reason: CUDA out of memory. Tried to allocate 660.00 MiB (GPU 6; 23.70 GiB total capacity; 20.88 GiB already allocated; 155.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14330, Reason: CUDA out of memory. Tried to allocate 242.00 MiB (GPU 6; 23.70 GiB total capacity; 20.00 GiB already allocated; 73.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14331, Reason: CUDA out of memory. Tried to allocate 454.00 MiB (GPU 6; 23.70 GiB total capacity; 20.12 GiB already allocated; 73.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14332, Reason: CUDA out of memory. Tried to allocate 454.00 MiB (GPU 6; 23.70 GiB total capacity; 20.12 GiB already allocated; 73.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14333, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 20.57 GiB already allocated; 73.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14790, Reason: CUDA out of memory. Tried to allocate 228.00 MiB (GPU 6; 23.70 GiB total capacity; 20.18 GiB already allocated; 101.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14791, Reason: CUDA out of memory. Tried to allocate 228.00 MiB (GPU 6; 23.70 GiB total capacity; 20.18 GiB already allocated; 43.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14792, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 19.55 GiB already allocated; 63.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 14793, Reason: CUDA out of memory. Tried to allocate 244.00 MiB (GPU 6; 23.70 GiB total capacity; 19.56 GiB already allocated; 63.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 15720, Reason: CUDA out of memory. Tried to allocate 622.00 MiB (GPU 6; 23.70 GiB total capacity; 19.92 GiB already allocated; 317.69 MiB free; 21.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 15721, Reason: CUDA out of memory. Tried to allocate 586.00 MiB (GPU 6; 23.70 GiB total capacity; 19.26 GiB already allocated; 317.69 MiB free; 21.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 15722, Reason: CUDA out of memory. Tried to allocate 262.00 MiB (GPU 6; 23.70 GiB total capacity; 18.87 GiB already allocated; 193.69 MiB free; 21.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
Skipping evaluation batch 269, Reason: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 23.70 GiB total capacity; 18.24 GiB already allocated; 119.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 270, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 18.47 GiB already allocated; 53.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 973, Reason: CUDA out of memory. Tried to allocate 628.00 MiB (GPU 6; 23.70 GiB total capacity; 19.87 GiB already allocated; 101.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 974, Reason: CUDA out of memory. Tried to allocate 514.00 MiB (GPU 6; 23.70 GiB total capacity; 17.92 GiB already allocated; 377.69 MiB free; 21.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1668, Reason: CUDA out of memory. Tried to allocate 492.00 MiB (GPU 6; 23.70 GiB total capacity; 18.16 GiB already allocated; 429.69 MiB free; 21.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1669, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 18.81 GiB already allocated; 95.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1670, Reason: CUDA out of memory. Tried to allocate 870.00 MiB (GPU 6; 23.70 GiB total capacity; 16.45 GiB already allocated; 611.69 MiB free; 21.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1671, Reason: CUDA out of memory. Tried to allocate 984.00 MiB (GPU 6; 23.70 GiB total capacity; 18.42 GiB already allocated; 595.69 MiB free; 21.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2407, Reason: CUDA out of memory. Tried to allocate 382.00 MiB (GPU 6; 23.70 GiB total capacity; 17.80 GiB already allocated; 275.69 MiB free; 21.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2408, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 18.15 GiB already allocated; 23.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2413, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 18.64 GiB already allocated; 3.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2414, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 20.14 GiB already allocated; 43.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2415, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 18.48 GiB already allocated; 61.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2416, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 18.34 GiB already allocated; 37.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2417, Reason: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 6; 23.70 GiB total capacity; 17.51 GiB already allocated; 41.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2418, Reason: CUDA out of memory. Tried to allocate 364.00 MiB (GPU 6; 23.70 GiB total capacity; 17.92 GiB already allocated; 259.69 MiB free; 21.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2419, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 18.15 GiB already allocated; 65.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2420, Reason: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 6; 23.70 GiB total capacity; 17.90 GiB already allocated; 127.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3607, Reason: CUDA out of memory. Tried to allocate 236.00 MiB (GPU 6; 23.70 GiB total capacity; 19.63 GiB already allocated; 117.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3729, Reason: CUDA out of memory. Tried to allocate 322.00 MiB (GPU 6; 23.70 GiB total capacity; 17.38 GiB already allocated; 187.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3757, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 20.01 GiB already allocated; 33.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3758, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 19.79 GiB already allocated; 33.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3823, Reason: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 6; 23.70 GiB total capacity; 19.72 GiB already allocated; 203.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3824, Reason: CUDA out of memory. Tried to allocate 146.00 MiB (GPU 6; 23.70 GiB total capacity; 19.32 GiB already allocated; 141.69 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3825, Reason: CUDA out of memory. Tried to allocate 162.00 MiB (GPU 6; 23.70 GiB total capacity; 19.28 GiB already allocated; 129.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4753, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 18.97 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4754, Reason: CUDA out of memory. Tried to allocate 272.00 MiB (GPU 6; 23.70 GiB total capacity; 18.77 GiB already allocated; 95.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4948, Reason: CUDA out of memory. Tried to allocate 248.00 MiB (GPU 6; 23.70 GiB total capacity; 20.33 GiB already allocated; 117.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5097, Reason: CUDA out of memory. Tried to allocate 494.00 MiB (GPU 6; 23.70 GiB total capacity; 20.03 GiB already allocated; 281.69 MiB free; 21.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5099, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 19.41 GiB already allocated; 61.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5100, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 18.76 GiB already allocated; 33.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5343, Reason: CUDA out of memory. Tried to allocate 740.00 MiB (GPU 6; 23.70 GiB total capacity; 18.79 GiB already allocated; 181.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5345, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 19.98 GiB already allocated; 29.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5346, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 19.64 GiB already allocated; 37.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6097, Reason: CUDA out of memory. Tried to allocate 1.11 GiB (GPU 6; 23.70 GiB total capacity; 19.83 GiB already allocated; 163.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6098, Reason: CUDA out of memory. Tried to allocate 118.00 MiB (GPU 6; 23.70 GiB total capacity; 20.80 GiB already allocated; 27.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6099, Reason: CUDA out of memory. Tried to allocate 364.00 MiB (GPU 6; 23.70 GiB total capacity; 18.53 GiB already allocated; 321.69 MiB free; 21.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6613, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 18.32 GiB already allocated; 49.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6670, Reason: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 6; 23.70 GiB total capacity; 19.52 GiB already allocated; 67.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6672, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 19.92 GiB already allocated; 7.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7154, Reason: CUDA out of memory. Tried to allocate 248.00 MiB (GPU 6; 23.70 GiB total capacity; 19.29 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7155, Reason: CUDA out of memory. Tried to allocate 470.00 MiB (GPU 6; 23.70 GiB total capacity; 18.53 GiB already allocated; 249.69 MiB free; 21.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7504, Reason: CUDA out of memory. Tried to allocate 720.00 MiB (GPU 6; 23.70 GiB total capacity; 18.16 GiB already allocated; 689.69 MiB free; 21.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7662, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 18.65 GiB already allocated; 395.69 MiB free; 21.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7810, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 18.26 GiB already allocated; 167.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7811, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 18.27 GiB already allocated; 107.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7812, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 18.27 GiB already allocated; 107.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7813, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 18.27 GiB already allocated; 107.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
Skipping evaluation batch 269, Reason: CUDA out of memory. Tried to allocate 572.00 MiB (GPU 6; 23.70 GiB total capacity; 17.13 GiB already allocated; 505.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 270, Reason: CUDA out of memory. Tried to allocate 264.00 MiB (GPU 6; 23.70 GiB total capacity; 17.70 GiB already allocated; 109.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 973, Reason: CUDA out of memory. Tried to allocate 210.00 MiB (GPU 6; 23.70 GiB total capacity; 19.56 GiB already allocated; 179.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 974, Reason: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 6; 23.70 GiB total capacity; 18.69 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1668, Reason: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 6; 23.70 GiB total capacity; 19.00 GiB already allocated; 35.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1669, Reason: CUDA out of memory. Tried to allocate 430.00 MiB (GPU 6; 23.70 GiB total capacity; 17.75 GiB already allocated; 131.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1670, Reason: CUDA out of memory. Tried to allocate 606.00 MiB (GPU 6; 23.70 GiB total capacity; 17.31 GiB already allocated; 11.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1671, Reason: CUDA out of memory. Tried to allocate 984.00 MiB (GPU 6; 23.70 GiB total capacity; 18.41 GiB already allocated; 357.69 MiB free; 21.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2406, Reason: CUDA out of memory. Tried to allocate 458.00 MiB (GPU 6; 23.70 GiB total capacity; 16.50 GiB already allocated; 143.69 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2407, Reason: CUDA out of memory. Tried to allocate 382.00 MiB (GPU 6; 23.70 GiB total capacity; 18.90 GiB already allocated; 307.69 MiB free; 21.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2408, Reason: CUDA out of memory. Tried to allocate 990.00 MiB (GPU 6; 23.70 GiB total capacity; 17.30 GiB already allocated; 305.69 MiB free; 21.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2413, Reason: CUDA out of memory. Tried to allocate 218.00 MiB (GPU 6; 23.70 GiB total capacity; 18.78 GiB already allocated; 215.69 MiB free; 21.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2414, Reason: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 6; 23.70 GiB total capacity; 18.05 GiB already allocated; 147.69 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2415, Reason: CUDA out of memory. Tried to allocate 244.00 MiB (GPU 6; 23.70 GiB total capacity; 19.68 GiB already allocated; 21.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2416, Reason: CUDA out of memory. Tried to allocate 268.00 MiB (GPU 6; 23.70 GiB total capacity; 18.53 GiB already allocated; 49.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2417, Reason: CUDA out of memory. Tried to allocate 478.00 MiB (GPU 6; 23.70 GiB total capacity; 18.65 GiB already allocated; 283.69 MiB free; 21.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2418, Reason: CUDA out of memory. Tried to allocate 364.00 MiB (GPU 6; 23.70 GiB total capacity; 17.93 GiB already allocated; 283.69 MiB free; 21.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2419, Reason: CUDA out of memory. Tried to allocate 324.00 MiB (GPU 6; 23.70 GiB total capacity; 17.52 GiB already allocated; 283.69 MiB free; 21.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2420, Reason: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 6; 23.70 GiB total capacity; 17.56 GiB already allocated; 283.69 MiB free; 21.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3607, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 20.20 GiB already allocated; 35.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3757, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 20.80 GiB already allocated; 21.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3758, Reason: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 6; 23.70 GiB total capacity; 20.61 GiB already allocated; 21.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3823, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 20.37 GiB already allocated; 21.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3824, Reason: CUDA out of memory. Tried to allocate 146.00 MiB (GPU 6; 23.70 GiB total capacity; 19.92 GiB already allocated; 21.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3825, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 19.94 GiB already allocated; 21.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4753, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 19.29 GiB already allocated; 33.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4754, Reason: CUDA out of memory. Tried to allocate 272.00 MiB (GPU 6; 23.70 GiB total capacity; 17.89 GiB already allocated; 145.69 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4948, Reason: CUDA out of memory. Tried to allocate 248.00 MiB (GPU 6; 23.70 GiB total capacity; 20.34 GiB already allocated; 203.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5097, Reason: CUDA out of memory. Tried to allocate 494.00 MiB (GPU 6; 23.70 GiB total capacity; 20.03 GiB already allocated; 261.69 MiB free; 21.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5099, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 19.54 GiB already allocated; 1.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5100, Reason: CUDA out of memory. Tried to allocate 212.00 MiB (GPU 6; 23.70 GiB total capacity; 18.25 GiB already allocated; 175.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5343, Reason: CUDA out of memory. Tried to allocate 952.00 MiB (GPU 6; 23.70 GiB total capacity; 17.09 GiB already allocated; 951.69 MiB free; 20.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5345, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 19.80 GiB already allocated; 11.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5346, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 18.81 GiB already allocated; 17.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6097, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 18.64 GiB already allocated; 31.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6098, Reason: CUDA out of memory. Tried to allocate 118.00 MiB (GPU 6; 23.70 GiB total capacity; 20.91 GiB already allocated; 25.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6099, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 18.89 GiB already allocated; 37.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6613, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 20.08 GiB already allocated; 53.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6670, Reason: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 6; 23.70 GiB total capacity; 19.51 GiB already allocated; 33.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6672, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 20.14 GiB already allocated; 37.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7154, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 18.60 GiB already allocated; 37.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7155, Reason: CUDA out of memory. Tried to allocate 470.00 MiB (GPU 6; 23.70 GiB total capacity; 18.52 GiB already allocated; 209.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7504, Reason: CUDA out of memory. Tried to allocate 452.00 MiB (GPU 6; 23.70 GiB total capacity; 18.89 GiB already allocated; 89.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7810, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 19.47 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7811, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 19.24 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7812, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 19.24 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7813, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 19.24 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
Skipping evaluation batch 269, Reason: CUDA out of memory. Tried to allocate 572.00 MiB (GPU 6; 23.70 GiB total capacity; 16.58 GiB already allocated; 333.69 MiB free; 21.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 270, Reason: CUDA out of memory. Tried to allocate 264.00 MiB (GPU 6; 23.70 GiB total capacity; 16.90 GiB already allocated; 69.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 973, Reason: CUDA out of memory. Tried to allocate 210.00 MiB (GPU 6; 23.70 GiB total capacity; 19.56 GiB already allocated; 59.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 974, Reason: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 6; 23.70 GiB total capacity; 18.94 GiB already allocated; 247.69 MiB free; 21.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1668, Reason: CUDA out of memory. Tried to allocate 492.00 MiB (GPU 6; 23.70 GiB total capacity; 18.16 GiB already allocated; 167.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1669, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 18.38 GiB already allocated; 107.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1670, Reason: CUDA out of memory. Tried to allocate 606.00 MiB (GPU 6; 23.70 GiB total capacity; 17.31 GiB already allocated; 95.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1671, Reason: CUDA out of memory. Tried to allocate 984.00 MiB (GPU 6; 23.70 GiB total capacity; 18.41 GiB already allocated; 95.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2407, Reason: CUDA out of memory. Tried to allocate 382.00 MiB (GPU 6; 23.70 GiB total capacity; 19.36 GiB already allocated; 359.69 MiB free; 21.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2408, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 19.50 GiB already allocated; 57.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2412, Reason: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 6; 23.70 GiB total capacity; 17.12 GiB already allocated; 309.69 MiB free; 21.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2413, Reason: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 6; 23.70 GiB total capacity; 17.43 GiB already allocated; 309.69 MiB free; 21.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2414, Reason: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 6; 23.70 GiB total capacity; 16.44 GiB already allocated; 309.69 MiB free; 21.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2415, Reason: CUDA out of memory. Tried to allocate 730.00 MiB (GPU 6; 23.70 GiB total capacity; 17.71 GiB already allocated; 309.69 MiB free; 21.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2416, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 16.97 GiB already allocated; 37.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2417, Reason: CUDA out of memory. Tried to allocate 478.00 MiB (GPU 6; 23.70 GiB total capacity; 17.05 GiB already allocated; 427.69 MiB free; 21.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2418, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 16.89 GiB already allocated; 71.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2419, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 16.63 GiB already allocated; 17.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2420, Reason: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 6; 23.70 GiB total capacity; 16.20 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3607, Reason: CUDA out of memory. Tried to allocate 236.00 MiB (GPU 6; 23.70 GiB total capacity; 19.40 GiB already allocated; 103.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3729, Reason: CUDA out of memory. Tried to allocate 322.00 MiB (GPU 6; 23.70 GiB total capacity; 17.07 GiB already allocated; 279.69 MiB free; 21.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3757, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 20.18 GiB already allocated; 27.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3758, Reason: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 6; 23.70 GiB total capacity; 19.94 GiB already allocated; 27.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3823, Reason: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 6; 23.70 GiB total capacity; 19.71 GiB already allocated; 215.69 MiB free; 21.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3824, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 19.78 GiB already allocated; 21.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3825, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 19.75 GiB already allocated; 13.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4753, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 18.97 GiB already allocated; 37.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4754, Reason: CUDA out of memory. Tried to allocate 272.00 MiB (GPU 6; 23.70 GiB total capacity; 18.77 GiB already allocated; 93.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4948, Reason: CUDA out of memory. Tried to allocate 248.00 MiB (GPU 6; 23.70 GiB total capacity; 20.34 GiB already allocated; 159.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5097, Reason: CUDA out of memory. Tried to allocate 494.00 MiB (GPU 6; 23.70 GiB total capacity; 20.03 GiB already allocated; 357.69 MiB free; 21.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5099, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 19.32 GiB already allocated; 59.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5100, Reason: CUDA out of memory. Tried to allocate 212.00 MiB (GPU 6; 23.70 GiB total capacity; 18.89 GiB already allocated; 147.69 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5343, Reason: CUDA out of memory. Tried to allocate 740.00 MiB (GPU 6; 23.70 GiB total capacity; 18.05 GiB already allocated; 157.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5345, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 19.93 GiB already allocated; 49.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5346, Reason: CUDA out of memory. Tried to allocate 342.00 MiB (GPU 6; 23.70 GiB total capacity; 18.81 GiB already allocated; 195.69 MiB free; 21.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6097, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 19.73 GiB already allocated; 81.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6098, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 21.14 GiB already allocated; 17.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6099, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 18.89 GiB already allocated; 91.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6613, Reason: CUDA out of memory. Tried to allocate 330.00 MiB (GPU 6; 23.70 GiB total capacity; 19.20 GiB already allocated; 45.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6670, Reason: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 6; 23.70 GiB total capacity; 19.91 GiB already allocated; 13.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6672, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 19.46 GiB already allocated; 71.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7154, Reason: CUDA out of memory. Tried to allocate 248.00 MiB (GPU 6; 23.70 GiB total capacity; 19.31 GiB already allocated; 159.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7155, Reason: CUDA out of memory. Tried to allocate 586.00 MiB (GPU 6; 23.70 GiB total capacity; 17.03 GiB already allocated; 551.69 MiB free; 21.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7504, Reason: CUDA out of memory. Tried to allocate 452.00 MiB (GPU 6; 23.70 GiB total capacity; 18.89 GiB already allocated; 33.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7810, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 19.23 GiB already allocated; 65.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7811, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 19.23 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7812, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 19.23 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7813, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 19.23 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
