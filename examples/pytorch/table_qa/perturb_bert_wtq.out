[ Using CUDA ]
{'tokenizer': <function tokenize_jobs at 0x7f97b0703310>, 'pretrained_word_emb_name': '6B', 'pretrained_word_emb_url': None, 'pretrained_word_emb_cache_dir': None, 'merge_strategy': 'tailhead', 'edge_strategy': 'heterogeneous', 'seed': None, 'word_emb_size': 300, 'thread_number': 1, 'port': 9000}
Using Table2Text read_raw_data
Found:  56355  items
Loaded SQL
Found:  11276  items
Found:  15878  items
Found:  8421  items
Found:  8421  items
Found:  8421  items
Main File: ../../../graph4nlp/pytorch/WikiTableQA/WikiSQL/seq2seq_data/test_perturb.csv
Found:  10576  items
Main File: ../../../graph4nlp/pytorch/WikiTableQA/WikiSQL/seq2seq_data/test_perturb.csv
Found:  10576  items
Main File: ../../../graph4nlp/pytorch/WikiTableQA/WikiSQL/seq2seq_data/dev_perturb.csv
Found:  5660  items
Main File: ../../../graph4nlp/pytorch/WikiTableQA/WikiSQL/seq2seq_data/dev_perturb.csv
Found:  5660  items
table _process works!
Port 9000, processing: 0 / 56355
Port 9000, processing: 1000 / 56355
Port 9000, processing: 2000 / 56355
Port 9000, processing: 3000 / 56355
Port 9000, processing: 4000 / 56355
Port 9000, processing: 5000 / 56355
Port 9000, processing: 6000 / 56355
Port 9000, processing: 7000 / 56355
Port 9000, processing: 8000 / 56355
Port 9000, processing: 9000 / 56355
Port 9000, processing: 10000 / 56355
Port 9000, processing: 11000 / 56355
Port 9000, processing: 12000 / 56355
Port 9000, processing: 13000 / 56355
Port 9000, processing: 14000 / 56355
Port 9000, processing: 15000 / 56355
Port 9000, processing: 16000 / 56355
Port 9000, processing: 17000 / 56355
Port 9000, processing: 18000 / 56355
Port 9000, processing: 19000 / 56355
Port 9000, processing: 20000 / 56355
Port 9000, processing: 21000 / 56355
Port 9000, processing: 22000 / 56355
Port 9000, processing: 23000 / 56355
Port 9000, processing: 24000 / 56355
Port 9000, processing: 25000 / 56355
Port 9000, processing: 26000 / 56355
Port 9000, processing: 27000 / 56355
Port 9000, processing: 28000 / 56355
Port 9000, processing: 29000 / 56355
Port 9000, processing: 30000 / 56355
Port 9000, processing: 31000 / 56355
Port 9000, processing: 32000 / 56355
Port 9000, processing: 33000 / 56355
Port 9000, processing: 34000 / 56355
Port 9000, processing: 35000 / 56355
Port 9000, processing: 36000 / 56355
Port 9000, processing: 37000 / 56355
Port 9000, processing: 38000 / 56355
Port 9000, processing: 39000 / 56355
Port 9000, processing: 40000 / 56355
Port 9000, processing: 41000 / 56355
Port 9000, processing: 42000 / 56355
Port 9000, processing: 43000 / 56355
Port 9000, processing: 44000 / 56355
Port 9000, processing: 45000 / 56355
Port 9000, processing: 46000 / 56355
Port 9000, processing: 47000 / 56355
Port 9000, processing: 48000 / 56355
Port 9000, processing: 49000 / 56355
Port 9000, processing: 50000 / 56355
Port 9000, processing: 51000 / 56355
Port 9000, processing: 52000 / 56355
Port 9000, processing: 53000 / 56355
Port 9000, processing: 54000 / 56355
Port 9000, processing: 55000 / 56355
Port 9000, processing: 56000 / 56355
Port 9000, processing: 0 / 11276
Port 9000, processing: 1000 / 11276
Port 9000, processing: 2000 / 11276
Port 9000, processing: 3000 / 11276
Port 9000, processing: 4000 / 11276
Port 9000, processing: 5000 / 11276
Port 9000, processing: 6000 / 11276
Port 9000, processing: 7000 / 11276
Port 9000, processing: 8000 / 11276
Port 9000, processing: 9000 / 11276
Port 9000, processing: 10000 / 11276
Port 9000, processing: 11000 / 11276
Port 9000, processing: 0 / 15878
Port 9000, processing: 1000 / 15878
Port 9000, processing: 2000 / 15878
Port 9000, processing: 3000 / 15878
Port 9000, processing: 4000 / 15878
Port 9000, processing: 5000 / 15878
Port 9000, processing: 6000 / 15878
Port 9000, processing: 7000 / 15878
Port 9000, processing: 8000 / 15878
Port 9000, processing: 9000 / 15878
Port 9000, processing: 10000 / 15878
Port 9000, processing: 11000 / 15878
Port 9000, processing: 12000 / 15878
Port 9000, processing: 13000 / 15878
Port 9000, processing: 14000 / 15878
Port 9000, processing: 15000 / 15878
Port 9000, processing: 0 / 8421
Port 9000, processing: 1000 / 8421
Port 9000, processing: 2000 / 8421
Port 9000, processing: 3000 / 8421
Port 9000, processing: 4000 / 8421
Port 9000, processing: 5000 / 8421
Port 9000, processing: 6000 / 8421
Port 9000, processing: 7000 / 8421
Port 9000, processing: 8000 / 8421
Port 9000, processing: 0 / 8421
Port 9000, processing: 1000 / 8421
Port 9000, processing: 2000 / 8421
Port 9000, processing: 3000 / 8421
Port 9000, processing: 4000 / 8421
Port 9000, processing: 5000 / 8421
Port 9000, processing: 6000 / 8421
Port 9000, processing: 7000 / 8421
Port 9000, processing: 8000 / 8421
Port 9000, processing: 0 / 8421
Port 9000, processing: 1000 / 8421
Port 9000, processing: 2000 / 8421
Port 9000, processing: 3000 / 8421
Port 9000, processing: 4000 / 8421
Port 9000, processing: 5000 / 8421
Port 9000, processing: 6000 / 8421
Port 9000, processing: 7000 / 8421
Port 9000, processing: 8000 / 8421
Port 9000, processing: 0 / 10576
Port 9000, processing: 1000 / 10576
Port 9000, processing: 2000 / 10576
Port 9000, processing: 3000 / 10576
Port 9000, processing: 4000 / 10576
Port 9000, processing: 5000 / 10576
Port 9000, processing: 6000 / 10576
Port 9000, processing: 7000 / 10576
Port 9000, processing: 8000 / 10576
Port 9000, processing: 9000 / 10576
Port 9000, processing: 10000 / 10576
Port 9000, processing: 0 / 10576
Port 9000, processing: 1000 / 10576
Port 9000, processing: 2000 / 10576
Port 9000, processing: 3000 / 10576
Port 9000, processing: 4000 / 10576
Port 9000, processing: 5000 / 10576
Port 9000, processing: 6000 / 10576
Port 9000, processing: 7000 / 10576
Port 9000, processing: 8000 / 10576
Port 9000, processing: 9000 / 10576
Port 9000, processing: 10000 / 10576
Port 9000, processing: 0 / 5660
Port 9000, processing: 1000 / 5660
Port 9000, processing: 2000 / 5660
Port 9000, processing: 3000 / 5660
Port 9000, processing: 4000 / 5660
Port 9000, processing: 5000 / 5660
Port 9000, processing: 0 / 5660
Port 9000, processing: 1000 / 5660
Port 9000, processing: 2000 / 5660
Port 9000, processing: 3000 / 5660
Port 9000, processing: 4000 / 5660
Port 9000, processing: 5000 / 5660
Loading pre-built vocab model stored in bothWikiSQL/processed/TableGraph/vocab.pt
loading model
Loaded pretrained model!
Skipping evaluation batch 6, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 11, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 18, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 19, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 37, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 42, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 43, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 44, Reason: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 6; 23.70 GiB total capacity; 2.28 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 46, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 54, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 55, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 56, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 57, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 58, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 78, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 79, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 80, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 81, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 82, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 83, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 84, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 96, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 99, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 100, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 101, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 102, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 103, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 104, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 105, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 106, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 108, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 109, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 116, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 122, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 125, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 126, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 130, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 140, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 141, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 142, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 143, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 144, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 145, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 146, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 157, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 90.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 159, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 160, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 161, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 171, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 173, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 174, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 175, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 176, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 177, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 178, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 179, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 180, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 181, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 182, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 183, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 222, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 223, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 224, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 225, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 226, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 227, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 228, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 233, Reason: CUDA out of memory. Tried to allocate 416.00 MiB (GPU 6; 23.70 GiB total capacity; 2.28 GiB already allocated; 376.69 MiB free; 2.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 240, Reason: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 241, Reason: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 112.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 242, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 243, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 244, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 248, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 249, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 250, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 251, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 252, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 253, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 254, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 256, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 257, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 258, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 259, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 266, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 267, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 268, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 269, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 270, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 273, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 274, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 275, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 298, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 299, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 303, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 305, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 306, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 307, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 308, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 309, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 310, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 319, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 321, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 325, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 326, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 327, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 335, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 336, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 337, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 338, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 363, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 364, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 365, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 370, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 375, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 376, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 377, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 378, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 387, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 388, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 389, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 390, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 391, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 392, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 393, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 394, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 395, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 396, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 397, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 398, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 399, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 400, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 401, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 402, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 403, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 404, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 405, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 429, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 430, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 431, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 432, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 433, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 434, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 435, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 439, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 454, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 455, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 456, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 472, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 473, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 475, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 476, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 477, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 478, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 483, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 484, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 485, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 486, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 487, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 488, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 489, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 490, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 491, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 492, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 528, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 529, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 544, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 546, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 547, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 550, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 551, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 552, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 553, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 554, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 555, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 559, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 560, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 561, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 562, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 563, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 564, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 567, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 568, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 569, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 570, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 571, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 572, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 574, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 577, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 579, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 580, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 581, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 582, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 583, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 587, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 589, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 590, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 591, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 592, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 593, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 594, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 595, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 596, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 597, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 598, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 599, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 600, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 601, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 602, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 603, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 604, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 608, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 609, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 610, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 611, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 612, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 613, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 614, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 615, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 616, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 617, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 618, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 619, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 620, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 621, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 630, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 631, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 632, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 633, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 634, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 635, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 636, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 637, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 647, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 648, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 649, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 3.04 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 650, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.04 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 654, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 669, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 676, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 698, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 699, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 700, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 701, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 703, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 710, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 712, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 713, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 714, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 725, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 726, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 750, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 752, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 767, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 768, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 769, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 770, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 782, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 783, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 784, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 785, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 786, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 787, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 791, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 792, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 807, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 812, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 814, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 815, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 816, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 820, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 821, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 822, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 824, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 833, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 836, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 837, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 847, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 848, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 858, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 873, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 874, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 875, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 876, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 880, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 887, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 888, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 889, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 890, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 892, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 902, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 903, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 906, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 907, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 908, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 909, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 910, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 911, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 912, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 913, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 914, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 915, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 918, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 919, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 920, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 921, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 927, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 938, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 939, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 940, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 946, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 948, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 949, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 2.35 GiB already allocated; 112.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 963, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 964, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 965, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 972, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 90.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 973, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 92.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 974, Reason: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 108.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 975, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 994, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 995, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 996, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 999, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1000, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1001, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1002, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1003, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1004, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1005, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1006, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1007, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1008, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1009, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1010, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1011, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1012, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1013, Reason: CUDA out of memory. Tried to allocate 446.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1014, Reason: CUDA out of memory. Tried to allocate 446.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1015, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1016, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1017, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1018, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1019, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1020, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1021, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1022, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1023, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1031, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1037, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1038, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1039, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1046, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1047, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1068, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1069, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1076, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 102.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1077, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1078, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1079, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1080, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1082, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1090, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1091, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1092, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1093, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1099, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1100, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1101, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1102, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1103, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1105, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1107, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1113, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1114, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1115, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1124, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1125, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1126, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1127, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1133, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1138, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1139, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1144, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1145, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1146, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1156, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1157, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1158, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1159, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1160, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1161, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1162, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1163, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1164, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1165, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1176, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1177, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1178, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1179, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1182, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1184, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1185, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1188, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1189, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1190, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1191, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1192, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1193, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1194, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1195, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1196, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1197, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1198, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1199, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1202, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1203, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1204, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1205, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1206, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1207, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1209, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1210, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1211, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1212, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1213, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1214, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1215, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1216, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1217, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1218, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1225, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1230, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1231, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1232, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1245, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1246, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1247, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1248, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1249, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1251, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1259, Reason: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 118.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1260, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1261, Reason: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 164.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1262, Reason: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 164.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1263, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1264, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1265, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1267, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1268, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1269, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1270, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1271, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1279, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1280, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1289, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1290, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1291, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1292, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1293, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1294, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1295, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1296, Reason: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1297, Reason: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 88.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1298, Reason: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 114.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1316, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1318, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1319, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1320, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1321, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1322, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1323, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1324, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1325, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1326, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1327, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 3.05 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1339, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1340, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1341, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1346, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1347, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1348, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1349, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1350, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1351, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1370, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1399, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1407, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1408, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1409, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1410, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1411, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1412, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1430, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1431, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1438, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1441, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1442, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1443, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1451, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1452, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1453, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1455, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1456, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1457, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1463, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1464, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1465, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1466, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1467, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1468, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1469, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1470, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1471, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1472, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1473, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1481, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1482, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1490, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1491, Reason: CUDA out of memory. Tried to allocate 208.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1503, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1504, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1505, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1506, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1507, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1508, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1509, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1510, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1511, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1517, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1527, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1537, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1538, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1539, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1540, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1567, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1578, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1579, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1580, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1582, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1583, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1586, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1587, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1588, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1589, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1611, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1612, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1613, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1614, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1615, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1616, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1617, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1618, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1629, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1630, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 78.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1632, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1633, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1634, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1636, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1637, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1638, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1639, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1649, Reason: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1650, Reason: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1651, Reason: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 120.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1652, Reason: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1653, Reason: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 118.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1661, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1662, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1663, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1664, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1665, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1666, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1667, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1668, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1669, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1670, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1671, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1672, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1679, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1680, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1681, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1684, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1685, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1686, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1687, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1688, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1689, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1690, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1691, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1700, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1701, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1713, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1715, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1726, Reason: CUDA out of memory. Tried to allocate 420.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 326.69 MiB free; 2.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1749, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1750, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1751, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1752, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1758, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1759, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1760, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1761, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1762, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1763, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1764, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1765, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1766, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1767, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1768, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1769, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1770, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1771, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1772, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1773, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1774, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1775, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1792, Reason: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1793, Reason: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 100.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1794, Reason: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 100.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1795, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1796, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1797, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1798, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1799, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1808, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1809, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1810, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1811, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1812, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1815, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1816, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1817, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1824, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1825, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1832, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1833, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1834, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1840, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1846, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 70.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1862, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1866, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1868, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1869, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1870, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1872, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1873, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1874, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1875, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1876, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1882, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1883, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1884, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1896, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1899, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1900, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1901, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1906, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1907, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1908, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1918, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1919, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1926, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1927, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 84.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1928, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1938, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1939, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1940, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1943, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1954, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1956, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1957, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1958, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1959, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1960, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1961, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1965, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1966, Reason: CUDA out of memory. Tried to allocate 186.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 108.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1972, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1973, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1974, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1975, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1983, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1984, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1990, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1991, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1992, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1998, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1999, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2000, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2001, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2002, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2003, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2023, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2024, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2027, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2028, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2043, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2044, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2045, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2046, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2047, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2059, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2060, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2062, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2063, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2078, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2079, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2080, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2081, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2082, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2083, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2084, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2085, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2090, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2091, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2092, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2095, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2096, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2097, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2107, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2108, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2110, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2111, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2112, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2113, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2127, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2143, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2144, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2149, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2150, Reason: CUDA out of memory. Tried to allocate 168.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2151, Reason: CUDA out of memory. Tried to allocate 168.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 110.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2152, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2153, Reason: CUDA out of memory. Tried to allocate 176.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 124.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2154, Reason: CUDA out of memory. Tried to allocate 176.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2156, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2158, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2159, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2160, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2161, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2162, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2173, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2182, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2183, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2184, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2185, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2186, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2191, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2193, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2194, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2195, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2201, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2202, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2203, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2204, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.05 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2211, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2214, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2235, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2238, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2239, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2240, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2241, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2242, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2246, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2247, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2248, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2249, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2250, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2251, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2252, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2253, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2254, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2255, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2256, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2257, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2258, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2259, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2260, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2261, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2262, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2263, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2264, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2266, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2268, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2269, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2270, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2271, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 74.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2272, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2282, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2283, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2284, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2287, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2288, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2289, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2290, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2291, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2292, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2293, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2305, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2306, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2307, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2323, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2324, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2325, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2326, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2327, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2328, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2329, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2330, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2332, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2333, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2335, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2340, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2341, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2342, Reason: CUDA out of memory. Tried to allocate 542.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 198.69 MiB free; 3.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2354, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2368, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2372, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2373, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2374, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2375, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2376, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2377, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2378, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2379, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2380, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2381, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2382, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2383, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2384, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2385, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2386, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2387, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2388, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2389, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2390, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2394, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2408, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2424, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2425, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2426, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2432, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2433, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2434, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2435, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2437, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2444, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2445, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2446, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2447, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2448, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2449, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2464, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2465, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2466, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2474, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2475, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2476, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2477, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2478, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2479, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2480, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2481, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2483, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2490, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2491, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2493, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2494, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2495, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2506, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2508, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2511, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2512, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2523, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2524, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2525, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2527, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2528, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2533, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2536, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2542, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2544, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2545, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2546, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2547, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2563, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2564, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2565, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2571, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2573, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2574, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2575, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2576, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2579, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2585, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2586, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2587, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2588, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2589, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2595, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2596, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2597, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2598, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2615, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2616, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2617, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2618, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2619, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2620, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2621, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2622, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2623, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2624, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2625, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2626, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2627, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2632, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2633, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2634, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2635, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2636, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2640, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2641, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2642, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2643, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2649, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2651, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2655, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2656, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2663, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2664, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2665, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2673, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2674, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2681, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2684, Reason: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 250.69 MiB free; 3.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2685, Reason: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 250.69 MiB free; 3.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2687, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2688, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2689, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2690, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2691, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2692, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2693, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2694, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2695, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2702, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2709, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2710, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2711, Reason: CUDA out of memory. Tried to allocate 268.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 238.69 MiB free; 3.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2712, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2714, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2715, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2717, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2718, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2719, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2725, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2726, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2727, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2728, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2729, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2730, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2732, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2733, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2734, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2744, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2747, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2748, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2757, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2758, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2759, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2769, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2777, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2781, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 2.23 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2782, Reason: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 6; 23.70 GiB total capacity; 1.96 GiB already allocated; 132.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2783, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2784, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2785, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.22 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2786, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.08 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2787, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.15 GiB already allocated; 104.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2791, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2792, Reason: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 6; 23.70 GiB total capacity; 2.19 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2824, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2826, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2828, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2829, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2838, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2839, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2840, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2841, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2843, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2857, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2863, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2864, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2865, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2866, Reason: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2867, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 82.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2868, Reason: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 126.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2870, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 74.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2872, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2873, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2874, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2875, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.33 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2876, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2877, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.35 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2878, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2879, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2880, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2881, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2882, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2883, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2884, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2902, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2903, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2907, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2944, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2945, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2946, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2947, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2948, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2949, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2950, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2961, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2967, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2968, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2969, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2980, Reason: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2994, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2995, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2996, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2997, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2998, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3002, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3005, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3006, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3010, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3012, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3013, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3014, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3025, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3027, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3028, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3029, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3030, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3031, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3032, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3039, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3040, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3041, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3045, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3046, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3047, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3052, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3061, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3062, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3077, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3078, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3079, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3080, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3081, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3082, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3085, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3086, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3087, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3094, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3157, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3158, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3159, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3160, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3161, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3162, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3163, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3164, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3168, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3169, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3178, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3179, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3205, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3206, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3210, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3211, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3214, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3232, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3270, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3271, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3272, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3277, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3278, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 70.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3283, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3285, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3286, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3288, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3299, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3313, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3314, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3318, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3319, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3320, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3321, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3322, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3323, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3324, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3331, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3358, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3359, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3360, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3374, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3384, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3385, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3388, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3389, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3396, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3397, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3406, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3407, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3411, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3412, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3413, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3414, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3420, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3421, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3422, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3424, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3436, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3439, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3449, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3452, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3453, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3454, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3455, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3456, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3457, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3460, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3461, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3472, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3475, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3476, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3485, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3486, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3488, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3489, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3490, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3491, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3492, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.11 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3505, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3507, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3508, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3510, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3511, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3512, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3522, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3523, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3524, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3525, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3538, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3539, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3540, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3541, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3546, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3547, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3548, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3552, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3553, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3555, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3556, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3557, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3558, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3559, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.13 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3574, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3575, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3597, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3598, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3599, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3600, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3603, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3604, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3606, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3607, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3608, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3609, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3610, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3611, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3612, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3614, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3615, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3618, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3619, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3621, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3628, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3629, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3630, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3639, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3640, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3642, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3643, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3644, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3645, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3646, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3647, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3648, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3649, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3650, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3651, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3652, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3653, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3659, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3660, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3661, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3662, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3670, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3671, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3672, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3673, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3674, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3675, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3676, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3677, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3678, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 78.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3679, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3684, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3685, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3686, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3687, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3688, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3689, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3695, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3702, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3709, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3710, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3711, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3712, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3713, Reason: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3714, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3715, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3727, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3728, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3729, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3730, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3738, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3740, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3742, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3743, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3745, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3746, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3747, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3748, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3749, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3750, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3759, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3760, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3761, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3762, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3763, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3764, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3765, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3778, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3779, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3780, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3785, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3788, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3789, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3791, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3797, Reason: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3810, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3811, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3813, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3814, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3815, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3816, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3817, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3820, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3822, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3833, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3834, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3835, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3836, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3839, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3840, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3841, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3856, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3860, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3861, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3865, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3866, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3867, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3868, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3869, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3870, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3871, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3878, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3880, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3882, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3890, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3891, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3892, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3893, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3894, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3898, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3899, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3907, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3908, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3910, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3911, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3912, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3916, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3917, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3918, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3919, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3920, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3921, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3922, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3923, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3924, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3925, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3930, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3931, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3932, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3933, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3934, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3935, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3936, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3937, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3938, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3939, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3940, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3942, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3943, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3944, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3945, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3946, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3947, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3948, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3950, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3951, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3952, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3953, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3954, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3955, Reason: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 6; 23.70 GiB total capacity; 1.82 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3956, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.04 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3957, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 1.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3958, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.24 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3959, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 1.96 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3972, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3973, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3974, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3981, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3983, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3984, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3985, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3988, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3989, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3998, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3999, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4029, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4030, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4045, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4047, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4048, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4049, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4050, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4060, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4068, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4073, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4078, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4083, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 3.06 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4084, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4100, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4103, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.20 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4104, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4105, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4106, Reason: CUDA out of memory. Tried to allocate 382.00 MiB (GPU 6; 23.70 GiB total capacity; 2.23 GiB already allocated; 70.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4108, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4109, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4110, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4111, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4113, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4114, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4115, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4116, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4117, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4118, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4123, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4125, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4126, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4127, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4128, Reason: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4131, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4132, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4133, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4134, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4135, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4145, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4147, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4149, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4151, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4152, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4153, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4154, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4160, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4161, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4162, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4163, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4164, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4165, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4166, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4167, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4168, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4169, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4170, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4176, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4177, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4178, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4179, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4181, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4184, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4185, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4186, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4187, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4188, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4189, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4190, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4202, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4203, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4204, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4205, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4206, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4207, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4214, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4218, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4219, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4220, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4224, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4225, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4246, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4247, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4249, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4252, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4254, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4255, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4260, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4261, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4262, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4263, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4273, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4274, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4275, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4276, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4277, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4280, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4281, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4282, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4283, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4288, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4289, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4290, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4291, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4292, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4293, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4294, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4295, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4308, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4309, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4310, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4311, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4312, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4313, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4322, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4328, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4341, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4366, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4378, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4379, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4380, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4381, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4382, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4383, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4386, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4388, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4389, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4390, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4406, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4407, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4408, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4409, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4410, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4411, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4412, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4413, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4414, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4415, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4416, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4427, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4428, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4429, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4430, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4431, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4432, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4433, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4434, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4435, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4436, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4447, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4448, Reason: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4449, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4450, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4451, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4453, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4454, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4455, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4456, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4457, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4458, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4459, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4460, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 3.05 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4461, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 3.05 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4462, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4473, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4474, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4475, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4483, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4484, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4485, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4486, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4498, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4499, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4500, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4503, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4504, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4505, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4506, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4507, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4508, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4509, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4510, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4511, Reason: CUDA out of memory. Tried to allocate 194.00 MiB (GPU 6; 23.70 GiB total capacity; 2.25 GiB already allocated; 118.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4512, Reason: CUDA out of memory. Tried to allocate 238.00 MiB (GPU 6; 23.70 GiB total capacity; 2.17 GiB already allocated; 216.69 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4513, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4514, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4526, Reason: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4546, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4550, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4551, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4552, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4556, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4557, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4558, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4559, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4574, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4576, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4577, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4578, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4582, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4583, Reason: CUDA out of memory. Tried to allocate 162.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 84.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4584, Reason: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 126.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4585, Reason: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4586, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4587, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 114.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4588, Reason: CUDA out of memory. Tried to allocate 162.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4589, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4590, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4591, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4592, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4604, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4609, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4610, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4611, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4612, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4613, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4619, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4620, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4632, Reason: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 196.69 MiB free; 3.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4633, Reason: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4634, Reason: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 144.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4635, Reason: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 202.69 MiB free; 3.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4636, Reason: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4637, Reason: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4645, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4646, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4647, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4648, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4649, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4650, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4652, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4653, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4654, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4655, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4656, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4657, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4658, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4659, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4660, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4661, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4668, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4669, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4670, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4671, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4672, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4687, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4688, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4689, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4690, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4691, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4692, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4693, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4694, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4695, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4696, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4704, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4705, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4706, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4707, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4708, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4710, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4713, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4737, Reason: CUDA out of memory. Tried to allocate 312.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 276.69 MiB free; 3.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4738, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4739, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4742, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4743, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4745, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4747, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4748, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4749, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 70.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4750, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4751, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4755, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4757, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 100.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4762, Reason: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 124.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4763, Reason: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4764, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4769, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4783, Reason: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 112.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4784, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4790, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4791, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4803, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4825, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4826, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4827, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4828, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4829, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4830, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4832, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4833, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4834, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4835, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4836, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4837, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4838, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4843, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4844, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4845, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4846, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4847, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4865, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4866, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4868, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4879, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4880, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4881, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4882, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4883, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4884, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4887, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4888, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4889, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4890, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4891, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4892, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4899, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4900, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4901, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4902, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4903, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4910, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4911, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4912, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4913, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4920, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4936, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4942, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4943, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4944, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4945, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4946, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4947, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4948, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4949, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4950, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4951, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4959, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4961, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4962, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4964, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4965, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4966, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4976, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4977, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4978, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4979, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4980, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4981, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4986, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4994, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4995, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4997, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5001, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5005, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5006, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5008, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5009, Reason: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 6; 23.70 GiB total capacity; 2.10 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5010, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.26 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5011, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.35 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5012, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.23 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5017, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5028, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5029, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5042, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5043, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5044, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5045, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5046, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5047, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5055, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5057, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5064, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5065, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5066, Reason: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5067, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5068, Reason: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5069, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5070, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5071, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5072, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5073, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5087, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5089, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5090, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5093, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5102, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5104, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5105, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5106, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5119, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5121, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5122, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5123, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5124, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5126, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5134, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5143, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5144, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5146, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5162, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5164, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5165, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5170, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5194, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5201, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5202, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5203, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5204, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5205, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5209, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5211, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5212, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5215, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5216, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5219, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5226, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5227, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5228, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5229, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5230, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5238, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5239, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5240, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5241, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5242, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5250, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5265, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5266, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5267, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5268, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5269, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5270, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5275, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5276, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5277, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5278, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5279, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5288, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5289, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5290, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5291, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5292, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5293, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5294, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5295, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5296, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5297, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5304, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5311, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5319, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5321, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5322, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5323, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5324, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5325, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5326, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5327, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5331, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5346, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5347, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5348, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5349, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5350, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5351, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5352, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5353, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5355, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5356, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5357, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5358, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5359, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5360, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5362, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5363, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5366, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5372, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5373, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5374, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5391, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5392, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5393, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5409, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5417, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5420, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5435, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5437, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5438, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5440, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.21 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5441, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5451, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5453, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5459, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5460, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5461, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5466, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5467, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5468, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5469, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5470, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5471, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5475, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5476, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5477, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5492, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5496, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5499, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5501, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5502, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5503, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5504, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5530, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5531, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5532, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5533, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5534, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5535, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5536, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5545, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5549, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5550, Reason: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 6; 23.70 GiB total capacity; 2.07 GiB already allocated; 222.69 MiB free; 3.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5551, Reason: CUDA out of memory. Tried to allocate 152.00 MiB (GPU 6; 23.70 GiB total capacity; 2.25 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5552, Reason: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5553, Reason: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 23.70 GiB total capacity; 2.19 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5554, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.28 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5558, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5559, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5562, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5564, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5566, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5570, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5571, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5572, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5586, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5595, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5596, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5597, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5610, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5613, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5614, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5615, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5616, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5617, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5624, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5625, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5631, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5634, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5647, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5648, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5652, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5655, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5658, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5669, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5677, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5678, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5681, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5682, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5707, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5719, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5730, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5743, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5744, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5745, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5746, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5747, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5749, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5750, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5755, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5776, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5792, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5793, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5794, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5795, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5796, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5797, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5798, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5799, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5800, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5835, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5843, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5844, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5845, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5846, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5848, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5859, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5866, Reason: CUDA out of memory. Tried to allocate 332.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5867, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5868, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5872, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5873, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5874, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5875, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5905, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5909, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5913, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5928, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5929, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5930, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 88.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5939, Reason: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 114.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5940, Reason: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 6; 23.70 GiB total capacity; 2.22 GiB already allocated; 140.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5941, Reason: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 6; 23.70 GiB total capacity; 2.29 GiB already allocated; 110.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5942, Reason: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 6; 23.70 GiB total capacity; 2.14 GiB already allocated; 140.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5943, Reason: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 6; 23.70 GiB total capacity; 2.13 GiB already allocated; 128.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5944, Reason: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 6; 23.70 GiB total capacity; 2.12 GiB already allocated; 84.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5950, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5951, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5952, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5953, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5954, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5955, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5957, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5958, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5960, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5961, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5968, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5972, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5973, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5974, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5980, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5984, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5990, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5991, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5992, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5993, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5994, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5995, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5996, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5997, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5998, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5999, Reason: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6000, Reason: CUDA out of memory. Tried to allocate 140.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 96.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6001, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6002, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6003, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6004, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6005, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6014, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6015, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6020, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6021, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6022, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6023, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6024, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6025, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6026, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6027, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6028, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6029, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6037, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6039, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6041, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6042, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6056, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6067, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6078, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6079, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6081, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6083, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6096, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6097, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 70.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6098, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6099, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6108, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6109, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6110, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6133, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6135, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6136, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6162, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6165, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6166, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6167, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6168, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6169, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6170, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6171, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6182, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6183, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6184, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6187, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6220, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6221, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6222, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6223, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6224, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6225, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6226, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6228, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6242, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6243, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6244, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6263, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6265, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6266, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6267, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6268, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6269, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6270, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6271, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6287, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6288, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6291, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6292, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6293, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6308, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6320, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6321, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6323, Reason: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6324, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6328, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6335, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6337, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6338, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6350, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6351, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6352, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6353, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6354, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6355, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6358, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6359, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 74.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6363, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6365, Reason: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 102.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6366, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6367, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6368, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6369, Reason: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6370, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 88.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6375, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6376, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6377, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6378, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6379, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6380, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6381, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6382, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6383, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6384, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6385, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6386, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6387, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6388, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6391, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6392, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6400, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6401, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6402, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6403, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6404, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6405, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6409, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6410, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6411, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6415, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6416, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6418, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6419, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6420, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6421, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6422, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6423, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6424, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6425, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6426, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6427, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.08 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6428, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6429, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6430, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6431, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6432, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6433, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 3.06 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6434, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6435, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 3.04 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6436, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.08 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6437, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.08 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6438, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6439, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6440, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6441, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6442, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6443, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.05 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6456, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6472, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6475, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6478, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6479, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6494, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6497, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6503, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6504, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6505, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6507, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6521, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6522, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6523, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6524, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6525, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6526, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6529, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6536, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6537, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6538, Reason: CUDA out of memory. Tried to allocate 326.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 180.69 MiB free; 3.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6541, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6544, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6545, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6546, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6547, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6549, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6550, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6565, Reason: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6576, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6579, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6580, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6587, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6588, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6597, Reason: CUDA out of memory. Tried to allocate 142.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6598, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6599, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6600, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6602, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6603, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6604, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6605, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6606, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6610, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6615, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6618, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6619, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6621, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6624, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6629, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6630, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6638, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6647, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6652, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6653, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6654, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6655, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6656, Reason: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 102.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6671, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6672, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6673, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6674, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6675, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6676, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6677, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6678, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6679, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6681, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6682, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6683, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6684, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6685, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6694, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6695, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6696, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6727, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6728, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6729, Reason: CUDA out of memory. Tried to allocate 152.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 114.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6730, Reason: CUDA out of memory. Tried to allocate 152.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 114.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6731, Reason: CUDA out of memory. Tried to allocate 152.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 114.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6732, Reason: CUDA out of memory. Tried to allocate 152.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 114.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6741, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6742, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6743, Reason: CUDA out of memory. Tried to allocate 140.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6744, Reason: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 6; 23.70 GiB total capacity; 2.33 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6745, Reason: CUDA out of memory. Tried to allocate 118.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6746, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6747, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6754, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6760, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6765, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6767, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6769, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6772, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6773, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6774, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6790, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6794, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6811, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6812, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6813, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6815, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6821, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6822, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6823, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6824, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6830, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6832, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6833, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6840, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6842, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6843, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6844, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6860, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6861, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6862, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6864, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6865, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6868, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6869, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6873, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6876, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6877, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6878, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6898, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6899, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6900, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6901, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6908, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6911, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6912, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6917, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6918, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6921, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6923, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6926, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6927, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6929, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6930, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6931, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6932, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6936, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6937, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6938, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6939, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6946, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6947, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6955, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6957, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6958, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6959, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6960, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6962, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6963, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6964, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6971, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6973, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.10 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6974, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6983, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6984, Reason: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 98.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6985, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.19 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6995, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7005, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7006, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.29 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7007, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7008, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7009, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7010, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7015, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7020, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7021, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7022, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7023, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7024, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7025, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7026, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7034, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7035, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7045, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7046, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7047, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7053, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7083, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7084, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7094, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7095, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 70.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7109, Reason: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 136.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7111, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7112, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7113, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7114, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 90.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7115, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7116, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7117, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7123, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7124, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7129, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7138, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7152, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7153, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7165, Reason: CUDA out of memory. Tried to allocate 146.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 138.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7173, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7178, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7180, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7181, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7182, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7183, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7189, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7193, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7198, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7199, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7200, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 74.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7215, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7218, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7231, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7235, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7236, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7238, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7239, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7254, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7255, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7256, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7259, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 102.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7260, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7261, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7263, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7264, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7275, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7276, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7277, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7281, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7282, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7283, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7284, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7285, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7289, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7292, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7316, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7320, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7322, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7328, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7329, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7330, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7332, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7346, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7350, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7351, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7352, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7353, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7354, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7355, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7356, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7357, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7358, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7367, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7370, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7371, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7372, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7376, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7377, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7386, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7387, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7388, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7397, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7399, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7401, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7402, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7405, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7417, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7422, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7423, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7424, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7425, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7426, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7434, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7442, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7452, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7456, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7457, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7470, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7471, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7472, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7481, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7485, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7486, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7502, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7503, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7504, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7505, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7506, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7511, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7513, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7517, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7530, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7533, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7534, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7538, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7547, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7548, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7551, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7556, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7557, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7558, Reason: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 96.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7559, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 96.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7563, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7564, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7565, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7570, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7571, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7572, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7573, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7581, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7582, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7590, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7591, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7592, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7599, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7600, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7626, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7635, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7636, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7637, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7641, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7642, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7643, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7657, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7665, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7667, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7670, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7672, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7679, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7680, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7681, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7685, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7686, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7696, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7697, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7698, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7699, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7700, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7701, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7702, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7703, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7712, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7713, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7715, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7716, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7748, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7762, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7763, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7775, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7781, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7782, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7783, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7784, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7785, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7788, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7789, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7790, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7791, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7792, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7793, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7794, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7795, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7798, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7805, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7806, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7807, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7808, Reason: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7809, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7828, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7829, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7830, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7836, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7839, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7841, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7842, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7845, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7846, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7847, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7848, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7849, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7857, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7858, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7859, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7864, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 84.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7865, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 84.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7871, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7872, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7889, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7890, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7891, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7892, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7893, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7895, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7896, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7897, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7898, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7899, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7900, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 82.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7901, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7902, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7903, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7904, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7905, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7923, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7924, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7925, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7926, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7927, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7931, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7932, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7933, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7936, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7937, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7938, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7947, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7948, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7949, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7961, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7964, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7966, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7976, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7983, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7984, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7985, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7986, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7987, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7988, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7989, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7990, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7998, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8001, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8003, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8005, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8007, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8008, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8009, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8010, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8017, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8019, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8028, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8029, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8040, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8041, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8042, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8043, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8044, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8045, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8046, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8051, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8052, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8057, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8058, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8059, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8060, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8062, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8076, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8077, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8082, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8085, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8086, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8087, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8088, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8089, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8110, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8122, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8123, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8124, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8128, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8131, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8132, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8133, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8134, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8135, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8136, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8137, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 70.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8151, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8153, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8154, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8155, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8159, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8160, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8171, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8181, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8197, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8198, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8199, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8200, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8206, Reason: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 96.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8207, Reason: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8208, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.31 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8217, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8236, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8249, Reason: CUDA out of memory. Tried to allocate 568.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 356.69 MiB free; 2.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8262, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 3.05 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8292, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8293, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8294, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8304, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8305, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8306, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8307, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8319, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8320, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8321, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8323, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8324, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8330, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8331, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8333, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8355, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8376, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8377, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8379, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8380, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8381, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8382, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8387, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8388, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8391, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8392, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8393, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8394, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8395, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8406, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8409, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8410, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8411, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8412, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8413, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8414, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8423, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8427, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8428, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8429, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8434, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8435, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8438, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8439, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8440, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8441, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8442, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8443, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8453, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8454, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8458, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8459, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8460, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8467, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8474, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8483, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8495, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8512, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8514, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8515, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8522, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8528, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8529, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8530, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8531, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8532, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8533, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8534, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8535, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8539, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8551, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8552, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8553, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8554, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8555, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8556, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8557, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8561, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8567, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8569, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8571, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8577, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8578, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8579, Reason: CUDA out of memory. Tried to allocate 234.00 MiB (GPU 6; 23.70 GiB total capacity; 2.03 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8582, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8583, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8584, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8585, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8615, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8616, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8617, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8618, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8619, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8620, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8621, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8622, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8623, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8627, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8632, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8638, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8654, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8655, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8656, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8657, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8659, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8674, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8675, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8676, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8677, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8678, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8687, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8692, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8693, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8710, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8711, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8712, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8713, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8714, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8738, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8739, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8740, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8743, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8744, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8745, Reason: CUDA out of memory. Tried to allocate 158.00 MiB (GPU 6; 23.70 GiB total capacity; 2.19 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8746, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8749, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8750, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8755, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8756, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8761, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 70.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8762, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 74.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8763, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8775, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8776, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8777, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.27 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8778, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8793, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8794, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8795, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8796, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8797, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8805, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8806, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8826, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8827, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8828, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8859, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8860, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8862, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8871, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 70.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8881, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8882, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8888, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8896, Reason: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 96.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8897, Reason: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8907, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8909, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8910, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8932, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8933, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8934, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8935, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8936, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8937, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8940, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8947, Reason: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 74.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8952, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8962, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8966, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8967, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8968, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8972, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8978, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8985, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8986, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8987, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8988, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8989, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8990, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8992, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9002, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
2022-12-11 15:46:29,453 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `perturb_test_orig` split: 0.000
2022-12-11 19:22:43,165 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `perturb_test_changed` split: 0.000
2022-12-11 20:33:46,398 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `perturb_dev_orig` split: 0.000
2022-12-11 21:44:19,124 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `perturb_dev_changed` split: 0.000
Skipping evaluation batch 9003, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9004, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9005, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9006, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9007, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9008, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9009, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9011, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9012, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9021, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9022, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9023, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9024, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9025, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9034, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9035, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9042, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9045, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9046, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9054, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 1.96 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9055, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 1.87 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9056, Reason: CUDA out of memory. Tried to allocate 246.00 MiB (GPU 6; 23.70 GiB total capacity; 1.72 GiB already allocated; 148.69 MiB free; 3.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9059, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9060, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9061, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9063, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9064, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9065, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9066, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9079, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9080, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9090, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9091, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9092, Reason: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 70.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9103, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9104, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9105, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9106, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9114, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9118, Reason: CUDA out of memory. Tried to allocate 400.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9129, Reason: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 120.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9130, Reason: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9131, Reason: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9132, Reason: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9145, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9146, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9147, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9148, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9150, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9171, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9172, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9173, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9174, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9177, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9178, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9190, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9192, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9193, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9205, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9206, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.25 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9211, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9212, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9213, Reason: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 132.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9218, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9224, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9225, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9226, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9227, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9234, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9236, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9237, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9238, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9239, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9240, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9241, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9260, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9261, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9264, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9271, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9272, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9273, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9274, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9275, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9276, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9277, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9278, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9279, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9280, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9281, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9282, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9283, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9284, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9286, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9287, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9295, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9296, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9308, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9309, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9310, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9311, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9312, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9319, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9320, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9321, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 90.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9322, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9323, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9333, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9355, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9356, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9358, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9359, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9376, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9377, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9384, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9385, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9386, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9387, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9388, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9391, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9392, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9394, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9409, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9410, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9411, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9412, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9413, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9418, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9419, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9420, Reason: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 78.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9421, Reason: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9423, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 3.06 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9424, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 3.06 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9430, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9434, Reason: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9439, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9440, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9441, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9442, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9443, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9444, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9451, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9460, Reason: CUDA out of memory. Tried to allocate 808.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 172.69 MiB free; 3.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9461, Reason: CUDA out of memory. Tried to allocate 1.27 GiB (GPU 6; 23.70 GiB total capacity; 1.11 GiB already allocated; 980.69 MiB free; 2.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9462, Reason: CUDA out of memory. Tried to allocate 946.00 MiB (GPU 6; 23.70 GiB total capacity; 2.01 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9464, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9465, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9466, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9467, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9468, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9477, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9478, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9479, Reason: CUDA out of memory. Tried to allocate 292.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9480, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9481, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9482, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9483, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9484, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9485, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9494, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 1.68 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9500, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9508, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9516, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9557, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 84.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9558, Reason: CUDA out of memory. Tried to allocate 230.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 226.69 MiB free; 3.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9559, Reason: CUDA out of memory. Tried to allocate 454.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9571, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9572, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9573, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9596, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9597, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9598, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9599, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9600, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9601, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9605, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9606, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9607, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9608, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9609, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9610, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9611, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9612, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9613, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9620, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9621, Reason: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 132.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9628, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9629, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9631, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9632, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9633, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9634, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9635, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9643, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9644, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9659, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9660, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9661, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9662, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9664, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9665, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9667, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9677, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9688, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9689, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9706, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9707, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9713, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9724, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9725, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9726, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9727, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9728, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9729, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9730, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9731, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9732, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9733, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9734, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9735, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9736, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9737, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9738, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9749, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9754, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9755, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9764, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9765, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9766, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9767, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9768, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9769, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9779, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9780, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9781, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9782, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9783, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9784, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9785, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9786, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9787, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9788, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9789, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9797, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9798, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9799, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9800, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9801, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9809, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9810, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9814, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9815, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9816, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9831, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9832, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9835, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9836, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9837, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9844, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9845, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9849, Reason: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9862, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9863, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9864, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9865, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9866, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9867, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9868, Reason: CUDA out of memory. Tried to allocate 278.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 102.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9869, Reason: CUDA out of memory. Tried to allocate 228.00 MiB (GPU 6; 23.70 GiB total capacity; 2.29 GiB already allocated; 102.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9870, Reason: CUDA out of memory. Tried to allocate 228.00 MiB (GPU 6; 23.70 GiB total capacity; 2.29 GiB already allocated; 102.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9871, Reason: CUDA out of memory. Tried to allocate 310.00 MiB (GPU 6; 23.70 GiB total capacity; 2.25 GiB already allocated; 268.69 MiB free; 3.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9872, Reason: CUDA out of memory. Tried to allocate 244.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 102.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9880, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9881, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9882, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9883, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9884, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9885, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9886, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9887, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9888, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9896, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9897, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9898, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9899, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9907, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9931, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9934, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9935, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9962, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9969, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9972, Reason: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9973, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9974, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9975, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9977, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9989, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9990, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9991, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9993, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9994, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9995, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9996, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9997, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9998, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10000, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10001, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10013, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10014, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10015, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10016, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10017, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10018, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10019, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 100.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10020, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 100.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10028, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10030, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10031, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10032, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10033, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10036, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10037, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10038, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10039, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10040, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10041, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10042, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10043, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10044, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10050, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10052, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10053, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10054, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10055, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10056, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10057, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10058, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10059, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10060, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10061, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10069, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10070, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10072, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10074, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10075, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10082, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10083, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10084, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10085, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10101, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10102, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10103, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10104, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10106, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10112, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10113, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10114, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10115, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10116, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10117, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10143, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10149, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10150, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10151, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 88.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10152, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10153, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10155, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10156, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.33 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10157, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10158, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10159, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10161, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10162, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10163, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10164, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10165, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10166, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10176, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10177, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10181, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10182, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10183, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10184, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10186, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10187, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10190, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10191, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10192, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10193, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10212, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10213, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10214, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10215, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10216, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10217, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10218, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10219, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10220, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10225, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10226, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10243, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10244, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10245, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10246, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10251, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10265, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10266, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10267, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10268, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10282, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10289, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10292, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10293, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10294, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10295, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10342, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10343, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10344, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10351, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10362, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10363, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10364, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10365, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10366, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10367, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10372, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10373, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10374, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10375, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10376, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10397, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10398, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10399, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10407, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10408, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10419, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10423, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10424, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10425, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10426, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10427, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10428, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10430, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10432, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10433, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10434, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10438, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10439, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10440, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10444, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10445, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10449, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10450, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10461, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10463, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10464, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10465, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10466, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10467, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10473, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10474, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10475, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10476, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10477, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10478, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10480, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10481, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10492, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10493, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10496, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10497, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10498, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10499, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10500, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10512, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10516, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10538, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10539, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10540, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10541, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10542, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10543, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10545, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10546, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10547, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10548, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10552, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10558, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10559, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10560, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10561, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10562, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10565, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10566, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10567, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10568, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10569, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10570, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
Skipping evaluation batch 6, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 7, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 11, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 18, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 19, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 37, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 42, Reason: CUDA out of memory. Tried to allocate 216.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 208.69 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 43, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 44, Reason: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 6; 23.70 GiB total capacity; 2.27 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 45, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 54, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 55, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 56, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 57, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 58, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 78, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 79, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 80, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 81, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 82, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 83, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 84, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 96, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 99, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 100, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 101, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 102, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 103, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 104, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 105, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 106, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 116, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 122, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 125, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 126, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 130, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 140, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 141, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 142, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 143, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 144, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 145, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 146, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 157, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 88.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 159, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 160, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 161, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 171, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 173, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 174, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 175, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 176, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 177, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 178, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 179, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 180, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 181, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 182, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 183, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 222, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 223, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 224, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 225, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 226, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 227, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 228, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 233, Reason: CUDA out of memory. Tried to allocate 416.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 240, Reason: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 6; 23.70 GiB total capacity; 2.35 GiB already allocated; 120.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 241, Reason: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 6; 23.70 GiB total capacity; 2.35 GiB already allocated; 120.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 242, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 243, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 244, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 248, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 249, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 250, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 251, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 252, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 253, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 256, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 257, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 258, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 259, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 266, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 267, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 268, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 269, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 270, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 273, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 274, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 275, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 297, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 298, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 299, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 303, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 305, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 306, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 307, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 308, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 319, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 320, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 325, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 326, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 327, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 335, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 336, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 337, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 338, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 363, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 364, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 365, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 369, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 370, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 375, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 376, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 377, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 378, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 382, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 387, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 388, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 389, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 390, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 391, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 392, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 393, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 394, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 395, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 396, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 397, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 398, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 399, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 400, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 401, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 402, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 403, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 404, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 405, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 429, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 430, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 431, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 432, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 433, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 434, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 435, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 439, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 454, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 455, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 456, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 472, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 473, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 475, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 476, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 477, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 478, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 483, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 484, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 485, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 486, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 487, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 488, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 489, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 490, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 491, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 492, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 528, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 529, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 544, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 546, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 547, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 550, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 551, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 552, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 553, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 554, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 555, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 559, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 560, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 561, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 562, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 563, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 564, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 567, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 568, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 569, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 570, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 571, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 572, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 577, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 579, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 580, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 581, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 582, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 583, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 587, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 589, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 590, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 591, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 592, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 593, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 594, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 595, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 596, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 597, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 598, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 599, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 600, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 601, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 602, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 603, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 604, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 608, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 609, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 610, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 611, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 612, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 613, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 614, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 615, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 616, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 617, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 618, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 619, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 620, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 621, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 630, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 631, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 632, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 633, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 634, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 635, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 636, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 637, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 647, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 648, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 649, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 650, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 651, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 654, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 669, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 676, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 698, Reason: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 108.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 699, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 700, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 701, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 703, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 710, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 712, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 713, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 714, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 725, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 726, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 748, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 750, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 767, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 768, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 769, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 770, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 782, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 783, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.29 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 784, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 785, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 786, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 787, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 791, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 792, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 807, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 812, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 813, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 814, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 815, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 816, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 820, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 821, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 822, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 824, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 833, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 836, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 837, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 847, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 848, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 858, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 873, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 874, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 875, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 876, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 880, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 887, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 888, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 889, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 890, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 892, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 902, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 903, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 906, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 907, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 908, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 909, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 910, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 911, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 912, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 913, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 914, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 915, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 918, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 919, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 920, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 921, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 927, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 938, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 939, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 940, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 946, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 948, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 949, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 963, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 964, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 965, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 972, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 973, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 974, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 975, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 994, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 995, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 996, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 999, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1000, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1001, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1002, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1003, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1004, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1005, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1006, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1007, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1008, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1009, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1010, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1011, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1012, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1013, Reason: CUDA out of memory. Tried to allocate 446.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 280.69 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1014, Reason: CUDA out of memory. Tried to allocate 446.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 280.69 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1015, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1016, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1017, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1018, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1019, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1020, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1021, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1022, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1023, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1038, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1039, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1046, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1047, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1068, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1076, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1077, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1078, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1079, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1080, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1082, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1090, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1091, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1092, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1093, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1193, Reason: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 6; 23.70 GiB total capacity; 5.71 GiB already allocated; 54.69 MiB free; 6.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1194, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 5.82 GiB already allocated; 42.69 MiB free; 6.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1199, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 5.44 GiB already allocated; 60.69 MiB free; 6.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1204, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 6.35 GiB already allocated; 8.69 MiB free; 7.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1205, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 6.22 GiB already allocated; 8.69 MiB free; 7.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1206, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 6.22 GiB already allocated; 8.69 MiB free; 7.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1207, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 6.18 GiB already allocated; 8.69 MiB free; 7.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1260, Reason: CUDA out of memory. Tried to allocate 390.00 MiB (GPU 6; 23.70 GiB total capacity; 20.28 GiB already allocated; 71.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1261, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 20.23 GiB already allocated; 53.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1262, Reason: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 6; 23.70 GiB total capacity; 19.96 GiB already allocated; 53.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1491, Reason: CUDA out of memory. Tried to allocate 624.00 MiB (GPU 6; 23.70 GiB total capacity; 20.21 GiB already allocated; 103.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1726, Reason: CUDA out of memory. Tried to allocate 420.00 MiB (GPU 6; 23.70 GiB total capacity; 19.87 GiB already allocated; 65.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1927, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 19.17 GiB already allocated; 51.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1928, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 18.62 GiB already allocated; 11.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1965, Reason: CUDA out of memory. Tried to allocate 446.00 MiB (GPU 6; 23.70 GiB total capacity; 18.97 GiB already allocated; 417.69 MiB free; 21.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1966, Reason: CUDA out of memory. Tried to allocate 186.00 MiB (GPU 6; 23.70 GiB total capacity; 21.08 GiB already allocated; 183.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2154, Reason: CUDA out of memory. Tried to allocate 204.00 MiB (GPU 6; 23.70 GiB total capacity; 19.13 GiB already allocated; 189.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2333, Reason: CUDA out of memory. Tried to allocate 212.00 MiB (GPU 6; 23.70 GiB total capacity; 18.99 GiB already allocated; 117.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2342, Reason: CUDA out of memory. Tried to allocate 542.00 MiB (GPU 6; 23.70 GiB total capacity; 20.58 GiB already allocated; 189.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2684, Reason: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 6; 23.70 GiB total capacity; 19.87 GiB already allocated; 251.69 MiB free; 21.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2685, Reason: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 6; 23.70 GiB total capacity; 19.87 GiB already allocated; 251.69 MiB free; 21.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2782, Reason: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 6; 23.70 GiB total capacity; 20.63 GiB already allocated; 181.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2785, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 19.67 GiB already allocated; 77.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2787, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 19.88 GiB already allocated; 69.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2792, Reason: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 6; 23.70 GiB total capacity; 20.26 GiB already allocated; 257.69 MiB free; 21.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3621, Reason: CUDA out of memory. Tried to allocate 780.00 MiB (GPU 6; 23.70 GiB total capacity; 20.28 GiB already allocated; 207.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4105, Reason: CUDA out of memory. Tried to allocate 282.00 MiB (GPU 6; 23.70 GiB total capacity; 20.39 GiB already allocated; 17.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4106, Reason: CUDA out of memory. Tried to allocate 382.00 MiB (GPU 6; 23.70 GiB total capacity; 20.08 GiB already allocated; 89.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4448, Reason: CUDA out of memory. Tried to allocate 482.00 MiB (GPU 6; 23.70 GiB total capacity; 20.39 GiB already allocated; 53.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4484, Reason: CUDA out of memory. Tried to allocate 564.00 MiB (GPU 6; 23.70 GiB total capacity; 19.35 GiB already allocated; 127.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4485, Reason: CUDA out of memory. Tried to allocate 354.00 MiB (GPU 6; 23.70 GiB total capacity; 18.95 GiB already allocated; 127.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4510, Reason: CUDA out of memory. Tried to allocate 312.00 MiB (GPU 6; 23.70 GiB total capacity; 18.74 GiB already allocated; 245.69 MiB free; 21.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4511, Reason: CUDA out of memory. Tried to allocate 194.00 MiB (GPU 6; 23.70 GiB total capacity; 18.90 GiB already allocated; 109.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4512, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 18.51 GiB already allocated; 21.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4633, Reason: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 6; 23.70 GiB total capacity; 20.05 GiB already allocated; 89.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4634, Reason: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 6; 23.70 GiB total capacity; 20.05 GiB already allocated; 31.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4635, Reason: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 6; 23.70 GiB total capacity; 20.05 GiB already allocated; 31.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4636, Reason: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 6; 23.70 GiB total capacity; 20.05 GiB already allocated; 31.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4637, Reason: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 6; 23.70 GiB total capacity; 20.05 GiB already allocated; 31.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5550, Reason: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 6; 23.70 GiB total capacity; 19.85 GiB already allocated; 95.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5551, Reason: CUDA out of memory. Tried to allocate 152.00 MiB (GPU 6; 23.70 GiB total capacity; 18.24 GiB already allocated; 95.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5552, Reason: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 6; 23.70 GiB total capacity; 18.63 GiB already allocated; 55.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5553, Reason: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 23.70 GiB total capacity; 17.99 GiB already allocated; 55.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5866, Reason: CUDA out of memory. Tried to allocate 996.00 MiB (GPU 6; 23.70 GiB total capacity; 18.04 GiB already allocated; 921.69 MiB free; 20.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5868, Reason: CUDA out of memory. Tried to allocate 418.00 MiB (GPU 6; 23.70 GiB total capacity; 17.29 GiB already allocated; 417.69 MiB free; 21.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5943, Reason: CUDA out of memory. Tried to allocate 418.00 MiB (GPU 6; 23.70 GiB total capacity; 17.98 GiB already allocated; 33.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5944, Reason: CUDA out of memory. Tried to allocate 268.00 MiB (GPU 6; 23.70 GiB total capacity; 17.68 GiB already allocated; 33.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6323, Reason: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 6; 23.70 GiB total capacity; 20.77 GiB already allocated; 55.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6324, Reason: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 6; 23.70 GiB total capacity; 20.73 GiB already allocated; 43.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6538, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 20.76 GiB already allocated; 23.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6728, Reason: CUDA out of memory. Tried to allocate 244.00 MiB (GPU 6; 23.70 GiB total capacity; 20.51 GiB already allocated; 35.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6729, Reason: CUDA out of memory. Tried to allocate 452.00 MiB (GPU 6; 23.70 GiB total capacity; 20.07 GiB already allocated; 35.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6730, Reason: CUDA out of memory. Tried to allocate 452.00 MiB (GPU 6; 23.70 GiB total capacity; 20.07 GiB already allocated; 35.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6731, Reason: CUDA out of memory. Tried to allocate 452.00 MiB (GPU 6; 23.70 GiB total capacity; 20.07 GiB already allocated; 35.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6732, Reason: CUDA out of memory. Tried to allocate 452.00 MiB (GPU 6; 23.70 GiB total capacity; 20.07 GiB already allocated; 35.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8199, Reason: CUDA out of memory. Tried to allocate 204.00 MiB (GPU 6; 23.70 GiB total capacity; 18.84 GiB already allocated; 53.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8249, Reason: CUDA out of memory. Tried to allocate 568.00 MiB (GPU 6; 23.70 GiB total capacity; 21.17 GiB already allocated; 129.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8578, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 19.04 GiB already allocated; 169.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8579, Reason: CUDA out of memory. Tried to allocate 234.00 MiB (GPU 6; 23.70 GiB total capacity; 19.24 GiB already allocated; 163.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8744, Reason: CUDA out of memory. Tried to allocate 604.00 MiB (GPU 6; 23.70 GiB total capacity; 18.85 GiB already allocated; 225.69 MiB free; 21.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8761, Reason: CUDA out of memory. Tried to allocate 892.00 MiB (GPU 6; 23.70 GiB total capacity; 15.99 GiB already allocated; 47.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 8762, Reason: CUDA out of memory. Tried to allocate 316.00 MiB (GPU 6; 23.70 GiB total capacity; 16.52 GiB already allocated; 273.69 MiB free; 21.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9054, Reason: CUDA out of memory. Tried to allocate 410.00 MiB (GPU 6; 23.70 GiB total capacity; 18.15 GiB already allocated; 353.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9055, Reason: CUDA out of memory. Tried to allocate 364.00 MiB (GPU 6; 23.70 GiB total capacity; 17.66 GiB already allocated; 353.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9118, Reason: CUDA out of memory. Tried to allocate 400.00 MiB (GPU 6; 23.70 GiB total capacity; 18.60 GiB already allocated; 357.69 MiB free; 21.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9286, Reason: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 6; 23.70 GiB total capacity; 18.10 GiB already allocated; 241.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9287, Reason: CUDA out of memory. Tried to allocate 964.00 MiB (GPU 6; 23.70 GiB total capacity; 17.02 GiB already allocated; 241.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9333, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 20.43 GiB already allocated; 75.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9460, Reason: CUDA out of memory. Tried to allocate 808.00 MiB (GPU 6; 23.70 GiB total capacity; 21.24 GiB already allocated; 199.69 MiB free; 21.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9461, Reason: CUDA out of memory. Tried to allocate 5.08 GiB (GPU 6; 23.70 GiB total capacity; 13.88 GiB already allocated; 4.58 GiB free; 17.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9462, Reason: CUDA out of memory. Tried to allocate 3.69 GiB (GPU 6; 23.70 GiB total capacity; 13.54 GiB already allocated; 2.16 GiB free; 19.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9478, Reason: CUDA out of memory. Tried to allocate 254.00 MiB (GPU 6; 23.70 GiB total capacity; 21.17 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9479, Reason: CUDA out of memory. Tried to allocate 292.00 MiB (GPU 6; 23.70 GiB total capacity; 21.06 GiB already allocated; 1.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9557, Reason: CUDA out of memory. Tried to allocate 660.00 MiB (GPU 6; 23.70 GiB total capacity; 20.66 GiB already allocated; 181.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9558, Reason: CUDA out of memory. Tried to allocate 242.00 MiB (GPU 6; 23.70 GiB total capacity; 19.38 GiB already allocated; 181.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9559, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 20.81 GiB already allocated; 67.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9869, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 20.82 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9870, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 20.82 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9871, Reason: CUDA out of memory. Tried to allocate 310.00 MiB (GPU 6; 23.70 GiB total capacity; 18.58 GiB already allocated; 195.69 MiB free; 21.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9872, Reason: CUDA out of memory. Tried to allocate 244.00 MiB (GPU 6; 23.70 GiB total capacity; 20.76 GiB already allocated; 19.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10493, Reason: CUDA out of memory. Tried to allocate 262.00 MiB (GPU 6; 23.70 GiB total capacity; 18.88 GiB already allocated; 205.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
Skipping evaluation batch 200, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 18.84 GiB already allocated; 53.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 667, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 19.87 GiB already allocated; 15.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 668, Reason: CUDA out of memory. Tried to allocate 258.00 MiB (GPU 6; 23.70 GiB total capacity; 18.68 GiB already allocated; 53.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1619, Reason: CUDA out of memory. Tried to allocate 218.00 MiB (GPU 6; 23.70 GiB total capacity; 19.86 GiB already allocated; 163.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1620, Reason: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 6; 23.70 GiB total capacity; 19.59 GiB already allocated; 71.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1621, Reason: CUDA out of memory. Tried to allocate 268.00 MiB (GPU 6; 23.70 GiB total capacity; 18.53 GiB already allocated; 3.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1622, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 18.65 GiB already allocated; 43.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2510, Reason: CUDA out of memory. Tried to allocate 140.00 MiB (GPU 6; 23.70 GiB total capacity; 20.34 GiB already allocated; 127.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2511, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 20.32 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2512, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 19.25 GiB already allocated; 5.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2562, Reason: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 6; 23.70 GiB total capacity; 19.99 GiB already allocated; 175.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3301, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 18.73 GiB already allocated; 37.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3402, Reason: CUDA out of memory. Tried to allocate 1022.00 MiB (GPU 6; 23.70 GiB total capacity; 19.02 GiB already allocated; 801.69 MiB free; 21.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3404, Reason: CUDA out of memory. Tried to allocate 352.00 MiB (GPU 6; 23.70 GiB total capacity; 18.55 GiB already allocated; 305.69 MiB free; 21.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3405, Reason: CUDA out of memory. Tried to allocate 212.00 MiB (GPU 6; 23.70 GiB total capacity; 18.03 GiB already allocated; 59.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3563, Reason: CUDA out of memory. Tried to allocate 740.00 MiB (GPU 6; 23.70 GiB total capacity; 18.82 GiB already allocated; 639.69 MiB free; 21.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3565, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 18.88 GiB already allocated; 31.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4425, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 18.88 GiB already allocated; 9.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4472, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 19.98 GiB already allocated; 33.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4785, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 19.67 GiB already allocated; 41.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4786, Reason: CUDA out of memory. Tried to allocate 470.00 MiB (GPU 6; 23.70 GiB total capacity; 18.53 GiB already allocated; 323.69 MiB free; 21.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5028, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 19.90 GiB already allocated; 41.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5210, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 19.90 GiB already allocated; 53.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5211, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 19.90 GiB already allocated; 53.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5212, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 19.90 GiB already allocated; 53.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5213, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 19.90 GiB already allocated; 53.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
Skipping evaluation batch 200, Reason: CUDA out of memory. Tried to allocate 264.00 MiB (GPU 6; 23.70 GiB total capacity; 17.71 GiB already allocated; 147.69 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 667, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 19.36 GiB already allocated; 51.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 668, Reason: CUDA out of memory. Tried to allocate 514.00 MiB (GPU 6; 23.70 GiB total capacity; 17.93 GiB already allocated; 149.69 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1619, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 19.48 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1620, Reason: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 6; 23.70 GiB total capacity; 18.64 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1621, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 18.13 GiB already allocated; 1.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1622, Reason: CUDA out of memory. Tried to allocate 344.00 MiB (GPU 6; 23.70 GiB total capacity; 17.91 GiB already allocated; 65.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2510, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 20.33 GiB already allocated; 15.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2511, Reason: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 6; 23.70 GiB total capacity; 19.93 GiB already allocated; 15.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2562, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 20.32 GiB already allocated; 43.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3167, Reason: CUDA out of memory. Tried to allocate 312.00 MiB (GPU 6; 23.70 GiB total capacity; 17.21 GiB already allocated; 117.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3301, Reason: CUDA out of memory. Tried to allocate 408.00 MiB (GPU 6; 23.70 GiB total capacity; 20.84 GiB already allocated; 111.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3402, Reason: CUDA out of memory. Tried to allocate 1022.00 MiB (GPU 6; 23.70 GiB total capacity; 19.02 GiB already allocated; 187.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3404, Reason: CUDA out of memory. Tried to allocate 352.00 MiB (GPU 6; 23.70 GiB total capacity; 17.09 GiB already allocated; 157.69 MiB free; 21.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3405, Reason: CUDA out of memory. Tried to allocate 212.00 MiB (GPU 6; 23.70 GiB total capacity; 18.03 GiB already allocated; 203.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3563, Reason: CUDA out of memory. Tried to allocate 740.00 MiB (GPU 6; 23.70 GiB total capacity; 19.49 GiB already allocated; 13.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 3565, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 20.45 GiB already allocated; 33.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4425, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 18.88 GiB already allocated; 43.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4472, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 19.98 GiB already allocated; 37.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4785, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 19.60 GiB already allocated; 51.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4786, Reason: CUDA out of memory. Tried to allocate 470.00 MiB (GPU 6; 23.70 GiB total capacity; 18.07 GiB already allocated; 465.69 MiB free; 21.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5028, Reason: CUDA out of memory. Tried to allocate 168.00 MiB (GPU 6; 23.70 GiB total capacity; 19.65 GiB already allocated; 141.69 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5210, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 20.18 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5211, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 20.18 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5212, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 20.18 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5213, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 20.18 GiB already allocated; 39.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
