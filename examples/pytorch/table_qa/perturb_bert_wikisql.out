[ Using CUDA ]
{'tokenizer': <function tokenize_jobs at 0x7efffe688310>, 'pretrained_word_emb_name': '6B', 'pretrained_word_emb_url': None, 'pretrained_word_emb_cache_dir': None, 'merge_strategy': 'tailhead', 'edge_strategy': 'heterogeneous', 'seed': None, 'word_emb_size': 300, 'thread_number': 1, 'port': 9000}
Using Table2Text read_raw_data
Loaded WikitableQA, train
Found:  14149  items
Loaded SQL
Found:  11276  items
Loaded WikitableQA, test
Found:  4344  items
Loaded WikitableQA, dev1
Found:  2831  items
Loaded WikitableQA, dev2
Found:  2838  items
Loaded WikitableQA, dev3
Found:  2838  items
Main File: ../../../graph4nlp/pytorch/WikiTableQA/WikiTableQuestions/seq2seq_data/train_perturb.csv
Loaded WikitableQA, perturb_test_orig
Found:  2720  items
Main File: ../../../graph4nlp/pytorch/WikiTableQA/WikiTableQuestions/seq2seq_data/train_perturb.csv
Loaded WikitableQA, perturb_test_changed
Found:  2720  items
Main File: ../../../graph4nlp/pytorch/WikiTableQA/WikiTableQuestions/seq2seq_data/dev_perturb.csv
Loaded WikitableQA, perturb_dev_orig
Found:  585  items
Main File: ../../../graph4nlp/pytorch/WikiTableQA/WikiTableQuestions/seq2seq_data/dev_perturb.csv
Loaded WikitableQA, perturb_dev_changed
Found:  585  items
table _process works!
Port 9000, processing: 0 / 14149
Port 9000, processing: 1000 / 14149
Port 9000, processing: 2000 / 14149
Port 9000, processing: 3000 / 14149
Port 9000, processing: 4000 / 14149
Port 9000, processing: 5000 / 14149
Port 9000, processing: 6000 / 14149
Port 9000, processing: 7000 / 14149
Port 9000, processing: 8000 / 14149
Port 9000, processing: 9000 / 14149
Port 9000, processing: 10000 / 14149
Port 9000, processing: 11000 / 14149
Port 9000, processing: 12000 / 14149
Port 9000, processing: 13000 / 14149
Port 9000, processing: 14000 / 14149
Port 9000, processing: 0 / 11276
Port 9000, processing: 1000 / 11276
Port 9000, processing: 2000 / 11276
Port 9000, processing: 3000 / 11276
Port 9000, processing: 4000 / 11276
Port 9000, processing: 5000 / 11276
Port 9000, processing: 6000 / 11276
Port 9000, processing: 7000 / 11276
Port 9000, processing: 8000 / 11276
Port 9000, processing: 9000 / 11276
Port 9000, processing: 10000 / 11276
Port 9000, processing: 11000 / 11276
Port 9000, processing: 0 / 4344
Port 9000, processing: 1000 / 4344
Port 9000, processing: 2000 / 4344
Port 9000, processing: 3000 / 4344
Port 9000, processing: 4000 / 4344
Port 9000, processing: 0 / 2831
Port 9000, processing: 1000 / 2831
Port 9000, processing: 2000 / 2831
Port 9000, processing: 0 / 2838
Port 9000, processing: 1000 / 2838
Port 9000, processing: 2000 / 2838
Port 9000, processing: 0 / 2838
Port 9000, processing: 1000 / 2838
Port 9000, processing: 2000 / 2838
Port 9000, processing: 0 / 2720
Port 9000, processing: 1000 / 2720
Port 9000, processing: 2000 / 2720
Port 9000, processing: 0 / 2720
Port 9000, processing: 1000 / 2720
Port 9000, processing: 2000 / 2720
Port 9000, processing: 0 / 585
Port 9000, processing: 0 / 585
2022-12-10 02:02:42,859 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `perturb_test_orig` split: 0.038
2022-12-10 05:38:11,115 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `perturb_test_changed` split: 0.052
2022-12-10 06:20:38,921 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `perturb_dev_orig` split: 0.048
2022-12-10 07:06:28,199 - examples/pytorch/semantic_parsing/graph2seq/log/ggnn.txt - INFO - Evaluation accuracy in `perturb_dev_changed` split: 0.045
Loading pre-built vocab model stored in bothWTQ/processed/TableGraph/vocab.pt
loading model
Loaded pretrained model!
Skipping evaluation batch 1, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 11, Reason: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 104.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 16, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 17, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 19, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 21, Reason: CUDA out of memory. Tried to allocate 234.00 MiB (GPU 6; 23.70 GiB total capacity; 2.25 GiB already allocated; 154.69 MiB free; 3.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 27, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 32, Reason: CUDA out of memory. Tried to allocate 264.00 MiB (GPU 6; 23.70 GiB total capacity; 2.19 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 40, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 44, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 45, Reason: CUDA out of memory. Tried to allocate 154.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 47, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 48, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 55, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 60, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 61, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 62, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 67, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 73, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 76, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 78, Reason: CUDA out of memory. Tried to allocate 298.00 MiB (GPU 6; 23.70 GiB total capacity; 1.92 GiB already allocated; 114.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 80, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 82, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 83, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 88, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 89, Reason: CUDA out of memory. Tried to allocate 230.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 114.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 90, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 92, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 94, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 95, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 96, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 100, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 108, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 110, Reason: CUDA out of memory. Tried to allocate 338.00 MiB (GPU 6; 23.70 GiB total capacity; 2.25 GiB already allocated; 240.69 MiB free; 3.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 111, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 114, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 115, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 117, Reason: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 119, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 122, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 123, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 125, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 126, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 127, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 128, Reason: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 131, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 88.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 133, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 134, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 137, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 140, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 143, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 146, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 147, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 148, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 150, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 153, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 164, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 165, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 166, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 167, Reason: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 170, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 171, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 173, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 182, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 183, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 186, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 187, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 188, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 189, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 191, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 196, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 197, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 199, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 200, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 202, Reason: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 114.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 206, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 208, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 211, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 214, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 218, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 219, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 221, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 223, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 229, Reason: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 230, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 233, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 235, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 237, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 238, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 240, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 241, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 246, Reason: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 6; 23.70 GiB total capacity; 1.94 GiB already allocated; 88.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 254, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 257, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 264, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 267, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 269, Reason: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 6; 23.70 GiB total capacity; 2.11 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 271, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 273, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 274, Reason: CUDA out of memory. Tried to allocate 280.00 MiB (GPU 6; 23.70 GiB total capacity; 2.13 GiB already allocated; 116.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 280, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 283, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 285, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 290, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 293, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 304, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 309, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 310, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 311, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 312, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 313, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 94.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 314, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 318, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 323, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 326, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 327, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 331, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 333, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 334, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 336, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 337, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 338, Reason: CUDA out of memory. Tried to allocate 134.00 MiB (GPU 6; 23.70 GiB total capacity; 2.28 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 339, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.22 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 344, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 347, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 348, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 352, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 354, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 355, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 356, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 357, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 362, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 366, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 368, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 369, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 162.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 374, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 377, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 378, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 380, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 381, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 385, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 386, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 391, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 392, Reason: CUDA out of memory. Tried to allocate 214.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 162.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 403, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 96.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 405, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 407, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 409, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 410, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 420, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 421, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 422, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 423, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 426, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 427, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 432, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 437, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 438, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 442, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 450, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 453, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 456, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 458, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 461, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 462, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 465, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 467, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 469, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 474, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 475, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 478, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 482, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.21 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 487, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.20 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 489, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 491, Reason: CUDA out of memory. Tried to allocate 436.00 MiB (GPU 6; 23.70 GiB total capacity; 2.15 GiB already allocated; 280.69 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 495, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 500, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 503, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 504, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 507, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 508, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 509, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 514, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 517, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 524, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 526, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 534, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 535, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 536, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 539, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 542, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 70.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 546, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 547, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 552, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 555, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 558, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 560, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 84.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 563, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 565, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 567, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 570, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 571, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 573, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 576, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.15 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 586, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 588, Reason: CUDA out of memory. Tried to allocate 142.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 100.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 590, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 592, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 596, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 598, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 606, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 611, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 613, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 614, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 615, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 619, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 620, Reason: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 6; 23.70 GiB total capacity; 2.24 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 622, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 624, Reason: CUDA out of memory. Tried to allocate 278.00 MiB (GPU 6; 23.70 GiB total capacity; 2.31 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 625, Reason: CUDA out of memory. Tried to allocate 290.00 MiB (GPU 6; 23.70 GiB total capacity; 1.89 GiB already allocated; 276.69 MiB free; 3.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 626, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 627, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 630, Reason: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 6; 23.70 GiB total capacity; 2.16 GiB already allocated; 170.69 MiB free; 3.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 637, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 94.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 648, Reason: CUDA out of memory. Tried to allocate 186.00 MiB (GPU 6; 23.70 GiB total capacity; 2.31 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 650, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 653, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 655, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 656, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 661, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 664, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 665, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 666, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 667, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 668, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 669, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 673, Reason: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 674, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 679, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 683, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 686, Reason: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 6; 23.70 GiB total capacity; 2.28 GiB already allocated; 74.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 688, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 707, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 714, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 715, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 716, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 717, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 718, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 719, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 721, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 724, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 730, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 731, Reason: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 734, Reason: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 112.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 743, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 746, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 747, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 748, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 750, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 753, Reason: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 757, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 762, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 766, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 767, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 768, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 770, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 772, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 773, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 779, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 782, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 787, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 788, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 792, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 793, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 796, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 802, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 804, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 810, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 816, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 817, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 818, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 819, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 820, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 821, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 826, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 827, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 829, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 832, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 835, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 839, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 841, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 842, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 843, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 844, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 845, Reason: CUDA out of memory. Tried to allocate 194.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 158.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 846, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 847, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 848, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 849, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 850, Reason: CUDA out of memory. Tried to allocate 180.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 851, Reason: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 852, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 855, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 858, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 859, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 860, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 863, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 865, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 867, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 872, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 873, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 876, Reason: CUDA out of memory. Tried to allocate 140.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 92.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 882, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 883, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 884, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 885, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 886, Reason: CUDA out of memory. Tried to allocate 362.00 MiB (GPU 6; 23.70 GiB total capacity; 2.04 GiB already allocated; 326.69 MiB free; 2.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 887, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 889, Reason: CUDA out of memory. Tried to allocate 1.23 GiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 132.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 890, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 891, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 892, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 894, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 897, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 898, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 902, Reason: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 6; 23.70 GiB total capacity; 2.31 GiB already allocated; 260.69 MiB free; 3.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 907, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 908, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 909, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 910, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 911, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 912, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 913, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 914, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 917, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 919, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 921, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 923, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 924, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 931, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 932, Reason: CUDA out of memory. Tried to allocate 434.00 MiB (GPU 6; 23.70 GiB total capacity; 2.35 GiB already allocated; 206.69 MiB free; 3.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 933, Reason: CUDA out of memory. Tried to allocate 118.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 935, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 936, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 230.69 MiB free; 3.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 940, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 947, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 949, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 950, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 952, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 953, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.33 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 954, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 955, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 956, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 957, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 958, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 961, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 962, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 963, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 967, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 968, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 970, Reason: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 84.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 976, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 978, Reason: CUDA out of memory. Tried to allocate 236.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 979, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 980, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 981, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 986, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 991, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 994, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 995, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 996, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 997, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1000, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1001, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1006, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1010, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 96.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1015, Reason: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1021, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1022, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1023, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1026, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1027, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1031, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1033, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1034, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1035, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1038, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1039, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1040, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1041, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1044, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1049, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1054, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1055, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1057, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1061, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1064, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1065, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1070, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1072, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1074, Reason: CUDA out of memory. Tried to allocate 276.00 MiB (GPU 6; 23.70 GiB total capacity; 2.22 GiB already allocated; 92.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1076, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1078, Reason: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1080, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1084, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1086, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1088, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1092, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1094, Reason: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1099, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1100, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1105, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1106, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1111, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1112, Reason: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 120.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1117, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1118, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1119, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1120, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1121, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1122, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1123, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1126, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1127, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1128, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1129, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1133, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1135, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1136, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1140, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 98.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1143, Reason: CUDA out of memory. Tried to allocate 194.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 116.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1146, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1154, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1159, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1164, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1166, Reason: CUDA out of memory. Tried to allocate 214.00 MiB (GPU 6; 23.70 GiB total capacity; 2.04 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1173, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1175, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.33 GiB already allocated; 94.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1177, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1181, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1183, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1189, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1191, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1192, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1195, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1196, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1201, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1205, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1206, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1207, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1208, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1216, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1220, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1222, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1232, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1233, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1237, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1241, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1244, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1248, Reason: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1253, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1258, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1262, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1263, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1265, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1266, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1267, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1268, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1269, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1270, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1272, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1277, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1282, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1283, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1284, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1288, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1289, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1290, Reason: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1291, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1292, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1295, Reason: CUDA out of memory. Tried to allocate 728.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 488.69 MiB free; 2.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1306, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1307, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1312, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1313, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1316, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1317, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1319, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1321, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1322, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1328, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1329, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1330, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1332, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.05 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1333, Reason: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 130.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1334, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.05 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1340, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 136.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1342, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1343, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1344, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1349, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1351, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1352, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1353, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.15 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1354, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1357, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1358, Reason: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 160.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1359, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1360, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1369, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1371, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1373, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1378, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1379, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1381, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1385, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1393, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1394, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1399, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1402, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1404, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1405, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1409, Reason: CUDA out of memory. Tried to allocate 380.00 MiB (GPU 6; 23.70 GiB total capacity; 2.03 GiB already allocated; 262.69 MiB free; 3.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1410, Reason: CUDA out of memory. Tried to allocate 380.00 MiB (GPU 6; 23.70 GiB total capacity; 2.03 GiB already allocated; 262.69 MiB free; 3.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1411, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1413, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1416, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1418, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1419, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1421, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1423, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1426, Reason: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 6; 23.70 GiB total capacity; 2.12 GiB already allocated; 166.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1435, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1437, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1446, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 2.21 GiB already allocated; 256.69 MiB free; 3.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1449, Reason: CUDA out of memory. Tried to allocate 264.00 MiB (GPU 6; 23.70 GiB total capacity; 2.14 GiB already allocated; 204.69 MiB free; 3.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1453, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1454, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1456, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1457, Reason: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 88.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1458, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1460, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1464, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1465, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1467, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1470, Reason: CUDA out of memory. Tried to allocate 184.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1472, Reason: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1473, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1477, Reason: CUDA out of memory. Tried to allocate 1.23 GiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1478, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1479, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1480, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1481, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1482, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1486, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1488, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1490, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1491, Reason: CUDA out of memory. Tried to allocate 142.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1492, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1494, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1496, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1501, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1505, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1506, Reason: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 6; 23.70 GiB total capacity; 2.14 GiB already allocated; 108.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1507, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1510, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1511, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1518, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1521, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1524, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1527, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1528, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1529, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1530, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1531, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1534, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1537, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.26 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1540, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1541, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1543, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1544, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1547, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1549, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1550, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1552, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1554, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1555, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1558, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1560, Reason: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 106.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1567, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1568, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1571, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1573, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.26 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1580, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1581, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1583, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1584, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1589, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1591, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1595, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1597, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1600, Reason: CUDA out of memory. Tried to allocate 1.23 GiB (GPU 6; 23.70 GiB total capacity; 2.33 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1601, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1609, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1610, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1612, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1616, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1617, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1618, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1623, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1624, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1625, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1627, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1628, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1630, Reason: CUDA out of memory. Tried to allocate 290.00 MiB (GPU 6; 23.70 GiB total capacity; 2.13 GiB already allocated; 278.69 MiB free; 3.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1631, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1634, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1636, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1637, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1638, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1641, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1646, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1648, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1650, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1652, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1655, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1656, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1661, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1664, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1665, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1669, Reason: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 78.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1674, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1675, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1679, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1681, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1683, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1688, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1695, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1696, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1701, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1702, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1703, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1704, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1705, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 3.04 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1706, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 78.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1707, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1709, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1710, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1713, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1714, Reason: CUDA out of memory. Tried to allocate 196.00 MiB (GPU 6; 23.70 GiB total capacity; 1.93 GiB already allocated; 188.69 MiB free; 3.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1719, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1722, Reason: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 118.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1724, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1725, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1727, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1735, Reason: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 6; 23.70 GiB total capacity; 2.24 GiB already allocated; 78.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1739, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1740, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1741, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1745, Reason: CUDA out of memory. Tried to allocate 118.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 82.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1747, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1748, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1749, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1750, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1753, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1754, Reason: CUDA out of memory. Tried to allocate 118.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1758, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1759, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1760, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1762, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1764, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1766, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1767, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1771, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1776, Reason: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 130.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1779, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1781, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1784, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1786, Reason: CUDA out of memory. Tried to allocate 186.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 118.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1787, Reason: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1789, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1793, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1794, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1796, Reason: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1798, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1800, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1802, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1803, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1805, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1806, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.33 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1808, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1809, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1810, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1814, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1815, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1818, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1819, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.15 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1822, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1824, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.24 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1837, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1845, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1848, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1849, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1850, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1854, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1856, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1857, Reason: CUDA out of memory. Tried to allocate 290.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 162.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1858, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1870, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1872, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1875, Reason: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1879, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1883, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1885, Reason: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 6; 23.70 GiB total capacity; 1.91 GiB already allocated; 136.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1886, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1890, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1892, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1893, Reason: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 100.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1894, Reason: CUDA out of memory. Tried to allocate 140.00 MiB (GPU 6; 23.70 GiB total capacity; 2.33 GiB already allocated; 100.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1896, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1898, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1900, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1901, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1902, Reason: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1903, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1904, Reason: CUDA out of memory. Tried to allocate 280.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1906, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1907, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1909, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1912, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1913, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1921, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1925, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1927, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1928, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1929, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1931, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1932, Reason: CUDA out of memory. Tried to allocate 236.00 MiB (GPU 6; 23.70 GiB total capacity; 2.29 GiB already allocated; 172.69 MiB free; 3.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1938, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1944, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1945, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1948, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1950, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1951, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1953, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1958, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1959, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1961, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1963, Reason: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1968, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1969, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1972, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1973, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1975, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1976, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1977, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1978, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1982, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1983, Reason: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 6; 23.70 GiB total capacity; 2.17 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1984, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.14 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1986, Reason: CUDA out of memory. Tried to allocate 230.00 MiB (GPU 6; 23.70 GiB total capacity; 2.16 GiB already allocated; 192.69 MiB free; 3.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1987, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1988, Reason: CUDA out of memory. Tried to allocate 234.00 MiB (GPU 6; 23.70 GiB total capacity; 2.10 GiB already allocated; 130.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1992, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1993, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1997, Reason: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 6; 23.70 GiB total capacity; 1.87 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1998, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2000, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2002, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2004, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2006, Reason: CUDA out of memory. Tried to allocate 552.00 MiB (GPU 6; 23.70 GiB total capacity; 2.31 GiB already allocated; 164.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2011, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2016, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2017, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2018, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2019, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2021, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2027, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2028, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2029, Reason: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2031, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2033, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2037, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2041, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2042, Reason: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 6; 23.70 GiB total capacity; 2.28 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2044, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2045, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2049, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2050, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2051, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2053, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2055, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2057, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2059, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2063, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2065, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2066, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2069, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2072, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2073, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2074, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2076, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2079, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2080, Reason: CUDA out of memory. Tried to allocate 176.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 142.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2081, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2082, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2090, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2092, Reason: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 6; 23.70 GiB total capacity; 2.35 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2094, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2100, Reason: CUDA out of memory. Tried to allocate 728.00 MiB (GPU 6; 23.70 GiB total capacity; 2.23 GiB already allocated; 202.69 MiB free; 3.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2107, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2108, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2109, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2111, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2119, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2122, Reason: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2123, Reason: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 100.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2124, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2125, Reason: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2126, Reason: CUDA out of memory. Tried to allocate 214.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 100.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2127, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 100.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2132, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2134, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2136, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2142, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2143, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2144, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2148, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2154, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2157, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2159, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2161, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2162, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2163, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2164, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2165, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2167, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2168, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2171, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2173, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2174, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2176, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2177, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2179, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2181, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2184, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2185, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2188, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2193, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2195, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.21 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2196, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2198, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2202, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2206, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2207, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2209, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2210, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2211, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2213, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2214, Reason: CUDA out of memory. Tried to allocate 556.00 MiB (GPU 6; 23.70 GiB total capacity; 2.10 GiB already allocated; 366.69 MiB free; 2.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2219, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2220, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2222, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2223, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2224, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2226, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2227, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2230, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2236, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2237, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2238, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2241, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2246, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2247, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2253, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2254, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2263, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2264, Reason: CUDA out of memory. Tried to allocate 178.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 88.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2265, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2266, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2273, Reason: CUDA out of memory. Tried to allocate 228.00 MiB (GPU 6; 23.70 GiB total capacity; 2.21 GiB already allocated; 224.69 MiB free; 3.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2275, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2277, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2282, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2283, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2285, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2287, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2288, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2290, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2291, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2297, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2298, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2299, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2302, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2306, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2309, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2310, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2312, Reason: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 88.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2313, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2315, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2319, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2323, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2325, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2326, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2327, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2330, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2332, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2340, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2342, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2343, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2345, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2347, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2348, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2355, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2356, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2357, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2359, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2363, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2364, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2366, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2370, Reason: CUDA out of memory. Tried to allocate 202.00 MiB (GPU 6; 23.70 GiB total capacity; 2.23 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2371, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2372, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2374, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 104.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2375, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2379, Reason: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 6; 23.70 GiB total capacity; 2.04 GiB already allocated; 136.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2380, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 2.17 GiB already allocated; 94.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2383, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2388, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2390, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2394, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2402, Reason: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2404, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2407, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2408, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2410, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.19 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2411, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2415, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2416, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2421, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2422, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2424, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2425, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2426, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2427, Reason: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 90.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2431, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2433, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2434, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2435, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2436, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2438, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2443, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2444, Reason: CUDA out of memory. Tried to allocate 118.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2445, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2446, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2448, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2450, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2455, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2456, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2460, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2464, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2466, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2467, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2470, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2472, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2477, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2482, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2487, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2488, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2493, Reason: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 136.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2495, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2498, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2509, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2511, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2515, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2516, Reason: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 6; 23.70 GiB total capacity; 2.20 GiB already allocated; 108.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2519, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2520, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2521, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2522, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2523, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2526, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2531, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2532, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.14 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2535, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2537, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2538, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2540, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2541, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2542, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2548, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2549, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.26 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2552, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 70.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2554, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2555, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2557, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2558, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2559, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2560, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2564, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2567, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2570, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2571, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2572, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2573, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2574, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2576, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2578, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2579, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2581, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2583, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 98.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2584, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 98.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2585, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2586, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2587, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2592, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2593, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2594, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2595, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2596, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2598, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2600, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2601, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2603, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2604, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2607, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2608, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2611, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2614, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2615, Reason: CUDA out of memory. Tried to allocate 688.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 422.69 MiB free; 2.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2618, Reason: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 78.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2621, Reason: CUDA out of memory. Tried to allocate 390.00 MiB (GPU 6; 23.70 GiB total capacity; 1.86 GiB already allocated; 356.69 MiB free; 2.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2625, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 3.04 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2629, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2630, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2632, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2635, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2640, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2641, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2643, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2645, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2652, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2654, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2656, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2659, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2661, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2663, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2664, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2667, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2668, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2672, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2675, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2679, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2681, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2682, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2683, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2687, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2688, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2689, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2691, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2697, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2700, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2702, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2703, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2704, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.20 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2705, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2711, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2713, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2714, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2715, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2716, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2717, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2718, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2719, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
Skipping evaluation batch 1, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.08 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 6, Reason: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 6; 23.70 GiB total capacity; 2.31 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 11, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 16, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 17, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 19, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 21, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 1.96 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 27, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 30, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 32, Reason: CUDA out of memory. Tried to allocate 264.00 MiB (GPU 6; 23.70 GiB total capacity; 2.19 GiB already allocated; 96.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 40, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 44, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 45, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 47, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 48, Reason: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 6; 23.70 GiB total capacity; 2.05 GiB already allocated; 124.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 55, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 60, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 61, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 62, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.26 GiB already allocated; 92.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 67, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 73, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 76, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 78, Reason: CUDA out of memory. Tried to allocate 298.00 MiB (GPU 6; 23.70 GiB total capacity; 1.92 GiB already allocated; 286.69 MiB free; 3.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 80, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 82, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 83, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 88, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 89, Reason: CUDA out of memory. Tried to allocate 230.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 158.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 90, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 92, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 94, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 95, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 96, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 100, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 108, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.23 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 110, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.33 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 111, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 114, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 70.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 115, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 117, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 1.99 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 119, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 122, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 123, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 125, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 126, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 127, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 128, Reason: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 6; 23.70 GiB total capacity; 2.22 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 131, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 90.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 133, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 134, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 137, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 140, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 143, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 146, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 147, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 148, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 150, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 153, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 164, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 165, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 166, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.04 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 167, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 170, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 171, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 173, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 182, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 94.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 183, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 186, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 187, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 188, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 189, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 191, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 196, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 96.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 197, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 199, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 200, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 202, Reason: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 110.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 206, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 208, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 211, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 218, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 219, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 221, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 223, Reason: CUDA out of memory. Tried to allocate 380.00 MiB (GPU 6; 23.70 GiB total capacity; 2.33 GiB already allocated; 174.69 MiB free; 3.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 229, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 102.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 230, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 233, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 235, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 237, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 238, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 240, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 241, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 246, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 254, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 257, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 264, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 267, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 269, Reason: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 271, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 273, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 274, Reason: CUDA out of memory. Tried to allocate 280.00 MiB (GPU 6; 23.70 GiB total capacity; 1.85 GiB already allocated; 128.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 280, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 283, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 285, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 290, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 293, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 309, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 310, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 311, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 312, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 313, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 314, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 317, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 323, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 326, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 327, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 331, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 333, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 334, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 336, Reason: CUDA out of memory. Tried to allocate 200.00 MiB (GPU 6; 23.70 GiB total capacity; 1.95 GiB already allocated; 122.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 337, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 338, Reason: CUDA out of memory. Tried to allocate 134.00 MiB (GPU 6; 23.70 GiB total capacity; 2.16 GiB already allocated; 96.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 339, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 344, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 347, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 348, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 352, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 354, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 355, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 356, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 357, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 362, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.26 GiB already allocated; 96.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 366, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 368, Reason: CUDA out of memory. Tried to allocate 134.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 122.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 369, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 102.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 374, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 377, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 378, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 380, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 381, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 386, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 391, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 392, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 403, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 405, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 407, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 409, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 410, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 420, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 421, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 422, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 423, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 426, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 427, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 432, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 437, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 438, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 442, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 450, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 453, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 456, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 458, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 461, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 462, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 465, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 467, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 469, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 474, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 475, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 478, Reason: CUDA out of memory. Tried to allocate 140.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 482, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.22 GiB already allocated; 98.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 487, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 489, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 491, Reason: CUDA out of memory. Tried to allocate 436.00 MiB (GPU 6; 23.70 GiB total capacity; 2.14 GiB already allocated; 266.69 MiB free; 3.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 495, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 500, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 503, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 504, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 507, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 508, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 509, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 514, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 517, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 524, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 526, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 534, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 535, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 536, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 539, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 542, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 82.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 546, Reason: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 547, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 552, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 555, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 558, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 560, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 563, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 564, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 565, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 566, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 567, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 570, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 571, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 573, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 576, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 586, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 588, Reason: CUDA out of memory. Tried to allocate 142.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 590, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 592, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 596, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 598, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.17 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 606, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 611, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 613, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 614, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 615, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 619, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.33 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 620, Reason: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 6; 23.70 GiB total capacity; 2.23 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 622, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 624, Reason: CUDA out of memory. Tried to allocate 278.00 MiB (GPU 6; 23.70 GiB total capacity; 2.04 GiB already allocated; 158.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 625, Reason: CUDA out of memory. Tried to allocate 290.00 MiB (GPU 6; 23.70 GiB total capacity; 1.89 GiB already allocated; 122.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 626, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 627, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 630, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 637, Reason: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 648, Reason: CUDA out of memory. Tried to allocate 186.00 MiB (GPU 6; 23.70 GiB total capacity; 2.13 GiB already allocated; 186.69 MiB free; 3.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 650, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 653, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 655, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 656, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 661, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 664, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 665, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 667, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 1.87 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 668, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 673, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 674, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 679, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 683, Reason: CUDA out of memory. Tried to allocate 206.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 166.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 686, Reason: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 6; 23.70 GiB total capacity; 2.29 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 688, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 689, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 690, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 707, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 714, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 715, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 716, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 717, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 718, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 719, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 721, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 74.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 724, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 730, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 731, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 734, Reason: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 743, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 747, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 748, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 750, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 753, Reason: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 757, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 762, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 766, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 767, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 768, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 770, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 772, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 773, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 779, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 782, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 787, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 788, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 792, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 793, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 795, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 796, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 804, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 810, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 816, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 817, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 818, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 819, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 820, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 821, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 84.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 826, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 827, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 829, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 832, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 835, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 839, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 840, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 842, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 843, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 845, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 846, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 847, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.28 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 850, Reason: CUDA out of memory. Tried to allocate 180.00 MiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 851, Reason: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 6; 23.70 GiB total capacity; 2.15 GiB already allocated; 130.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 855, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 858, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 859, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 863, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 865, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 867, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 872, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 873, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 876, Reason: CUDA out of memory. Tried to allocate 140.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 90.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 882, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 883, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 884, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 885, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 886, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.03 GiB already allocated; 88.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 887, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 889, Reason: CUDA out of memory. Tried to allocate 1.23 GiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 326.69 MiB free; 2.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 890, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 90.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 891, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 894, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 897, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 898, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 902, Reason: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 907, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 908, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 909, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 911, Reason: CUDA out of memory. Tried to allocate 364.00 MiB (GPU 6; 23.70 GiB total capacity; 2.08 GiB already allocated; 132.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 913, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 914, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 917, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 919, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 922, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 923, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 924, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 925, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 926, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 931, Reason: CUDA out of memory. Tried to allocate 142.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 134.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 932, Reason: CUDA out of memory. Tried to allocate 434.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 116.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 933, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 935, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 936, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 940, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 947, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 949, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 950, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 952, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 953, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 954, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 955, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 956, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 957, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 958, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 961, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 962, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 963, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 967, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 968, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 970, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 976, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 978, Reason: CUDA out of memory. Tried to allocate 236.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 164.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 979, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 980, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 981, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 986, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 991, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 994, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 995, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 996, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 997, Reason: CUDA out of memory. Tried to allocate 140.00 MiB (GPU 6; 23.70 GiB total capacity; 2.21 GiB already allocated; 138.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1000, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1001, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1006, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1010, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1015, Reason: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1021, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1022, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1023, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1026, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1027, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1031, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1033, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1034, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1035, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1038, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1039, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1040, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1041, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1044, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1049, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1054, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1055, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1057, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1061, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1064, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1065, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1070, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1074, Reason: CUDA out of memory. Tried to allocate 276.00 MiB (GPU 6; 23.70 GiB total capacity; 2.21 GiB already allocated; 114.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1076, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1078, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1080, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1084, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1088, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1092, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1094, Reason: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1099, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1100, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1105, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.22 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1106, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1111, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1112, Reason: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 110.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1117, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1118, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1119, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1122, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1123, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1126, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1127, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1128, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1129, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1133, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1135, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1136, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1140, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1143, Reason: CUDA out of memory. Tried to allocate 194.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 182.69 MiB free; 3.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1146, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1154, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1159, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1164, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1166, Reason: CUDA out of memory. Tried to allocate 214.00 MiB (GPU 6; 23.70 GiB total capacity; 2.04 GiB already allocated; 98.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1173, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1175, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1177, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1181, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1183, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1189, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1191, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1192, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1194, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1195, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1201, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1205, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1206, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1207, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1208, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1216, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1220, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1222, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1232, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1233, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1237, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1241, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1242, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1244, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1248, Reason: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 94.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1253, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1258, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1262, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1263, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1264, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1266, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1268, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1269, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1270, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1272, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1277, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1282, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1283, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1284, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1288, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1289, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1290, Reason: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 126.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1291, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1292, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1295, Reason: CUDA out of memory. Tried to allocate 728.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 446.69 MiB free; 2.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1306, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1307, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1312, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1313, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1316, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1317, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1319, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1321, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1322, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1328, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1329, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1330, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1332, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.05 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1333, Reason: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 84.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1334, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1335, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1336, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1337, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 3.06 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1338, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1340, Reason: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 96.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1342, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1343, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1349, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1352, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1354, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1357, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1358, Reason: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 128.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1359, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1360, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1361, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 1.84 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1363, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.11 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1365, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1367, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1369, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1371, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1373, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1378, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1379, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1381, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1385, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1393, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1394, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1399, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1402, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1404, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1405, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1409, Reason: CUDA out of memory. Tried to allocate 380.00 MiB (GPU 6; 23.70 GiB total capacity; 2.03 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1410, Reason: CUDA out of memory. Tried to allocate 380.00 MiB (GPU 6; 23.70 GiB total capacity; 2.03 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1411, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1413, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1416, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1418, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1419, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1421, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1423, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1426, Reason: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 138.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1435, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1437, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1446, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 2.21 GiB already allocated; 248.69 MiB free; 3.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1449, Reason: CUDA out of memory. Tried to allocate 264.00 MiB (GPU 6; 23.70 GiB total capacity; 2.14 GiB already allocated; 192.69 MiB free; 3.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1453, Reason: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 132.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1454, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1456, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1457, Reason: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1458, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1460, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1464, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1465, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1467, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1470, Reason: CUDA out of memory. Tried to allocate 184.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1472, Reason: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1473, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1477, Reason: CUDA out of memory. Tried to allocate 1.23 GiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1478, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1479, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1480, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1481, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1482, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1486, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1488, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1489, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1490, Reason: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1491, Reason: CUDA out of memory. Tried to allocate 142.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1492, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1494, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1496, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1501, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1506, Reason: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 6; 23.70 GiB total capacity; 2.31 GiB already allocated; 112.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1507, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1510, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1511, Reason: CUDA out of memory. Tried to allocate 316.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 150.69 MiB free; 3.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1518, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1521, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1524, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1527, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1528, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1529, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1530, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1531, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1537, Reason: CUDA out of memory. Tried to allocate 140.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1540, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1541, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1543, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1544, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1547, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1549, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1550, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1552, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1554, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1555, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1558, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1560, Reason: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1567, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1568, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1571, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1573, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.26 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1580, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1583, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1584, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1589, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1591, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1595, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1597, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1600, Reason: CUDA out of memory. Tried to allocate 1.23 GiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 138.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1601, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.31 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1609, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1612, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1616, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1617, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1618, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1623, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1624, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1625, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1627, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1628, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1630, Reason: CUDA out of memory. Tried to allocate 290.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 268.69 MiB free; 3.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1631, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1634, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1636, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1637, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1638, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1641, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1646, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1648, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1650, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1652, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1655, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1656, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1661, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1664, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1665, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1669, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1674, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1675, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1679, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1681, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1683, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1688, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1695, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1696, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1701, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1702, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1703, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1704, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1705, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 3.04 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1706, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1707, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1709, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1710, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1713, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1714, Reason: CUDA out of memory. Tried to allocate 196.00 MiB (GPU 6; 23.70 GiB total capacity; 1.92 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1719, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1722, Reason: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 144.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1724, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1725, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1727, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1735, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1739, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1740, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1741, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1745, Reason: CUDA out of memory. Tried to allocate 118.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 100.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1747, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1748, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1749, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1750, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1753, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1754, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1758, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1759, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 3.03 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1760, Reason: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 128.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1762, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1764, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1766, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1767, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1771, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1776, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1779, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1781, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1784, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1786, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1787, Reason: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1789, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1793, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1794, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1796, Reason: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 126.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1798, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1800, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1802, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1803, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1805, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1806, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1808, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1809, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1810, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1814, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1815, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1818, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1819, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.22 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1822, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.26 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1824, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.24 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1837, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1845, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1847, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1848, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1849, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1850, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1854, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1856, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1857, Reason: CUDA out of memory. Tried to allocate 290.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 184.69 MiB free; 3.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1858, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1870, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1872, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1875, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1879, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1883, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1885, Reason: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 6; 23.70 GiB total capacity; 2.23 GiB already allocated; 134.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1886, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1890, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1892, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1893, Reason: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 110.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1894, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1896, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1898, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1900, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1901, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1902, Reason: CUDA out of memory. Tried to allocate 188.00 MiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1903, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1904, Reason: CUDA out of memory. Tried to allocate 280.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1906, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1907, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1909, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1912, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1913, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1921, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1925, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1927, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1928, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1929, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1930, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1931, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1932, Reason: CUDA out of memory. Tried to allocate 236.00 MiB (GPU 6; 23.70 GiB total capacity; 2.21 GiB already allocated; 184.69 MiB free; 3.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1938, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1944, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1945, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1948, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1951, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1953, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1958, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1959, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1961, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1963, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1965, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1968, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1969, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1972, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1973, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1975, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1976, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1978, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1982, Reason: CUDA out of memory. Tried to allocate 276.00 MiB (GPU 6; 23.70 GiB total capacity; 2.07 GiB already allocated; 130.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1983, Reason: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 6; 23.70 GiB total capacity; 2.17 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1984, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.04 GiB already allocated; 84.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1986, Reason: CUDA out of memory. Tried to allocate 230.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 114.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1987, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1988, Reason: CUDA out of memory. Tried to allocate 234.00 MiB (GPU 6; 23.70 GiB total capacity; 2.10 GiB already allocated; 126.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1992, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1993, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1997, Reason: CUDA out of memory. Tried to allocate 302.00 MiB (GPU 6; 23.70 GiB total capacity; 2.15 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 1998, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2000, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 92.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2002, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2004, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 3.04 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2006, Reason: CUDA out of memory. Tried to allocate 138.00 MiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2011, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2016, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2017, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2018, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2019, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2021, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2027, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2028, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2029, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2031, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2033, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2037, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2041, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2042, Reason: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 6; 23.70 GiB total capacity; 2.28 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2044, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2045, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2049, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2050, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.29 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2051, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2053, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2055, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2057, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2059, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2063, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2065, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2066, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2069, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.16 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2072, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2073, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2074, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2076, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2079, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2080, Reason: CUDA out of memory. Tried to allocate 176.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 158.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2081, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2082, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2090, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2092, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2094, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2100, Reason: CUDA out of memory. Tried to allocate 728.00 MiB (GPU 6; 23.70 GiB total capacity; 2.23 GiB already allocated; 576.69 MiB free; 2.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2107, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2108, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2109, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2111, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2119, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2122, Reason: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2123, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2124, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2125, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2126, Reason: CUDA out of memory. Tried to allocate 214.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 98.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2127, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 98.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2132, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2134, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2136, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.04 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2142, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2143, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2144, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2148, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2154, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2157, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2159, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2161, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2162, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2163, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2164, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2165, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2167, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2168, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2171, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2173, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2174, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2176, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2177, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2179, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2181, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2184, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2185, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2188, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2193, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2195, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2196, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2197, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2202, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2206, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2207, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2209, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2210, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2211, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2213, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.31 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2214, Reason: CUDA out of memory. Tried to allocate 556.00 MiB (GPU 6; 23.70 GiB total capacity; 2.10 GiB already allocated; 324.69 MiB free; 2.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2219, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2220, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2222, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2223, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2224, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2226, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2227, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2230, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2237, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2238, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2241, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2246, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2247, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2253, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2254, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2263, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2264, Reason: CUDA out of memory. Tried to allocate 178.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 118.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2265, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2266, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2273, Reason: CUDA out of memory. Tried to allocate 228.00 MiB (GPU 6; 23.70 GiB total capacity; 2.21 GiB already allocated; 208.69 MiB free; 3.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2275, Reason: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2277, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2282, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2283, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2285, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2287, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2288, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2290, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2291, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2297, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2298, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2299, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2302, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2306, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2309, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2310, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.25 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2312, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2313, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2315, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2319, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2320, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2323, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2325, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2327, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2330, Reason: CUDA out of memory. Tried to allocate 228.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2332, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2340, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2342, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2343, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2345, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2347, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2348, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2355, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2356, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2357, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2359, Reason: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 6; 23.70 GiB total capacity; 2.13 GiB already allocated; 186.69 MiB free; 3.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2363, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 70.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2364, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2366, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2370, Reason: CUDA out of memory. Tried to allocate 202.00 MiB (GPU 6; 23.70 GiB total capacity; 2.03 GiB already allocated; 198.69 MiB free; 3.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2371, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2372, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2374, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2375, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2379, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2380, Reason: CUDA out of memory. Tried to allocate 106.00 MiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2383, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2388, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2390, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2394, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2402, Reason: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 156.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2404, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2407, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2408, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2410, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.27 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2411, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2415, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2416, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2421, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2422, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2424, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2425, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2426, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2427, Reason: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2431, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2433, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2434, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2435, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2436, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2438, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2443, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2444, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2445, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2446, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2448, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2455, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2456, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2460, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2464, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2466, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2467, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2470, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2472, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2477, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2482, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2487, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2488, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2493, Reason: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 90.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2495, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2498, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2509, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2511, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2515, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2516, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2519, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2520, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2521, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2522, Reason: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2523, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.36 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2526, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2531, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2532, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.18 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2535, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2537, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2538, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2540, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2541, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2542, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2548, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.25 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2549, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.26 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2552, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2554, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.49 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2555, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2557, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2558, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2559, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2560, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2564, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2567, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2570, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2571, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2572, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2573, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2574, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2576, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2578, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2579, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2581, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2583, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2584, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2585, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.28 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2586, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2587, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2592, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2593, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2594, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2595, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2596, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2598, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2600, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2601, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2604, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2607, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2608, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2611, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2614, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2615, Reason: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 110.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2618, Reason: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 146.69 MiB free; 3.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2621, Reason: CUDA out of memory. Tried to allocate 390.00 MiB (GPU 6; 23.70 GiB total capacity; 2.25 GiB already allocated; 98.69 MiB free; 3.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2625, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 3.04 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2629, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2630, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2632, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2635, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2639, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2640, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2641, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2643, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.22 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2645, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2650, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2652, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2654, Reason: CUDA out of memory. Tried to allocate 158.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2656, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2659, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2662, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2663, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2664, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2665, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 3.01 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2667, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2668, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2672, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2675, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2679, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2681, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2682, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2683, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2687, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2688, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2689, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2691, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2697, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2700, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2702, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2703, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.97 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2704, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.28 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2705, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2711, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2712, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2713, Reason: CUDA out of memory. Tried to allocate 254.00 MiB (GPU 6; 23.70 GiB total capacity; 2.16 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2714, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2715, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2716, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2717, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2718, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 2719, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
Skipping evaluation batch 2, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.11 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 18, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 20, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 21, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 22, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 23, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 25, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 74.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 27, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 28, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 29, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 34, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 60.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 36, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 40, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 41, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 42, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 43, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.28 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 47, Reason: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 118.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 54, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 56, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 60, Reason: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 61, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 65, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 66, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 68, Reason: CUDA out of memory. Tried to allocate 118.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 74.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 70, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 72, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 74, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 76, Reason: CUDA out of memory. Tried to allocate 158.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 76.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 83, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 84, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 87, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 91, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 92, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 93, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 96, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 99, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 100, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 104, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 106, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 111, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 113, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 119, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 121, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 126, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 129, Reason: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 131, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 132, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 134, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 135, Reason: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 136, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 139, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 62.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 140, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 141, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 143, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 144, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 146, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 150, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 151, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 153, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 156, Reason: CUDA out of memory. Tried to allocate 184.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 157, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 158, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 162, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 163, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 164, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 167, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 173, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 175, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 179, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 181, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 188, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 189, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 191, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 192, Reason: CUDA out of memory. Tried to allocate 398.00 MiB (GPU 6; 23.70 GiB total capacity; 2.29 GiB already allocated; 324.69 MiB free; 2.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 193, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 198, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 199, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 201, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 202, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 203, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 206, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 213, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 217, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 80.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 219, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 233, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 234, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 236, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 238, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 239, Reason: CUDA out of memory. Tried to allocate 728.00 MiB (GPU 6; 23.70 GiB total capacity; 2.27 GiB already allocated; 184.69 MiB free; 3.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 240, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 242, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 243, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 247, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 84.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 249, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 252, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 253, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 259, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 263, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 266, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 268, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 269, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 273, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 274, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 279, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 283, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 284, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 286, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 288, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 289, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 52.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 292, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 295, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 296, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 299, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 301, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 302, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 303, Reason: CUDA out of memory. Tried to allocate 118.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 78.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 307, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.19 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 309, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 312, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 315, Reason: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 138.69 MiB free; 3.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 317, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 78.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 318, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 319, Reason: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 322, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 323, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 326, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 334, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 340, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 341, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 344, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 345, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 352, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 356, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 357, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 362, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 367, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.30 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 370, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 373, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 74.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 374, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 379, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 380, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 381, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 382, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 383, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 385, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 389, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 391, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 392, Reason: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 112.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 393, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 398, Reason: CUDA out of memory. Tried to allocate 156.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 399, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 82.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 401, Reason: CUDA out of memory. Tried to allocate 220.00 MiB (GPU 6; 23.70 GiB total capacity; 1.82 GiB already allocated; 176.69 MiB free; 3.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 402, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 409, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.32 GiB already allocated; 112.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 410, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 414, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 415, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 420, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 421, Reason: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 6; 23.70 GiB total capacity; 2.21 GiB already allocated; 154.69 MiB free; 3.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 424, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 425, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 426, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 429, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 435, Reason: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 6; 23.70 GiB total capacity; 2.48 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 437, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 444, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 453, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 458, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 461, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 463, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 464, Reason: CUDA out of memory. Tried to allocate 70.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 465, Reason: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 467, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 469, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 470, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 471, Reason: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 6; 23.70 GiB total capacity; 2.08 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 472, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 476, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 477, Reason: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 483, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 485, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 487, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 491, Reason: CUDA out of memory. Tried to allocate 276.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 92.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 496, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 502, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 503, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 504, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 506, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 507, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 511, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 512, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.54 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 513, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 515, Reason: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 516, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 518, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 519, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 520, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 523, Reason: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 526, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 527, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 529, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 82.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 531, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 532, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.26 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 535, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 536, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 539, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 540, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 544, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 545, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 547, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 548, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 556, Reason: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 148.69 MiB free; 3.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 557, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 558, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 559, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 560, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 565, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 568, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 573, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 575, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 576, Reason: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 6; 23.70 GiB total capacity; 2.41 GiB already allocated; 74.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 578, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.58 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 583, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 584, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
Skipping evaluation batch 2, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 4, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 3.02 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 5, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 3.05 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 9, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 10, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.82 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 12, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 18, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 20, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 21, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 22, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 23, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 25, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 27, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 28, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 36.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 29, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 34, Reason: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 36, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 40, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 41, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 42, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 43, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 47, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 54, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 56, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 60, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.25 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 61, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 65, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 66, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 68, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.24 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 70, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 74, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 76, Reason: CUDA out of memory. Tried to allocate 158.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 83, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.96 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 84, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.51 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 87, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 72.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 91, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 92, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 93, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 96, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 99, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 100, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 104, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 56.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 106, Reason: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 111, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 113, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 119, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 121, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 126, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 129, Reason: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 124.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 131, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 132, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 134, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 135, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 136, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 139, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 140, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 143, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 144, Reason: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 146, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 64.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 150, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 151, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 153, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 156, Reason: CUDA out of memory. Tried to allocate 184.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 157, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.95 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 158, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 162, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 163, Reason: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 6; 23.70 GiB total capacity; 2.57 GiB already allocated; 114.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 164, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 167, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 173, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 175, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 179, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 182, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 188, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 3.00 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 189, Reason: CUDA out of memory. Tried to allocate 134.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 126.69 MiB free; 3.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 191, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 192, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.39 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 193, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 198, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 199, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 201, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 202, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 206, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 213, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.53 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 217, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 219, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 233, Reason: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 234, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 236, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 238, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 239, Reason: CUDA out of memory. Tried to allocate 728.00 MiB (GPU 6; 23.70 GiB total capacity; 2.27 GiB already allocated; 304.69 MiB free; 2.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 240, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 242, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 243, Reason: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 247, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 249, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 250, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 252, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.45 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 253, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 259, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 263, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.42 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 266, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 268, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 269, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 273, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 274, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 279, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 283, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 284, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 286, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 288, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 289, Reason: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 292, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 295, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 296, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 299, Reason: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 301, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 302, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 303, Reason: CUDA out of memory. Tried to allocate 118.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 307, Reason: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 6; 23.70 GiB total capacity; 2.18 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 309, Reason: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 312, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.35 GiB already allocated; 86.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 315, Reason: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 6; 23.70 GiB total capacity; 2.47 GiB already allocated; 112.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 317, Reason: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 68.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 318, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 319, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 322, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.31 GiB already allocated; 54.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 323, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.43 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 326, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 334, Reason: CUDA out of memory. Tried to allocate 134.00 MiB (GPU 6; 23.70 GiB total capacity; 2.65 GiB already allocated; 92.69 MiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 340, Reason: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 341, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 344, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 345, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 352, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 116.69 MiB free; 3.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 356, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 357, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.86 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 362, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 367, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.34 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 370, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.37 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 373, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.46 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 374, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 379, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 40.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 380, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 381, Reason: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 382, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 383, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.91 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 385, Reason: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 389, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 391, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 392, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 393, Reason: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 42.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 398, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 399, Reason: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 22.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 401, Reason: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 6; 23.70 GiB total capacity; 2.35 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 402, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 409, Reason: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 6; 23.70 GiB total capacity; 2.44 GiB already allocated; 34.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 410, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 3.04 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 414, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 415, Reason: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 420, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 421, Reason: CUDA out of memory. Tried to allocate 166.00 MiB (GPU 6; 23.70 GiB total capacity; 2.20 GiB already allocated; 84.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 424, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.70 GiB already allocated; 18.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 425, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 426, Reason: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 6; 23.70 GiB total capacity; 2.60 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 429, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 435, Reason: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 6; 23.70 GiB total capacity; 2.31 GiB already allocated; 160.69 MiB free; 3.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 437, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 444, Reason: CUDA out of memory. Tried to allocate 104.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 66.69 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 453, Reason: CUDA out of memory. Tried to allocate 214.00 MiB (GPU 6; 23.70 GiB total capacity; 2.20 GiB already allocated; 114.69 MiB free; 3.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 458, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 46.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 461, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.69 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 463, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 464, Reason: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 466, Reason: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 23.70 GiB total capacity; 2.99 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 467, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 469, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.68 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 470, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 471, Reason: CUDA out of memory. Tried to allocate 276.00 MiB (GPU 6; 23.70 GiB total capacity; 2.40 GiB already allocated; 44.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 472, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.75 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 476, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 24.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 477, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 6.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 483, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.81 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 484, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 485, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 487, Reason: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 6; 23.70 GiB total capacity; 2.79 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 491, Reason: CUDA out of memory. Tried to allocate 276.00 MiB (GPU 6; 23.70 GiB total capacity; 2.38 GiB already allocated; 174.69 MiB free; 3.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 496, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.93 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 502, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.64 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 503, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 28.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 504, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.87 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 506, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.77 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 507, Reason: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 6; 23.70 GiB total capacity; 2.66 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 511, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.94 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 512, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.63 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 513, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 515, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.88 GiB already allocated; 8.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 516, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.90 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 518, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 519, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 520, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 10.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 523, Reason: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 526, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 527, Reason: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 6; 23.70 GiB total capacity; 2.74 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 529, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.61 GiB already allocated; 48.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 531, Reason: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 6; 23.70 GiB total capacity; 2.20 GiB already allocated; 20.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 532, Reason: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 6; 23.70 GiB total capacity; 2.08 GiB already allocated; 38.69 MiB free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 535, Reason: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 6; 23.70 GiB total capacity; 2.52 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 536, Reason: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 539, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.72 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 540, Reason: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 6; 23.70 GiB total capacity; 2.89 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 544, Reason: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 30.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 545, Reason: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 6; 23.70 GiB total capacity; 2.71 GiB already allocated; 50.69 MiB free; 3.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 547, Reason: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 6; 23.70 GiB total capacity; 2.92 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 548, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.98 GiB already allocated; 12.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 556, Reason: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 6; 23.70 GiB total capacity; 2.55 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 557, Reason: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 6; 23.70 GiB total capacity; 2.83 GiB already allocated; 14.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 558, Reason: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 6; 23.70 GiB total capacity; 2.67 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 559, Reason: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 6; 23.70 GiB total capacity; 2.62 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 560, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.84 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 565, Reason: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 6; 23.70 GiB total capacity; 2.50 GiB already allocated; 84.69 MiB free; 3.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 568, Reason: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 6; 23.70 GiB total capacity; 2.85 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 573, Reason: CUDA out of memory. Tried to allocate 82.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 58.69 MiB free; 3.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 575, Reason: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 6; 23.70 GiB total capacity; 2.80 GiB already allocated; 26.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 576, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.56 GiB already allocated; 2.69 MiB free; 3.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 578, Reason: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 6; 23.70 GiB total capacity; 2.78 GiB already allocated; 4.69 MiB free; 3.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 582, Reason: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 6; 23.70 GiB total capacity; 2.73 GiB already allocated; 32.69 MiB free; 3.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 583, Reason: CUDA out of memory. Tried to allocate 52.00 MiB (GPU 6; 23.70 GiB total capacity; 2.59 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Skipping evaluation batch 584, Reason: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 6; 23.70 GiB total capacity; 2.76 GiB already allocated; 16.69 MiB free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Done
Done
